[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Animals In Motion (with answers)",
    "section": "",
    "text": "Preface\nAnimals in Motion is an online handbook that introduces free, open-source tools for tracking animal motion from videos and extracting quantitative descriptions of behaviour from motion tracks.\nEarlier¬†Versions of this handbook have been taught as hands-on workshops in the following settings:\nWhether you are attending a workshop in person, or following along the materials at your own pace, be sure to check out Appendix A ‚Äî Prerequisites.\nAll materials are shared under the CC-BY-4.0 license so you are welcome to use and adapt them for your own workshops. There is no need to ask for permission, but we‚Äôd love to hear about it if you do! Feedback and contributions to the handbook are always welcome, see Appendix B ‚Äî Contributing.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#preface",
    "href": "index.html#preface",
    "title": "Animals In Motion (with answers)",
    "section": "",
    "text": "the Neuroinformatics Unit Open Software Summer School\nthe Sainsbury Wellcome Centre‚Äôs (SWC) Systems Neurscience PhD Programme.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "Animals In Motion (with answers)",
    "section": "Schedule",
    "text": "Schedule\nThe workshop on October 8th, 2025, for students of the SWC Systems Neurscience PhD Programme will cover the following chapters of this handbook:\n\nWorkshop schedule\n\n\nTime\nTopics\n\n\n\n\nMorning\n1¬† Introduction  3¬† Pose estimation with SLEAP\n\n\nAfternoon\n4¬† Analysing tracks with movement\n\n\n\nStudents may also review the following chapters after the workshop, depending on their interests:\n\n2¬† Deep learning for computer vision primer\nCase studies applying the movement package to real-world datasets:\n\n5¬† A mouse‚Äôs daily activity log\n6¬† Zebra escape trajectories",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#sec-versions",
    "href": "index.html#sec-versions",
    "title": "Animals In Motion (with answers)",
    "section": "Versions",
    "text": "Versions\nThe latest release version is always available at the following URL:\nhttps://animals-in-motion.neuroinformatics.dev/latest/\nTo view other versions, replace latest in the URL with one of the following version names:\n\n\n\nVersion\nDescription\n\n\n\n\ndev\ndevelopment version, corresponding to the main branch\n\n\nv2025.10a1\npre-release of the version taught as part of the SWC Systems Neurscience PhD Programme in October 2025\n\n\nv2025.10a1-answers\nsame as v2025.10a1 but with answers to exercises\n\n\nv2025.08\nversion used during the inaugural Open Software Week in August 2025\n\n\nv2025.08-answers\nsame as v2025.08 but with answers to exercises",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#funding-acknowledgements",
    "href": "index.html#funding-acknowledgements",
    "title": "Animals In Motion (with answers)",
    "section": "Funding & acknowledgements",
    "text": "Funding & acknowledgements\nThe inaugural Open Software Week, which inspired the creation of this handbook, was made possible by a Software Sustainability Institute fellowship to Niko Sirmpilatze, as well as further funding support by the Sainsbury Wellcome Centre, the Society for Research Software Engineering and AIBIO-UK.\nWe thank the Sainsbury Wellcome Centre and the Gatsby Computational Neuroscience Unit for providing facilities for the workshops.\n\n\n\nLogos of workshop sponsors",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "01-intro.html",
    "href": "01-intro.html",
    "title": "1¬† Introduction",
    "section": "",
    "text": "1.1 Summary\nMeasuring animal behaviour often comes down to measuring movement, whether of the whole body or specific parts, using tools such as video cameras and GPS loggers.\nAdvances in computer vision, particularly deep learning-based markerless motion tracking, have transformed how we study animal behaviour, bridging traditions from multiple scientific disciplines. These methods now underpin the emerging field of computational (neuro)ethology, enabling researchers to extract precise motion data from videos, quantify behavioural features, and (in neuroscience) relate them directly to neural activity.\nIn this course, we focus on two key steps in this workflow: pose estimation and motion quantification, while recognising the broader ecosystem of open-source tools available.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#measuring-behaviour-as-movement",
    "href": "01-intro.html#measuring-behaviour-as-movement",
    "title": "1¬† Introduction",
    "section": "1.2 Measuring behaviour as movement",
    "text": "1.2 Measuring behaviour as movement\nDefining behaviour is tricky, and behavioural biologists cannot agree on a single definition (Levitis, Lidicker, and Freund 2009). The following one by Tinbergen has some historical sway:\n\nThe total movements made by the intact animal (Tinbergen 1951).\n\nFraming behaviour as ‚Äúmovement‚Äù is useful. In most studies, what we actually measure is the motion of one or more animals‚Äîand/or their body parts‚Äîover time. We can see that reflected in the devices we most often use to record animal behaviour:\n\nüé• Video cameras\nüì± Inertial measurement units (IMUs)\nüõ∞Ô∏è GPS-based biologgers\nüé§ Microphones\n\nWith the exception of microphones, these devices measure movement, at different spatial and temporal scales.\n\n\n\n\n\n\nNote\n\n\n\nYou could say that microphones capture body movements indirectly: sound waves are generated via the motion of specialised organs such as vocal cords.\n\n\nIn this course, we focus exclusively on video recordings. There are two main ways to extract motion from videos:\n\nMarker-based methods: physical markers are placed on individuals or body parts.\nMarkerless methods: computer vision is used to extract user-defined features directly from videos.\n\n\n\n\n\n\n\n\n\nBy danceinthesky, openverse.org\n\n\n\n\n\n\n\nSource: ‚ÄúCRISPR Ants Lose Ability to Smell‚Äù (2017)\n\n\n\n\n\n\n\n\n\nSource: Mathis et al. (2020)\n\n\n\n\n\n\nFigure¬†1.1: Examples of marker-based (top row) and markerless (bottom row) methods.\n\n\n\n\n\n\n\n\n\nTipDiscuss\n\n\n\n\nWhat are some advantages and disadvantages of marker-based vs.¬†markerless methods?\nIn which cases would you use one over the other?",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#the-rise-of-markerless-methods",
    "href": "01-intro.html#the-rise-of-markerless-methods",
    "title": "1¬† Introduction",
    "section": "1.3 The rise of markerless methods",
    "text": "1.3 The rise of markerless methods\nThere have broadly been two traditions in the study of animal behaviour (Datta et al. 2019):\n\nNeuroscientists have long focused on how animals generate behaviours in response to rewards and punishments, often training them to perform simple, easily measured actions.\nEthologists, in contrast, tend to study naturalistic behaviours expressed freely in ecologically relevant contexts, aiming to understand how behaviour is structured and organised over time‚Äîfor example through the use of ethograms.\n\n\n\n\n\n\n\n\n\nBy Ethan Tyler & Lex Kravitz, scidraw.io\n\n\n\n\n\n\n\nBy Andrea Colins Rodriguez, scidraw.io\n\n\n\n\n\n\n\nExcerpt from a fiddler crab ethogram by Sanna Titus\n\n\n\n\n\n\nFigure¬†1.2: Schematic representation of the neuroscientific tradition, juxtaposed with an ethogram.\n\n\n\nIn the last 10 years, computer vision methods for motion tracking have had an utterly transformative impact. Tools such as DeepLabCut (Mathis et al. 2018) and SLEAP (Pereira et al. 2022) enable researchers to track animal movements in video recordings with impressive accuracy, without the need for physical markers.\n\n\n\n\n\n\n\n\nSource: mackenziemathislab.org/deeplabcut\n\n\n\n\n\n\n\nSource: idtracker.ai\n\n\n\n\n\n\n\n\n\nSource: sleap.ai\n\n\n\n\n\n\nFigure¬†1.3: Examples of markerless motion tracking in action.\n\n\n\nThis rise is mostly driven by advances in deep learning and has dramatically accelerated the scale at which naturalistic behaviour can be measured and analysed. The old distinctions between research traditions are being erased and a new field is emerging:\n\nIn the past decade, a field we now call ‚Äúcomputational ethology‚Äù has begun to take shape. It involves the use of machine vision and machine learning to measure and analyze the patterns of action generated by animals in contexts designed to evoke ecologically relevant behaviors (Anderson and Perona 2014). Technical progress in statistical inference and deep learning, the democratization of high-performance computing (due to falling hardware costs and the ability to rent GPUs and CPUs in the cloud), and new and creative ideas about how to apply technology to measuring naturalistic behavior have dramatically accelerated progress in this research area. (Datta et al. 2019)\n\n\n\n\n\n\n\nFigure¬†1.4: Scientific disciplines converging on markerless motion tracking methods.\n\n\n\n\n\n\n\n\n\nTipDiscuss\n\n\n\n\nWhich scientific disciplines do you represent?\nWhat species are you working with?\nHave you witnessed an increasing use of markerless tracking methods in your field?",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#the-computational-neuroethology-workflow",
    "href": "01-intro.html#the-computational-neuroethology-workflow",
    "title": "1¬† Introduction",
    "section": "1.4 The computational (neuro)ethology workflow",
    "text": "1.4 The computational (neuro)ethology workflow\nThe rise of markerless methods has reconfigured the data acquisition and analysis workflows for scientists interested in measuring animal behaviour:\n\nVideo recordings are the primary data source.\nComputer vision tools (most often based on deep learning) are used to extract user-defined features from video frames and track them over time.\nThe resulting tracks can then be used to quantify various aspects of motion, such as speed, orientation, distance travelled, etc.\nThe motion tracks, video features and derived kinematic features may serve as input for behaviour segmentation algorithms.\n\n\n\n\n\n\n\nFigure¬†1.5: An overview of modern computational ethology workflows\n\n\n\nAn ecosystem of open-source tools has emerged to support this workflow. See Luxem et al. (2023), Pereira, Shaevitz, and Murthy (2020), Blau et al. (2024) for comprehensive reviews. We also provide a non-exhaustive list of open-source tools in Section 1.6.\n\n\n\n\n\n\nNoteWhere does the ‚Äòneuro‚Äô come in?\n\n\n\n\n\nNeuroscientists are increasingly interested in precisely quantifying naturalistic behaviour, for several reasons:\n\nif we are to understand how the brain works, we need to think about the actual problems it evolved to solve. Addressing this challenge means studying natural behavior ‚Äî the kinds of behaviors generated by animals when they are free to act on their own internally-generated goals without physical or psychological restraint (source: Datta Lab website).\n‚Ä¶detailed examination of brain parts or their selective perturbation is not sufficient to understand how the brain generates behavior (Krakauer et al. 2017).\nThe behavioral work needs to be as fine-grained as work at the neural level. Otherwise one is imperiled by a granularity mismatch between levels‚Ä¶ (Krakauer et al. 2017).\n\nThis shift in focus within neuroscience has been a major driver for the rapid development of the computational approaches described above. Some refer to this field as computational neuroethology‚Äîthe science of quantifying naturalistic behaviours to understand the brain (Datta et al. 2019).\nThe data acquisition and analysis workflows used in computational neuroethology are similar to those shown in Figure¬†1.5. The key difference is that derived measures of behaviour‚Äîwhether continuous variables like speed or discrete actions such as ‚Äúgrooming‚Äù‚Äîare ultimately analysed alongside neural data, such as spike trains or calcium imaging traces (Figure¬†1.6).\n\n\n\n\n\n\nFigure¬†1.6: An overview of modern computational neuroethology workflows.\n\n\n\nHighly recommended readings:\n\nKrakauer et al. (2017)\nDatta et al. (2019)\nPereira, Shaevitz, and Murthy (2020)",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#the-scope-of-this-course",
    "href": "01-intro.html#the-scope-of-this-course",
    "title": "1¬† Introduction",
    "section": "1.5 The scope of this course",
    "text": "1.5 The scope of this course\nWe will start with a primer on deep learning for computer vision (Chapter 2), going over the key concepts and technologies that underpin most markerless tracking approaches.\nAfter that we could take a whirlwind tour through all the stages of a computaional ethology workflow Figure¬†1.5 and the various tools available for each step. However, since we want this to work as a two-day hands-on workshop with plenty of time for exercises and active learning, we have instead chosen to focus on two key steps:\n\nChapter 3: Pose estimation and tracking with SLEAP (Pereira et al. 2022). We chose SLEAP because we know it best, but the knowledge gained should be applicable to most other pose estimation tools.\nChapter 4: Analysing motion tracks with movement‚Äîa Python package we develop (Sirmpilatze et al. 2025)‚Äîfollowed by two case studies on real-world datasets Chapter 5, Chapter 6.\n\n\n\n\n\n\n\nFigure¬†1.7: What we‚Äôll cover in this course\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nWe want to emphasise that there are many excellent open-source tools beyond those we focus on here. The next section provides a non-exhaustive list of tools that may be useful for your own projects.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#sec-useful-tools",
    "href": "01-intro.html#sec-useful-tools",
    "title": "1¬† Introduction",
    "section": "1.6 Useful open-source tools",
    "text": "1.6 Useful open-source tools\n\n\n\n\n\n\nNote\n\n\n\nWe‚Äôve mainly highlighted tools commonly applied to animal behaviour data. This is not a comprehensive list, and the tools appear in no particular order. Some don‚Äôt fit neatly into the categories below, and their classification is somewhat subjective. If you‚Äôd like to add a tool you‚Äôve built or enjoy using, please open an issue or submit a pull request.\n\n\n\n1.6.1 Data acquisition\n\nBonsai (Lopes et al. 2015)\n\n\n\n1.6.2 Video processing\n\nOpenCV Bradski (2000)\nffmpeg Tomar (2006)\n\n\n\n1.6.3 Motion tracking\n\nDeepLabCut Mathis et al. (2018); Lauer et al. (2022)\nSLEAP Pereira et al. (2022)\nLightningPose Biderman et al. (2024)\nTRex Walter and Couzin (2021)\nidtracker.ai Romero-Ferrero et al. (2019)\nAnipose Karashchuk et al. (2021)\nDANNCE Dunn et al. (2021)\nDeepPoseKit Graving et al. (2019)\nFastTrack Gallois and Candelier (2021)\nDeepFly3D G√ºnel et al. (2019)\n\n\n\n1.6.4 Motion quantification\n\nmovement Sirmpilatze et al. (2025)\nanimovement Roald-Arb√∏l (2024)\nPyRat De Almeida et al. (2022)\nDLC2Kinematics\npyomeca Martinez, Michaud, and Begon (2020)\nmovingpandas Graser (2019)\nscikit-mobility Pappalardo et al. (2022)\nRtrack Overall et al. (2020)\nColonyTrack Overall (2024)\nswaRm\n\n\n\n1.6.5 Behaviour segmentation\n\n(Keypoint) MoSeq Wiltschko et al. (2015); Weinreb (2024)\nVAME Luxem et al. (2022)\nB-SOiD Hsu and Yttri (2021)\nA-SOiD Schweihoff et al. (2022)\nDeepEthogram Bohnslav et al. (2021)\nSimBA Goodwin et al. (2024)\nDeepOF Miranda et al. (2023)\nLISBET Chindemi, Girard, and Bellone (2023)\nDLC2action\nLabGym Hu et al. (2023)\nJABS Beane et al. (2023)\nJAABA Kabra et al. (2013)\nMotionMapper Berman et al. (2014)\nBORIS Friard and Gamba (2016)\nMARS Segalin et al. (2021)\n\n\n\n\n\nAnderson, David J., and Pietro Perona. 2014. ‚ÄúToward a Science of Computational Ethology.‚Äù Neuron 84 (1): 18‚Äì31. https://doi.org/10.1016/j.neuron.2014.09.005.\n\n\nBeane, Glen, Brian Q. Geuther, Thomas J. Sproule, Anshul Choudhary, Jarek Trapszo, Leinani Hession, Vivek Kohar, and Vivek Kumar. 2023. ‚ÄúJAX Animal Behavior System (JABS): A Video-Based Phenotyping Platform for the Laboratory Mouse.‚Äù bioRxiv. https://doi.org/10.1101/2022.01.13.476229.\n\n\nBerman, Gordon J., Daniel M. Choi, William Bialek, and Joshua W. Shaevitz. 2014. ‚ÄúMapping the Stereotyped Behaviour of Freely Moving Fruit Flies.‚Äù Journal of The Royal Society Interface 11 (99): 20140672. https://doi.org/10.1098/rsif.2014.0672.\n\n\nBiderman, Dan, Matthew R. Whiteway, Cole Hurwitz, Nicholas Greenspan, Robert S. Lee, Ankit Vishnubhotla, Richard Warren, et al. 2024. ‚ÄúLightning Pose: Improved Animal Pose Estimation via Semi-Supervised Learning, Bayesian Ensembling and Cloud-Native Open-Source Tools.‚Äù Nature Methods 21 (7): 1316‚Äì28. https://doi.org/10.1038/s41592-024-02319-1.\n\n\nBlau, Ari, Evan S. Schaffer, Neeli Mishra, Nathaniel J. Miska, International Brain Laboratory, Liam Paninski, and Matthew R. Whiteway. 2024. ‚ÄúA Study of Animal Action Segmentation Algorithms Across Supervised, Unsupervised, and Semi-Supervised Learning Paradigms.‚Äù Neurons, Behavior, Data Analysis, and Theory, December, 1‚Äì46. https://doi.org/10.51628/001c.127770.\n\n\nBohnslav, James P, Nivanthika K Wimalasena, Kelsey J Clausing, Yu Y Dai, David A Yarmolinsky, Tom√°s Cruz, Adam D Kashlan, et al. 2021. ‚ÄúDeepEthogram, a Machine Learning Pipeline for Supervised Behavior Classification from Raw Pixels.‚Äù Edited by Mackenzie W Mathis, Timothy E Behrens, Mackenzie W Mathis, and Johannes Bohacek. eLife 10 (September): e63377. https://doi.org/10.7554/eLife.63377.\n\n\nBradski, G. 2000. ‚ÄúThe OpenCV Library.‚Äù Dr. Dobb‚Äôs Journal of Software Tools.\n\n\nChindemi, Giuseppe, Benoit Girard, and Camilla Bellone. 2023. ‚ÄúLISBET: A Machine Learning Model for the Automatic Segmentation of Social Behavior Motifs.‚Äù https://arxiv.org/abs/2311.04069.\n\n\n‚ÄúCRISPR Ants Lose Ability to Smell.‚Äù 2017. Nature 548 (7667): 263‚Äì63. https://doi.org/10.1038/d41586-017-02337-4.\n\n\nDatta, Sandeep Robert, David J. Anderson, Kristin Branson, Pietro Perona, and Andrew Leifer. 2019. ‚ÄúComputational Neuroethology: A Call to Action.‚Äù Neuron 104 (1): 11‚Äì24. https://doi.org/10.1016/j.neuron.2019.09.038.\n\n\nDe Almeida, Tulio Fernandes, Bruno Guedes Spinelli, Ram√≥n Hypolito Lima, Maria Carolina Gonzalez, and Abner Cardoso Rodrigues. 2022. ‚ÄúPyRAT: An Open-Source Python Library for Animal Behavior Analysis.‚Äù Frontiers in Neuroscience 16. https://www.frontiersin.org/articles/10.3389/fnins.2022.779106.\n\n\nDunn, Timothy W., Jesse D. Marshall, Kyle S. Severson, Diego E. Aldarondo, David G. C. Hildebrand, Selmaan N. Chettih, William L. Wang, et al. 2021. ‚ÄúGeometric Deep Learning Enables 3D Kinematic Profiling Across Species and Environments.‚Äù Nature Methods 18 (5): 564‚Äì73. https://doi.org/10.1038/s41592-021-01106-6.\n\n\nFriard, Olivier, and Marco Gamba. 2016. ‚ÄúBORIS: A Free, Versatile Open-Source Event-Logging Software for Video/Audio Coding and Live Observations.‚Äù Methods in Ecology and Evolution 7 (11): 1325‚Äì30. https://doi.org/10.1111/2041-210X.12584.\n\n\nGallois, Benjamin, and Rapha√´l Candelier. 2021. ‚ÄúFastTrack: An Open-Source Software for Tracking Varying Numbers of Deformable Objects.‚Äù PLOS Computational Biology 17 (2): e1008697. https://doi.org/10.1371/journal.pcbi.1008697.\n\n\nGoodwin, Nastacia L., Jia J. Choong, Sophia Hwang, Kayla Pitts, Liana Bloom, Aasiya Islam, Yizhe Y. Zhang, et al. 2024. ‚ÄúSimple Behavioral Analysis (SimBA) as a Platform for Explainable Machine Learning in Behavioral Neuroscience.‚Äù Nature Neuroscience, May, 1‚Äì14. https://doi.org/10.1038/s41593-024-01649-9.\n\n\nGraser, Anita. 2019. ‚ÄúMovingPandas: Efficient Structures for Movement Data in Python.‚Äù GI_Forum 2019, Volume 7, (June): 54‚Äì68. https://doi.org/10.1553/giscience2019_01_s54.\n\n\nGraving, Jacob M, Daniel Chae, Hemal Naik, Liang Li, Benjamin Koger, Blair R Costelloe, and Iain D Couzin. 2019. ‚ÄúDeepPoseKit, a Software Toolkit for Fast and Robust Animal Pose Estimation Using Deep Learning.‚Äù Edited by Ian T Baldwin, Josh W Shaevitz, Josh W Shaevitz, and Greg Stephens. eLife 8 (October): e47994. https://doi.org/10.7554/eLife.47994.\n\n\nG√ºnel, Semih, Helge Rhodin, Daniel Morales, Jo√£o Campagnolo, Pavan Ramdya, and Pascal Fua. 2019. ‚ÄúDeepFly3D, a Deep Learning-Based Approach for 3D Limb and Appendage Tracking in Tethered, Adult Drosophila.‚Äù Edited by Timothy O‚ÄôLeary, Ronald L Calabrese, and Josh W Shaevitz. eLife 8 (October): e48571. https://doi.org/10.7554/eLife.48571.\n\n\nHsu, Alexander I., and Eric A. Yttri. 2021. ‚ÄúB-SOiD, an Open-Source Unsupervised Algorithm for Identification and Fast Prediction of Behaviors.‚Äù Nature Communications 12 (1): 5188. https://doi.org/10.1038/s41467-021-25420-x.\n\n\nHu, Yujia, Carrie R. Ferrario, Alexander D. Maitland, Rita B. Ionides, Anjesh Ghimire, Brendon Watson, Kenichi Iwasaki, et al. 2023. ‚ÄúLabGym: Quantification of User-Defined Animal Behaviors Using Learning-Based Holistic Assessment.‚Äù Cell Reports Methods 0 (0). https://doi.org/10.1016/j.crmeth.2023.100415.\n\n\nKabra, Mayank, Alice A. Robie, Marta Rivera-Alba, Steven Branson, and Kristin Branson. 2013. ‚ÄúJAABA: Interactive Machine Learning for Automatic Annotation of Animal Behavior.‚Äù Nature Methods 10 (1): 64‚Äì67. https://doi.org/10.1038/nmeth.2281.\n\n\nKarashchuk, Pierre, Katie L. Rupp, Evyn S. Dickinson, Sarah Walling-Bell, Elischa Sanders, Eiman Azim, Bingni W. Brunton, and John C. Tuthill. 2021. ‚ÄúAnipose: A Toolkit for Robust Markerless 3D Pose Estimation.‚Äù Cell Reports 36 (13): 109730. https://doi.org/10.1016/j.celrep.2021.109730.\n\n\nKrakauer, John W., Asif A. Ghazanfar, Alex Gomez-Marin, Malcolm A. MacIver, and David Poeppel. 2017. ‚ÄúNeuroscience Needs Behavior: Correcting a Reductionist Bias.‚Äù Neuron 93 (3): 480‚Äì90. https://doi.org/10.1016/j.neuron.2016.12.041.\n\n\nLauer, Jessy, Mu Zhou, Shaokai Ye, William Menegas, Steffen Schneider, Tanmay Nath, Mohammed Mostafizur Rahman, et al. 2022. ‚ÄúMulti-Animal Pose Estimation, Identification and Tracking with DeepLabCut.‚Äù Nature Methods 19 (4): 496‚Äì504. https://doi.org/10.1038/s41592-022-01443-0.\n\n\nLevitis, Daniel A., William Z. Lidicker, and Glenn Freund. 2009. ‚ÄúBehavioural Biologists Don‚Äôt Agree on What Constitutes Behaviour.‚Äù Animal Behaviour 78 (1): 103‚Äì10. https://doi.org/10.1016/j.anbehav.2009.03.018.\n\n\nLopes, Gon√ßalo, Niccol√≤ Bonacchi, Jo√£o Fraz√£o, Joana P. Neto, Bassam V. Atallah, Sofia Soares, Lu√≠s Moreira, et al. 2015. ‚ÄúBonsai: An Event-Based Framework for Processing and Controlling Data Streams.‚Äù Frontiers in Neuroinformatics 9. https://www.frontiersin.org/articles/10.3389/fninf.2015.00007.\n\n\nLuxem, Kevin, Petra Mocellin, Falko Fuhrmann, Johannes K√ºrsch, Stephanie R. Miller, Jorge J. Palop, Stefan Remy, and Pavol Bauer. 2022. ‚ÄúIdentifying Behavioral Structure from Deep Variational Embeddings of Animal Motion.‚Äù Communications Biology 5 (1): 1‚Äì15. https://doi.org/10.1038/s42003-022-04080-7.\n\n\nLuxem, Kevin, Jennifer J Sun, Sean P Bradley, Keerthi Krishnan, Eric Yttri, Jan Zimmermann, Talmo D Pereira, and Mark Laubach. 2023. ‚ÄúOpen-Source Tools for Behavioral Video Analysis: Setup, Methods, and Best Practices.‚Äù Edited by Denise J Cai and Laura L Colgin. eLife 12 (March): e79305. https://doi.org/10.7554/eLife.79305.\n\n\nMartinez, Romain, Benjamin Michaud, and Mickael Begon. 2020. ‚Äú‚ÄòPyomeca‚Äò: An Open-Source Framework for Biomechanical Analysis.‚Äù Journal of Open Source Software 5 (53): 2431. https://doi.org/10.21105/joss.02431.\n\n\nMathis, Alexander, Pranav Mamidanna, Kevin M. Cury, Taiga Abe, Venkatesh N. Murthy, Mackenzie Weygandt Mathis, and Matthias Bethge. 2018. ‚ÄúDeepLabCut: Markerless Pose Estimation of User-Defined Body Parts with Deep Learning.‚Äù Nature Neuroscience 21 (9): 1281‚Äì89. https://doi.org/10.1038/s41593-018-0209-y.\n\n\nMathis, Alexander, Steffen Schneider, Jessy Lauer, and Mackenzie Weygandt Mathis. 2020. ‚ÄúA Primer on Motion Capture with Deep Learning: Principles, Pitfalls, and Perspectives.‚Äù Neuron 108 (1): 44‚Äì65. https://doi.org/10.1016/j.neuron.2020.09.017.\n\n\nMiranda, Lucas, Joeri Bordes, Benno P√ºtz, Mathias V. Schmidt, and Bertram M√ºller-Myhsok. 2023. ‚ÄúDeepOF: A Python Package for Supervised and Unsupervised Pattern Recognition in Mice Motion Tracking Data.‚Äù Journal of Open Source Software 8 (86): 5394. https://doi.org/10.21105/joss.05394.\n\n\nOverall, Rupert W. 2024. ColonyTrack: Analysis of Multi-Subject Tracking Data from Interconnected Cage Networks.\n\n\nOverall, Rupert W, Sara Zocher, Alexander Garthe, and Gerd Kempermann. 2020. ‚ÄúRtrack: A Software Package for Reproducible Automated Water Maze Analysis.‚Äù bioRxiv 2020.02.27.967372. https://doi.org/10.1101/2020.02.27.967372.\n\n\nPappalardo, Luca, Filippo Simini, Gianni Barlacchi, and Roberto Pellungrini. 2022. ‚ÄúScikit-Mobility: A Python Library for the Analysis, Generation, and Risk Assessment of Mobility Data.‚Äù Journal of Statistical Software 103 (1): 1‚Äì38. https://doi.org/10.18637/jss.v103.i04.\n\n\nPereira, Talmo D., Joshua W. Shaevitz, and Mala Murthy. 2020. ‚ÄúQuantifying Behavior to Understand the Brain.‚Äù Nature Neuroscience 23 (12): 1537‚Äì49. https://doi.org/10.1038/s41593-020-00734-z.\n\n\nPereira, Talmo D., Nathaniel Tabris, Arie Matsliah, David M. Turner, Junyu Li, Shruthi Ravindranath, Eleni S. Papadoyannis, et al. 2022. ‚ÄúSLEAP: A Deep Learning System for Multi-Animal Pose Tracking.‚Äù Nature Methods 19 (4): 486‚Äì95. https://doi.org/10.1038/s41592-022-01426-1.\n\n\nRoald-Arb√∏l, Mikkel. 2024. ‚ÄúAnimovement: An r Toolbox for Analysing Animal Movement Across Space and Time.‚Äù http://www.roald-arboel.com/animovement/.\n\n\nRomero-Ferrero, Francisco, Mattia G. Bergomi, Robert C. Hinz, Francisco J. H. Heras, and Gonzalo G. de Polavieja. 2019. ‚ÄúIdtracker.ai: Tracking All Individuals in Small or Large Collectives of Unmarked Animals.‚Äù Nature Methods 16 (2): 179‚Äì82. https://doi.org/10.1038/s41592-018-0295-5.\n\n\nSchweihoff, Jens F., Alexander I. Hsu, Martin K. Schwarz, and Eric A. Yttri. 2022. ‚ÄúA-SOiD, an Active Learning Platform for Expert-Guided, Data Efficient Discovery of Behavior.‚Äù bioRxiv. https://doi.org/10.1101/2022.11.04.515138.\n\n\nSegalin, Cristina, Jalani Williams, Tomomi Karigo, May Hui, Moriel Zelikowsky, Jennifer J Sun, Pietro Perona, David J Anderson, and Ann Kennedy. 2021. ‚ÄúThe Mouse Action Recognition System (MARS) Software Pipeline for Automated Analysis of Social Behaviors in Mice.‚Äù Edited by Gordon J Berman, Kate M Wassum, and Asaf Gal. eLife 10 (November): e63720. https://doi.org/10.7554/eLife.63720.\n\n\nSirmpilatze, Niko, Sof√≠a Mi√±ano, Chang Huan Lo, Adam Tyson, Will Graham, Stella Prins, Brandon Peri, et al. 2025. ‚ÄúNeuroinformatics-Unit/Movement: V0.9.0.‚Äù Zenodo. https://doi.org/10.5281/zenodo.16754905.\n\n\nTinbergen, Niko. 1951. The Study of Instinct. Clarendon Press.\n\n\nTomar, Suramya. 2006. ‚ÄúConverting Video Formats with FFmpeg.‚Äù Linux Journal 2006 (146): 10.\n\n\nWalter, Tristan, and Iain D Couzin. 2021. ‚ÄúTRex, a Fast Multi-Animal Tracking System with Markerless Identification, and 2D Estimation of Posture and Visual Fields.‚Äù Edited by David Lentink, Christian Rutz, and Sergi Pujades. eLife 10 (February): e64000. https://doi.org/10.7554/eLife.64000.\n\n\nWeinreb, Caleb. 2024. ‚ÄúKeypoint-MoSeq: Parsing Behavior by Linking Point Tracking to Pose Dynamics.‚Äù Nature Methods 21.\n\n\nWiltschko, Alexander B., Matthew J. Johnson, Giuliano Iurilli, Ralph E. Peterson, Jesse M. Katon, Stan L. Pashkovski, Victoria E. Abraira, Ryan P. Adams, and Sandeep Robert Datta. 2015. ‚ÄúMapping Sub-Second Structure in Mouse Behavior.‚Äù Neuron 88 (6): 1121‚Äì35. https://doi.org/10.1016/j.neuron.2015.11.031.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "02-deep-learning.html",
    "href": "02-deep-learning.html",
    "title": "2¬† Deep learning for computer vision primer",
    "section": "",
    "text": "All you need to know about deep learning and its applications to computer vision to understand the rest of the course. This chapter is available as a slides deck:\nDeep learning for computer vision: a primer for the animal behaviour scientist.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Deep learning for computer vision primer</span>"
    ]
  },
  {
    "objectID": "03-sleap-tutorial.html",
    "href": "03-sleap-tutorial.html",
    "title": "3¬† Pose estimation with SLEAP",
    "section": "",
    "text": "3.1 Single-animal vs multi-animal pose estimation\nBefore we proceed, make sure you have installed SLEAP (Pereira et al. 2022) and activated the corresponding conda environment (see prerequisites¬†A.3.2). You will also need to download the CalMS21 dataset (Sun et al. 2021) video file mouse044_task1_annotator1.mp4 from Dropbox (see prerequisites¬†A.4 for details on the folder contents).\nSingle-animal pose estimation focuses on detecting keypoints for one animal per frame, which is considered a landmark-localisation task where each body part has a unique coordinate. This approach is simpler and faster to train and run.\nMulti-animal pose estimation aims to detect and track multiple animals simultaneously within the same frame. This is essential for studying social behaviours, group dynamics, or any scenario where animals interact, as it addresses the unique challenges of assigning detections reliably to individuals both within an image (part-grouping problem) and across frames (identity-tracking problem).",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Pose estimation with SLEAP</span>"
    ]
  },
  {
    "objectID": "03-sleap-tutorial.html#single-animal-vs-multi-animal-pose-estimation",
    "href": "03-sleap-tutorial.html#single-animal-vs-multi-animal-pose-estimation",
    "title": "3¬† Pose estimation with SLEAP",
    "section": "",
    "text": "Single-animal pose estimation, source: Pereira, Shaevitz, and Murthy (2020).\n\n\n\n\n\n\nThe part-grouping problem in multi-animal pose estimation.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Pose estimation with SLEAP</span>"
    ]
  },
  {
    "objectID": "03-sleap-tutorial.html#top-down-vs-bottom-up-approaches",
    "href": "03-sleap-tutorial.html#top-down-vs-bottom-up-approaches",
    "title": "3¬† Pose estimation with SLEAP",
    "section": "3.2 Top-down vs bottom-up approaches",
    "text": "3.2 Top-down vs bottom-up approaches\nFor multi-animal pose estimation, SLEAP offers both top-down and bottom-up approaches.\n\n\n\nSource:Pereira et al. (2022)\n\n\nTop-down approaches use two models in sequence. First, an anchor detection model (e.g.¬†a centroid model) locates each animal in the frame. Then, for each detected animal, a pose estimation model processes an anchor-centred crop to predict confidence maps for the body parts of the centred animal.\nThis approach typically yields more accurate pose estimates and is well-suited to datasets with few animals. As the second stage of the network runs once per animal, inference speed scales linearly with the number of animals.\n\n\n\nSource: Pereira et al. (2022)\n\n\nBottom-up approaches use a single model that processes the entire frame in a single pass. This model outputs confidence maps for all body parts in the image, along with Part Affinity Fields (PAFs)‚Äîvector fields that represent spatial relationships between parts and are used to group them into individual animal instances.\nDue to their single-stage construction, bottom-up models scale efficiently with increasing numbers of animals and are particularly effective in crowded or high-occupancy scenes.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Pose estimation with SLEAP</span>"
    ]
  },
  {
    "objectID": "03-sleap-tutorial.html#identity-tracking-approaches",
    "href": "03-sleap-tutorial.html#identity-tracking-approaches",
    "title": "3¬† Pose estimation with SLEAP",
    "section": "3.3 Identity tracking approaches",
    "text": "3.3 Identity tracking approaches\n\n\n\n\n\nSLEAP addresses the challenge of maintaining consistent animal identities across frames using two primary strategies: temporal-based and appearance-based cues.\nTemporal-based tracking uses optical flow to estimate pose displacement across frames, associating past with current poses without requiring model training. This makes it well-suited for animals that are visually similar, as it avoids the need to label consecutive frames. However, errors, such as identity switches, can accumulate and propagate, limiting its reliability in long videos or real-time settings where post-hoc correction is not feasible.\nAppearance-based tracking (ID models) assigns identities based on visual features while simultaneously detecting and grouping landmarks. This approach mitigates error propagation but relies on animals having distinguishable visual traits that allow manual identification during labelling.\n\nTop-down ID models extend the centered-instance network to predict class probabilities for each animal-centred crop.\nBottom-up ID models replace PAFs with multi-class segmentation maps, collapsing body part masks into separate channels for each unique class ID. Grouping is implicit in the ID assignment.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Pose estimation with SLEAP</span>"
    ]
  },
  {
    "objectID": "03-sleap-tutorial.html#residentintruder-assay",
    "href": "03-sleap-tutorial.html#residentintruder-assay",
    "title": "3¬† Pose estimation with SLEAP",
    "section": "3.4 Resident‚Äìintruder assay",
    "text": "3.4 Resident‚Äìintruder assay\nThe resident‚Äìintruder assay (Koolhaas Jaap M. 2013) is a behavioural test used to study social interactions, especially aggression and territoriality, in rodents. A resident mouse, habituated to its home cage, is confronted with an unfamiliar intruder, and their interactions‚Äîsuch as chasing, attacking, or investigating‚Äîare observed and quantified. This assay is widely used in neuroscience to explore the neural and genetic basis of social behaviour.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Pose estimation with SLEAP</span>"
    ]
  },
  {
    "objectID": "03-sleap-tutorial.html#dataset",
    "href": "03-sleap-tutorial.html#dataset",
    "title": "3¬† Pose estimation with SLEAP",
    "section": "3.5 Dataset",
    "text": "3.5 Dataset\nIn this tutorial, we will use SLEAP to train a multi-animal top-down identity model to simultaneously perform pose estimation and identity tracking of two mice in a short video (mouse044_task1_annotator1.mp4) from the CalMS21 dataset.\nThis video captures a brief interaction between two mice in a resident‚Äìintruder assay, where the black mouse (the resident), implanted with a head-mounted microendoscope, has established territory and the white mouse (the intruder) is newly introduced into the resident‚Äôs cage.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Pose estimation with SLEAP</span>"
    ]
  },
  {
    "objectID": "03-sleap-tutorial.html#sleap-workflow",
    "href": "03-sleap-tutorial.html#sleap-workflow",
    "title": "3¬† Pose estimation with SLEAP",
    "section": "3.6 SLEAP workflow",
    "text": "3.6 SLEAP workflow\n\n\n\n\n\ngraph LR\n    videos(\"Videos&lt;br&gt;(1,2,...,n)\") --&gt; |extract| frames[/Sampled&lt;br&gt;frames/]\n    frames --&gt; |\"label&lt;br&gt;body parts&lt;br&gt;and ID\"| labels[/Training&lt;br&gt;dataset/]\n    labels --&gt; |train| model[/Model/]\n\n    videos --&gt; test[/Unseen&lt;br&gt;frames/]\n    test --&gt; model\n    model --&gt; |infer| predictions[/Predictions/]\n    \n    predictions --&gt; |fix labels&lt;br&gt;and merge| labels\n\n\n\n\n\n\nA typical SLEAP workflow for multi-animal pose estimation and identity tracking consists of the following key steps:\n\nCreate project: Start a new SLEAP project and import your video(s).\nDefine skeleton: Create a Skeleton that defines the Nodes (each representing a keypoint or body part of interest, e.g.¬†nose, left ear, right ear) and Edges (each representing the connections between keypoints, e.g.¬†nose‚Äìleft ear, nose‚Äìright ear) for the animals to be tracked.\nSample frames: Extract frames from your video(s) to create a set of frames for annotation.\nLabel frames: Annotate the sampled frames by marking the body parts and assigning identities (Tracks) to each animal. These labelled frames together form the training dataset.\nTrain model: Use the training dataset to train a pose estimation and identity tracking model.\nPredict on new data: Apply the trained model to new, unlabelled video frames to generate pose and identity predictions for each animal.\nProofread predictions: Review and correct the predictions as needed.\nRefine model: Corrected predictions can be merged back into the training dataset to retrain the model as needed.\n\n\n3.6.1 Create a new project\nActivate the sleap environment and launch the SLEAP GUI.\nconda activate sleap\nsleap-label\nAdd a video by navigating to the ‚ÄúVideos‚Äù panel and clicking on ‚ÄúAdd Videos‚Äù (or go to ‚ÄúFile‚Äù then ‚ÄúAdd Videos‚Äù). Since the video is in greyscale, enable the ‚ÄúGrayscale‚Äù option in the video import dialogue. This ensures SLEAP processes the input as a single-channel image, which can improve performance and reduce memory usage for greyscale videos. Further details can be found in SLEAP‚Äôs Creating a project guide.\n\n\n\n3.6.2 Define skeleton\n\n\n\n\n\nIn SLEAP, Skeletons are defined as a set of Nodes (body parts or keypoints of interest) and Edges (connections between body parts or keypoints). With the exception of bottom-up models, Edges serve primarily for visualisation.\nSwitch to the Skeleton panel and add Nodes for each body part of interest, e.g.:\n\nnose\nright_ear\nleft_ear\nneck\nright_hip\nleft_hip\ntail_base\n\nThen, define the Edges to connect the Nodes using the drop-down menus, e.g.:\n\nnose‚Äìleft_ear\nnose‚Äìright_ear\nleft_ear‚Äìneck\nright_ear‚Äìneck\nneck‚Äìleft_hip\nneck‚Äìright_hip\nleft_hip‚Äìtail_base\nright_hip‚Äìtail_base\n\n\nOnce you have defined the skeleton, save the project by clicking on ‚ÄúFile‚Äù then ‚ÄúSave‚Äù (or with Ctrl/Cmd+S).\n\n\n3.6.3 Sample frames\nFor assembling a set of frames for annotation, you can either pick your own frames, or let SLEAP suggest a set of frames using the ‚ÄúLabeling Suggestions‚Äù panel, which offers several automated sampling strategies to help select informative and diverse frames.\nIn this example, we will use the ‚ÄúLabeling Suggestions‚Äù panel to randomly sample 20 frames from the video.\n\n\n\n3.6.4 Label frames\nTo begin labelling, click on ‚ÄúLabels‚Äù then ‚ÄúAdd Instance‚Äù in the top menu bar. The initial instance will have its nodes placed randomly. Adjust each point to its correct location by dragging it with the mouse.\nIn this example, the black mouse‚Äôs nose is occluded by an implant. To mark the node as hidden, right-click the node or its name to toggle visibility. If you can reasonably infer its location, you can also mark it as ‚Äúvisible‚Äù to help the model learn to predict occluded nodes.\nFor tracking the identities of the mice, we will also assign identities to each instance by adding a Track. This step is optional if you are only tracking a single animal or if the animals are visually indistinguishable.\nTo assign a track, select an instance and click on ‚ÄúTracks‚Äù then ‚ÄúSet Instance Track‚Äù in the top menu bar. A new Track (‚ÄúTrack 1‚Äù) will be created. Tracks can be renamed in the ‚ÄúTrack‚Äù column. In this example, we will name the black mouse resident_b and the white mouse intruder_w.\n\nOnce you have labelled the initial frame, you can navigate to the previous or next suggested frame by clicking ‚ÄúPrevious‚Äù or ‚ÄúNext‚Äù in the ‚ÄúLabeling Suggestions‚Äù panel.\n\n\n\n\n\n\nImportant\n\n\n\nRemember to save your progress frequently by clicking ‚ÄúFile‚Äù then ‚ÄúSave‚Äù (or with Ctrl/Cmd+S).\n\n\nYou may also find the following keyboard shortcuts and mouse actions helpful:\n\n\n\n\n\n\n\nFunction\nKeyboard shortcut/Mouse actions\n\n\n\n\nAdd instance\nCtrl/Cmd+I\n\n\nAdd new track\nCtrl/Cmd+0\n\n\nAssign track\nSelect instance, then Ctrl/Cmd+1-9 (number = track ID)\n\n\nToggle node visibility (occlusion)\nRight-click on node or its label\n\n\nMove entire instance\nHold Alt (Windows) / Option (Mac) and drag any node\n\n\nRotate instance\nHold Alt (Windows) / Option (Mac), click on any node, and scroll mouse wheel\n\n\nZoom in/out\nPlace cursor over area to zoom in or out, then scroll mouse wheel\n\n\nDelete instance\nSelect instance, then Ctrl/Cmd+Backspace\n\n\nNavigate between frames\nLeft/Right\n\n\nGo to the next suggested frame\nSpace\n\n\nGo to the previous suggested frame\nShift + Space\n\n\nGo to the next labelled frame\nAlt (Windows) / Option (Mac) + right\n\n\nGo to the previous labelled frame\nAlt (Windows) / Option (Mac) + left\n\n\n\nTo speed up labelling in subsequent frames, right-click on the frame and choose from several options for adding a new instance. For example, selecting ‚ÄúCopy prior frame‚Äù will duplicate the instance(s) from the previous labelled frame, allowing you to quickly adjust only the necessary points. This is especially useful when animal poses change gradually between frames.\nSee also SLEAP‚Äôs GUI guide on ‚ÄúLabels‚Äù for a complete reference of all labelling functions.\n\n\n\n\n\n\nTipDiscuss\n\n\n\n\nHow did you find the labelling process? What challenges did you encounter?\nWhat strategies could annotators use to ensure consistency throughout the labelling process?\nHow might the quality of annotations affect model performance?\n\n\n\n\n\n3.6.5 Configure and train models\nOnce you have labelled a sufficient number of frames, you can configure and train your first model. To do this, go to ‚ÄúPredict‚Äù then ‚ÄúRun training‚Äù in the top menu bar.\nHere, we will employ the appearance-based tracking approach for maintaining consistent identities of individual mice across frames. To do so, select ‚Äúmulti-animal top-down-id‚Äù as the ‚ÄúTraining Pipeline Type‚Äù. We will also configure the training pipeline to predict on 20 more frames randomly sampled from the video once training is complete.\n\nSee also SLEAP‚Äôs Configuring models guide for further details on model types and training options.\n\n3.6.5.1 Centroid model\nWe will now configure the Centroid model, which predicts the location of each animal in each frame.\nSince we have labelled only a small number of frames, increase the validation split (e.g.¬†0.2) to obtain more stable and representative validation metrics. To reduce the risk of overfitting, set the number of training epochs to a low value (e.g.¬†10‚Äì20 epochs). In the ‚ÄúData‚Äù and ‚ÄúOptimization‚Äù panels, adjust ‚ÄúValidation‚Äù and ‚ÄúEpochs‚Äù accordingly.\nNext, adjust the receptive field size to match the scale of the features you want the model to detect. This can be achieved by adjusting ‚ÄúInput Scaling‚Äù and ‚ÄúMax Stride‚Äù. As a rule of thumb, the receptive field (blue box in the preview) should be large enough to cover the whole animal.\nTo help the model generalise to animals in different orientations, enable random rotation augmentation. In the ‚ÄúAugmentation‚Äù panel, enable ‚ÄúRotation‚Äù and set the rotation range (e.g.¬†-180¬∞ to 180¬∞).\n\n\n\n3.6.5.2 Top-down ID model\nThe Top-down ID model predicts the full pose (locations of all defined nodes) and assigns identities to each animal in each frame.\nAs with the Centroid model, configure the validation split, number of training epochs, random rotation augmentation, and receptive field size accordingly.\nYou may also want to reduce the batch size (e.g.¬†to 4) to ensure the model trains reliably and fits within memory constraints, especially when using limited hardware resources (e.g.¬†CPU or lower-end GPUs).\n\nOnce you have configured the models, click ‚ÄúRun‚Äù to begin training.\n\n\n\n\n\n\nTip\n\n\n\nFor top-down models, set a larger receptive field for the centroid model (by decreasing the ‚ÄúInput Scaling‚Äù value) than for the top-down ID model, since you only need to locate the animal, not fine details.\n\n\n\n\n\n3.6.6 Monitor training progress\nYou should now see a training progress window showing the loss curves and metrics for each model as it trains.\n\n\n\nCentroid model training progress\n\n\n\n\n\nTop-down ID model training progress\n\n\n\n\n\n\n\n\nTip\n\n\n\nSLEAP supports remote training and inference workflows, allowing users to leverage external computational resources, particularly those with GPU support, for batch training or inference. For detailed instructions and guidance, see the Running SLEAP remotely guide.\n\n\n\n\n3.6.7 Run inference and proofread predictions\nAfter the models are trained, if you had configured the training pipeline to run inference upon completion, SLEAP will automatically apply the trained models to the selected frames.\nAlternatively, to manually run inference on random frames with your trained models, go to the ‚ÄúPredict‚Äù and select ‚ÄúRun Inference‚Äù in the top menu bar.\n\n\n\n\n\n\n\nNote\n\n\n\nSince we have trained an appearance-based ID model, which outputs identity assignments directly for each detected animal in each frame, there is no need to configure a ‚ÄúTracker‚Äù (typically used for temporal-based tracking). Further details on the Tracker module is available in SLEAP‚Äôs Tracking and proofreading guide.\n\n\nLabelled frames (frames with user or predicted labels) will be marked in the seekbar.\n\nTo step through labelled frames, click on ‚ÄúGo‚Äù and ‚ÄúNext Labeled Frame‚Äù (or use Alt (Windows) / Option (Mac) + left/right).\nThe models are not expected to perform well at this stage, but you can review and correct these predictions, and merge them back into your training dataset. You can also generate more labelling suggestions and label them to ensure your training dataset captures a wider range of poses and variations. Once you have added and/or corrected more instances, you can repeat the process: train a new model, predict on more frames, correct those predictions, and so on until you are ready to apply the model on the entire video.\nSee SLEAP‚Äôs Prediction-assisted labeling guide for more details on this iterative process.\n\n\n\n\n\n\nNote\n\n\n\nPredicted instances will not be used for model training unless you correct the predictions in the GUI.\n\n\n\n\n3.6.8 Evaluate trained models\nThe metrics of trained models can be accessed by clicking on ‚ÄúPredict‚Äù then ‚ÄúEvaluation Metrics for Trained Models‚Äù in the top menu bar.\n\nSee SLEAP‚Äôs Model evaluation guide for examples on generating accuracy metrics for your trained model.\n\n\n\n\nKoolhaas Jaap M., de Boer Sietse F., Coppens Caroline M. 2013. ‚ÄúThe Resident-Intruder Paradigm: A Standardized Test for Aggression, Violence and Social Stress.‚Äù JoVE, no. 77: e4367. https://doi.org/doi:10.3791/4367.\n\n\nPereira, Talmo D., Joshua W. Shaevitz, and Mala Murthy. 2020. ‚ÄúQuantifying Behavior to Understand the Brain.‚Äù Nature Neuroscience 23 (12): 1537‚Äì49. https://doi.org/10.1038/s41593-020-00734-z.\n\n\nPereira, Talmo D., Nathaniel Tabris, Arie Matsliah, David M. Turner, Junyu Li, Shruthi Ravindranath, Eleni S. Papadoyannis, et al. 2022. ‚ÄúSLEAP: A Deep Learning System for Multi-Animal Pose Tracking.‚Äù Nature Methods 19 (4): 486‚Äì95. https://doi.org/10.1038/s41592-022-01426-1.\n\n\nSun, Jennifer J., Tomomi Karigo, David J. Anderson, Pietro Perona, Yisong Yue, and Ann Kennedy. 2021. ‚ÄúCaltech Mouse Social Interactions (CalMS21) Dataset.‚Äù CaltechDATA. https://doi.org/10.22002/D1.1991.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Pose estimation with SLEAP</span>"
    ]
  },
  {
    "objectID": "04-movement-intro.html",
    "href": "04-movement-intro.html",
    "title": "4¬† Analysing tracks with movement",
    "section": "",
    "text": "4.1 What is movement?\nIn this tutorial, we will introduce the movement package and walk through parts of its documentation.\nYou will be given a set of exercises to complete‚Äîusing movement to analyse the pose tracks you‚Äôve generated in Chapter 3 or any other tracking data you may have access to.\nIn chapters¬†1 and 2 we saw how the rise of deep learning-based markerless motion tracking tools is transforming the study of animal behaviour.\nIn Chapter 3 we dove deeper into SLEAP, a popular package for pose estimation and tracking. We saw how SLEAP and similar tools‚Äîlike DeepLabCut and LightningPose‚Äîdetect the positions of user-defined keypoints in video frames, group the keypoints into poses, and connect their identities across time into sequential collections we call pose tracks.\nThe extraction of pose tracks is often just the beginning of the analysis. Researchers use these tracks to investigate various aspects of animal behaviour, such as kinematics, spatial navigation, social interactions, etc. Typically, these analyses involve custom, project-specific scripts that are hard to reuse across different projects and are rarely maintained after the project‚Äôs conclusion.\nIn response to these challenges, we saw the need for a versatile and easy-to-use toolbox that is compatible with a range of ML-based motion tracking frameworks and supports interactive data exploration and analysis. That‚Äôs where movement comes in. We started building in early 2023 to answer the question: what can I do now with these tracks?",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Analysing tracks with movement</span>"
    ]
  },
  {
    "objectID": "04-movement-intro.html#what-is-movement",
    "href": "04-movement-intro.html#what-is-movement",
    "title": "4¬† Analysing tracks with movement",
    "section": "",
    "text": "Notemovement‚Äôs mission\n\n\n\nmovementaims to facilitate the study of animal behaviour by providing a consistent, modular interface for analysing motion tracks, enabling steps such as data cleaning, visualisation, and motion quantification.\nSee movement‚Äôs mission and scope statement for more details.\n\n\n\n\n\n\n\n\nFigure¬†4.1: Overview of the movement package",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Analysing tracks with movement</span>"
    ]
  },
  {
    "objectID": "04-movement-intro.html#a-unified-interface-for-motion-tracks",
    "href": "04-movement-intro.html#a-unified-interface-for-motion-tracks",
    "title": "4¬† Analysing tracks with movement",
    "section": "4.2 A unified interface for motion tracks",
    "text": "4.2 A unified interface for motion tracks\nmovement aims to support all popular animal tracking frameworks and file formats, in 2D and 3D, tracking single or multiple animals of any species.\nTo achieve this level of versatility, we had to identify what‚Äôs common across the outputs of motion tracking tools and how we can represent them in a standardised way.\nWhat we came up with takes the form of collections of multi-dimensional arrays‚Äîan xarray.Dataset object. Each array within a dataset is an xarray.DataArray object holding different aspects of the collected data (position, time, confidence scores‚Ä¶). You can think of an xarray.DataArray as a multi-dimensional numpy.ndarray with pandas-style indexing and labelling.\nThis may sound complicated but fear not, we‚Äôll build some understanding by exploring some example datasets that are included in the movement package.\n\nfrom movement import sample_data\n\nDownloading data from 'https://gin.g-node.org/neuroinformatics/movement-test-data/raw/master/metadata.yaml' to file '/home/runner/.movement/data/temp_metadata.yaml'.\nSHA256 hash of downloaded file: 22100193c76c0e3f274e4c986357c95fc645da1f5c67af41a8acfa91313205f4\nUse this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n\n\n\n4.2.1 Poses datasets\nFirst, let‚Äôs see how movement represents pose tracks, like the ones we get with SLEAP.\n\n\n\n\n\n\nFigure¬†4.2: The structure of a movement poses dataset\n\n\n\nLet‚Äôs load an example dataset and explore its contents.\n\nposes_ds = sample_data.fetch_dataset(\"SLEAP_two-mice_octagon.analysis.h5\")\nposes_ds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 2MB\nDimensions:      (time: 9000, space: 2, keypoints: 7, individuals: 2)\nCoordinates: (4)\nData variables:\n    position     (time, space, keypoints, individuals) float32 1MB 796.5 ... nan\n    confidence   (time, keypoints, individuals) float32 504kB 0.9292 ... 0.0\nAttributes: (6)xarray.DatasetDimensions:time: 9000space: 2keypoints: 7individuals: 2Coordinates: (4)time(time)float640.0 0.02 0.04 ... 179.9 180.0 180.0array([0.0000e+00, 2.0000e-02, 4.0000e-02, ..., 1.7994e+02, 1.7996e+02,\n       1.7998e+02], shape=(9000,))space(space)&lt;U1'x' 'y'array(['x', 'y'], dtype='&lt;U1')keypoints(keypoints)&lt;U9'Nose' 'EarLeft' ... 'TailBase'array(['Nose', 'EarLeft', 'EarRight', 'Neck', 'BodyUpper', 'BodyLower',\n       'TailBase'], dtype='&lt;U9')individuals(individuals)&lt;U1'1' '2'array(['1', '2'], dtype='&lt;U1')Data variables: (2)position(time, space, keypoints, individuals)float32796.5 724.5 812.8 ... 539.9 nanarray([[[[ 796.47314 ,  724.50464 ],\n         [ 812.7791  ,  721.03235 ],\n         [ 816.9322  ,  704.87585 ],\n         ...,\n         [ 840.7488  ,  704.7664  ],\n         [ 860.94714 ,  696.9818  ],\n         [ 891.86694 ,  696.5501  ]],\n\n        [[ 843.9411  , 1013.0088  ],\n         [ 853.02704 ,  984.7694  ],\n         [ 836.6645  ,  992.73926 ],\n         ...,\n         [ 852.37103 ,  969.5139  ],\n         [ 852.46027 ,  956.7811  ],\n         [ 852.8248  ,  940.1626  ]]],\n\n\n       [[[ 784.9976  ,  724.7622  ],\n         [ 804.41455 ,  721.14667 ],\n         [ 808.7286  ,  705.17615 ],\n...\n         [ 548.5738  ,  124.4707  ],\n         [ 539.9726  ,         nan]]],\n\n\n       [[[1168.8281  ,  628.0149  ],\n         [1164.842   ,  636.7952  ],\n         [1149.0544  ,  648.0165  ],\n         ...,\n         [1148.6505  ,  656.44324 ],\n         [1140.6172  ,  667.7969  ],\n         [1124.6349  ,         nan]],\n\n        [[ 612.5953  ,   96.91025 ],\n         [ 588.73206 ,  112.90335 ],\n         [ 596.61346 ,   96.67566 ],\n         ...,\n         [ 572.3625  ,  116.7138  ],\n         [ 552.75    ,  124.54907 ],\n         [ 539.917   ,         nan]]]],\n      shape=(9000, 2, 7, 2), dtype=float32)confidence(time, keypoints, individuals)float320.9292 0.8582 0.9183 ... 0.8343 0.0array([[[0.9292158 , 0.85823977],\n        [0.91834486, 0.9425898 ],\n        [0.9423268 , 1.0327555 ],\n        ...,\n        [1.0818218 , 1.0189738 ],\n        [0.82583773, 1.0566478 ],\n        [0.61231655, 0.7511665 ]],\n\n       [[0.87165236, 0.87277526],\n        [0.9183895 , 0.91004723],\n        [0.8998653 , 1.0173767 ],\n        ...,\n        [1.0842742 , 1.0057921 ],\n        [0.8515047 , 1.0472274 ],\n        [0.76587516, 0.7506779 ]],\n\n       [[0.83031344, 0.8633507 ],\n        [0.8614834 , 0.9128488 ],\n        [0.8628435 , 1.0143896 ],\n        ...,\n...\n        ...,\n        [0.9772766 , 0.85115355],\n        [0.9786657 , 0.61859107],\n        [0.88060284, 0.        ]],\n\n       [[0.8607382 , 0.5399062 ],\n        [0.9647669 , 0.8537044 ],\n        [0.9817054 , 0.8100933 ],\n        ...,\n        [1.0091084 , 0.8482252 ],\n        [0.9706513 , 0.6101817 ],\n        [0.8690833 , 0.        ]],\n\n       [[0.8661687 , 0.53857213],\n        [0.9934075 , 0.8632442 ],\n        [1.0423936 , 0.81270623],\n        ...,\n        [0.9675762 , 0.8551186 ],\n        [0.9172935 , 0.59800303],\n        [0.83427495, 0.        ]]], shape=(9000, 7, 2), dtype=float32)Indexes: (4)timePandasIndexPandasIndex(Index([   0.0,   0.02,   0.04,   0.06,   0.08,    0.1,   0.12,   0.14,   0.16,\n         0.18,\n       ...\n        179.8, 179.82, 179.84, 179.86, 179.88,  179.9, 179.92, 179.94, 179.96,\n       179.98],\n      dtype='float64', name='time', length=9000))spacePandasIndexPandasIndex(Index(['x', 'y'], dtype='object', name='space'))keypointsPandasIndexPandasIndex(Index(['Nose', 'EarLeft', 'EarRight', 'Neck', 'BodyUpper', 'BodyLower',\n       'TailBase'],\n      dtype='object', name='keypoints'))individualsPandasIndexPandasIndex(Index(['1', '2'], dtype='object', name='individuals'))Attributes: (6)source_software :SLEAPds_type :posesfps :50.0time_unit :secondssource_file :/home/runner/.movement/data/poses/SLEAP_two-mice_octagon.analysis.h5frame_path :/home/runner/.movement/data/frames/two-mice_octagon_frame-10sec.png\n\n\n\n\n4.2.2 Bounding boxes datasets\nmovement also supports datasets that consist of bounding boxes, like the ones we would get if we performed object detection on a video, followed by tracking identities across time.\n\n\n\n\n\n\nFigure¬†4.3: The structure of a movement bounding boxes dataset\n\n\n\n\nbboxes_ds = sample_data.fetch_dataset(\n    \"VIA_single-crab_MOCA-crab-1_linear-interp.csv\"\n)\n\n\nbboxes_ds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 8kB\nDimensions:      (time: 168, space: 2, individuals: 1)\nCoordinates: (3)\nData variables:\n    position     (time, space, individuals) float64 3kB 1.118e+03 ... 401.9\n    shape        (time, space, individuals) float64 3kB 320.1 153.2 ... 120.1\n    confidence   (time, individuals) float64 1kB nan nan nan nan ... nan nan nan\nAttributes: (6)xarray.DatasetDimensions:time: 168space: 2individuals: 1Coordinates: (3)time(time)float640.0 0.04167 0.08333 ... 6.917 6.958array([0.      , 0.041667, 0.083333, 0.125   , 0.166667, 0.208333, 0.25    ,\n       0.291667, 0.333333, 0.375   , 0.416667, 0.458333, 0.5     , 0.541667,\n       0.583333, 0.625   , 0.666667, 0.708333, 0.75    , 0.791667, 0.833333,\n       0.875   , 0.916667, 0.958333, 1.      , 1.041667, 1.083333, 1.125   ,\n       1.166667, 1.208333, 1.25    , 1.291667, 1.333333, 1.375   , 1.416667,\n       1.458333, 1.5     , 1.541667, 1.583333, 1.625   , 1.666667, 1.708333,\n       1.75    , 1.791667, 1.833333, 1.875   , 1.916667, 1.958333, 2.      ,\n       2.041667, 2.083333, 2.125   , 2.166667, 2.208333, 2.25    , 2.291667,\n       2.333333, 2.375   , 2.416667, 2.458333, 2.5     , 2.541667, 2.583333,\n       2.625   , 2.666667, 2.708333, 2.75    , 2.791667, 2.833333, 2.875   ,\n       2.916667, 2.958333, 3.      , 3.041667, 3.083333, 3.125   , 3.166667,\n       3.208333, 3.25    , 3.291667, 3.333333, 3.375   , 3.416667, 3.458333,\n       3.5     , 3.541667, 3.583333, 3.625   , 3.666667, 3.708333, 3.75    ,\n       3.791667, 3.833333, 3.875   , 3.916667, 3.958333, 4.      , 4.041667,\n       4.083333, 4.125   , 4.166667, 4.208333, 4.25    , 4.291667, 4.333333,\n       4.375   , 4.416667, 4.458333, 4.5     , 4.541667, 4.583333, 4.625   ,\n       4.666667, 4.708333, 4.75    , 4.791667, 4.833333, 4.875   , 4.916667,\n       4.958333, 5.      , 5.041667, 5.083333, 5.125   , 5.166667, 5.208333,\n       5.25    , 5.291667, 5.333333, 5.375   , 5.416667, 5.458333, 5.5     ,\n       5.541667, 5.583333, 5.625   , 5.666667, 5.708333, 5.75    , 5.791667,\n       5.833333, 5.875   , 5.916667, 5.958333, 6.      , 6.041667, 6.083333,\n       6.125   , 6.166667, 6.208333, 6.25    , 6.291667, 6.333333, 6.375   ,\n       6.416667, 6.458333, 6.5     , 6.541667, 6.583333, 6.625   , 6.666667,\n       6.708333, 6.75    , 6.791667, 6.833333, 6.875   , 6.916667, 6.958333])space(space)&lt;U1'x' 'y'array(['x', 'y'], dtype='&lt;U1')individuals(individuals)&lt;U4'id_1'array(['id_1'], dtype='&lt;U4')Data variables: (3)position(time, space, individuals)float641.118e+03 373.3 ... 614.4 401.9array([[[1117.895  ],\n        [ 373.3035 ]],\n\n       [[1113.9442 ],\n        [ 373.8679 ]],\n\n       [[1109.9934 ],\n        [ 374.4323 ]],\n\n       [[1106.0426 ],\n        [ 374.9967 ]],\n\n       [[1102.0918 ],\n        [ 375.5611 ]],\n\n       [[1098.141  ],\n        [ 376.1255 ]],\n\n       [[1093.0615 ],\n        [ 377.8187 ]],\n...\n       [[ 591.6426 ],\n        [ 398.8622 ]],\n\n       [[ 594.7062 ],\n        [ 399.0234 ]],\n\n       [[ 597.7698 ],\n        [ 399.1846 ]],\n\n       [[ 600.8334 ],\n        [ 399.3458 ]],\n\n       [[ 603.897  ],\n        [ 399.507  ]],\n\n       [[ 609.13775],\n        [ 400.7165 ]],\n\n       [[ 614.3785 ],\n        [ 401.926  ]]])shape(time, space, individuals)float64320.1 153.2 327.7 ... 249.9 120.1array([[[320.09  ],\n        [153.191 ]],\n\n       [[327.6688],\n        [154.3198]],\n\n       [[335.2476],\n        [155.4486]],\n\n       [[342.8264],\n        [156.5774]],\n\n       [[350.4052],\n        [157.7062]],\n\n       [[357.984 ],\n        [158.835 ]],\n\n       [[359.7578],\n        [159.6414]],\n...\n       [[249.2988],\n        [121.4236]],\n\n       [[248.6536],\n        [121.1012]],\n\n       [[248.0084],\n        [120.7788]],\n\n       [[247.3632],\n        [120.4564]],\n\n       [[246.718 ],\n        [120.134 ]],\n\n       [[248.3315],\n        [120.134 ]],\n\n       [[249.945 ],\n        [120.134 ]]])confidence(time, individuals)float64nan nan nan nan ... nan nan nan nanarray([[nan],\n       [nan],\n       [nan],\n       [nan],\n       [nan],\n       [nan],\n       [nan],\n       [nan],\n       [nan],\n       [nan],\n       [nan],\n       [nan],\n       [nan],\n       [nan],\n       [nan],\n       [nan],\n       [nan],\n       [nan],\n       [nan],\n       [nan],\n...\n       [nan],\n       [nan],\n       [nan],\n       [nan],\n       [nan],\n       [nan],\n       [nan],\n       [nan],\n       [nan],\n       [nan],\n       [nan],\n       [nan],\n       [nan],\n       [nan],\n       [nan],\n       [nan],\n       [nan],\n       [nan],\n       [nan],\n       [nan]])Indexes: (3)timePandasIndexPandasIndex(Index([                 0.0, 0.041666666666666664,  0.08333333333333333,\n                      0.125,  0.16666666666666666,  0.20833333333333334,\n                       0.25,   0.2916666666666667,   0.3333333333333333,\n                      0.375,\n       ...\n          6.583333333333333,                6.625,    6.666666666666667,\n          6.708333333333333,                 6.75,    6.791666666666667,\n          6.833333333333333,                6.875,    6.916666666666667,\n          6.958333333333333],\n      dtype='float64', name='time', length=168))spacePandasIndexPandasIndex(Index(['x', 'y'], dtype='object', name='space'))individualsPandasIndexPandasIndex(Index(['id_1'], dtype='object', name='individuals'))Attributes: (6)source_software :VIA-tracksds_type :bboxesfps :24.0time_unit :secondssource_file :/home/runner/.movement/data/bboxes/VIA_single-crab_MOCA-crab-1_linear-interp.csvframe_path :/home/runner/.movement/data/frames/single-crab_MOCA-crab-1_frame-0.jpg\n\n\n\n\n\n\n\n\nTipDiscuss\n\n\n\n\nWhat are some limitations of movement‚Äôs approach? What kinds of data cannot be accommodated?\nCan you think of alternative ways of representing these data?\n\nSee the documentation on movement datasets for more details on movement data structures.\n\n\n\n\n4.2.3 Working with xarray objects\nSince movement represents motion tracking data as xarray objects, we can use all of xarray‚Äôs intuitive interface and rich built-in functionalities for data manipulation and analysis.\nAccessing data variables and attributes (metadata) is straightforward:\n\nprint(f\"Source software: {poses_ds.source_software}\")\nprint(f\"Frames per second: {poses_ds.fps}\")\n\nposes_ds.position\n\nSource software: SLEAP\nFrames per second: 50.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'position' (time: 9000, space: 2, keypoints: 7, individuals: 2)&gt; Size: 1MB\n796.5 724.5 812.8 721.0 816.9 704.9 821.1 ... 572.4 116.7 552.8 124.5 539.9 nan\nCoordinates: (4)xarray.DataArray'position'time: 9000space: 2keypoints: 7individuals: 2796.5 724.5 812.8 721.0 816.9 704.9 ... 116.7 552.8 124.5 539.9 nanarray([[[[ 796.47314 ,  724.50464 ],\n         [ 812.7791  ,  721.03235 ],\n         [ 816.9322  ,  704.87585 ],\n         ...,\n         [ 840.7488  ,  704.7664  ],\n         [ 860.94714 ,  696.9818  ],\n         [ 891.86694 ,  696.5501  ]],\n\n        [[ 843.9411  , 1013.0088  ],\n         [ 853.02704 ,  984.7694  ],\n         [ 836.6645  ,  992.73926 ],\n         ...,\n         [ 852.37103 ,  969.5139  ],\n         [ 852.46027 ,  956.7811  ],\n         [ 852.8248  ,  940.1626  ]]],\n\n\n       [[[ 784.9976  ,  724.7622  ],\n         [ 804.41455 ,  721.14667 ],\n         [ 808.7286  ,  705.17615 ],\n...\n         [ 548.5738  ,  124.4707  ],\n         [ 539.9726  ,         nan]]],\n\n\n       [[[1168.8281  ,  628.0149  ],\n         [1164.842   ,  636.7952  ],\n         [1149.0544  ,  648.0165  ],\n         ...,\n         [1148.6505  ,  656.44324 ],\n         [1140.6172  ,  667.7969  ],\n         [1124.6349  ,         nan]],\n\n        [[ 612.5953  ,   96.91025 ],\n         [ 588.73206 ,  112.90335 ],\n         [ 596.61346 ,   96.67566 ],\n         ...,\n         [ 572.3625  ,  116.7138  ],\n         [ 552.75    ,  124.54907 ],\n         [ 539.917   ,         nan]]]],\n      shape=(9000, 2, 7, 2), dtype=float32)Coordinates: (4)time(time)float640.0 0.02 0.04 ... 179.9 180.0 180.0array([0.0000e+00, 2.0000e-02, 4.0000e-02, ..., 1.7994e+02, 1.7996e+02,\n       1.7998e+02], shape=(9000,))space(space)&lt;U1'x' 'y'array(['x', 'y'], dtype='&lt;U1')keypoints(keypoints)&lt;U9'Nose' 'EarLeft' ... 'TailBase'array(['Nose', 'EarLeft', 'EarRight', 'Neck', 'BodyUpper', 'BodyLower',\n       'TailBase'], dtype='&lt;U9')individuals(individuals)&lt;U1'1' '2'array(['1', '2'], dtype='&lt;U1')Indexes: (4)timePandasIndexPandasIndex(Index([   0.0,   0.02,   0.04,   0.06,   0.08,    0.1,   0.12,   0.14,   0.16,\n         0.18,\n       ...\n        179.8, 179.82, 179.84, 179.86, 179.88,  179.9, 179.92, 179.94, 179.96,\n       179.98],\n      dtype='float64', name='time', length=9000))spacePandasIndexPandasIndex(Index(['x', 'y'], dtype='object', name='space'))keypointsPandasIndexPandasIndex(Index(['Nose', 'EarLeft', 'EarRight', 'Neck', 'BodyUpper', 'BodyLower',\n       'TailBase'],\n      dtype='object', name='keypoints'))individualsPandasIndexPandasIndex(Index(['1', '2'], dtype='object', name='individuals'))Attributes: (0)\n\n\nWe can select a subset of data along any dimension in a variety of ways: by integer index (order) or coordinate label.\n\n# First individual, first time point\nposes_ds.position.isel(individuals=0, time=0)\n\n# 0-10 seconds, two specific keypoints\nposes_ds.position.sel(time=slice(0, 10), keypoints=[\"EarLeft\", \"EarRight\"])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'position' (time: 501, space: 2, keypoints: 2, individuals: 2)&gt; Size: 16kB\n812.8 721.0 816.9 704.9 853.0 984.8 ... 359.9 340.0 852.2 811.8 835.9 800.6\nCoordinates: (4)xarray.DataArray'position'time: 501space: 2keypoints: 2individuals: 2812.8 721.0 816.9 704.9 853.0 984.8 ... 340.0 852.2 811.8 835.9 800.6array([[[[812.7791 , 721.03235],\n         [816.9322 , 704.87585]],\n\n        [[853.02704, 984.7694 ],\n         [836.6645 , 992.73926]]],\n\n\n       [[[804.41455, 721.14667],\n         [808.7286 , 705.17615]],\n\n        [[852.35284, 984.18274],\n         [836.1602 , 992.3926 ]]],\n\n\n       [[[795.77686, 721.2672 ],\n         [800.2279 , 705.41705]],\n\n        [[849.3385 , 980.82336],\n         [833.1362 , 988.9888 ]]],\n\n...\n\n       [[[364.0884 , 356.805  ],\n         [359.95502, 340.36093]],\n\n        [[851.93097, 812.11383],\n         [835.68134, 804.3751 ]]],\n\n\n       [[[364.20483, 356.02448],\n         [360.05804, 340.04025]],\n\n        [[852.05334, 811.99725],\n         [835.7557 , 800.7991 ]]],\n\n\n       [[[364.10153, 355.91064],\n         [359.89383, 339.9605 ]],\n\n        [[852.17737, 811.76544],\n         [835.8717 , 800.5599 ]]]], shape=(501, 2, 2, 2), dtype=float32)Coordinates: (4)time(time)float640.0 0.02 0.04 ... 9.96 9.98 10.0array([ 0.  ,  0.02,  0.04, ...,  9.96,  9.98, 10.  ], shape=(501,))space(space)&lt;U1'x' 'y'array(['x', 'y'], dtype='&lt;U1')keypoints(keypoints)&lt;U9'EarLeft' 'EarRight'array(['EarLeft', 'EarRight'], dtype='&lt;U9')individuals(individuals)&lt;U1'1' '2'array(['1', '2'], dtype='&lt;U1')Indexes: (4)timePandasIndexPandasIndex(Index([ 0.0, 0.02, 0.04, 0.06, 0.08,  0.1, 0.12, 0.14, 0.16, 0.18,\n       ...\n       9.82, 9.84, 9.86, 9.88,  9.9, 9.92, 9.94, 9.96, 9.98, 10.0],\n      dtype='float64', name='time', length=501))spacePandasIndexPandasIndex(Index(['x', 'y'], dtype='object', name='space'))keypointsPandasIndexPandasIndex(Index(['EarLeft', 'EarRight'], dtype='object', name='keypoints'))individualsPandasIndexPandasIndex(Index(['1', '2'], dtype='object', name='individuals'))Attributes: (0)\n\n\nWe can also do all sorts of computations on the data, along any dimension.\n\n# Each point's median confidence score across time\nposes_ds.confidence.median(dim=\"time\")\n\n# Take the block mean for every 10 frames.\nposes_ds.position.coarsen(time=10, boundary=\"trim\").mean()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'position' (time: 900, space: 2, keypoints: 7, individuals: 2)&gt; Size: 101kB\n757.0 733.4 771.6 725.0 779.1 711.3 ... 553.9 116.5 544.9 124.2 541.0 128.6\nCoordinates: (4)xarray.DataArray'position'time: 900space: 2keypoints: 7individuals: 2757.0 733.4 771.6 725.0 779.1 711.3 ... 116.5 544.9 124.2 541.0 128.6array([[[[ 756.962   ,  733.3821  ],\n         [ 771.5825  ,  724.9951  ],\n         [ 779.1285  ,  711.3096  ],\n         ...,\n         [ 798.9844  ,  707.4451  ],\n         [ 818.64166 ,  698.38007 ],\n         [ 848.1536  ,  695.7847  ]],\n\n        [[ 824.01636 , 1003.74567 ],\n         [ 838.2208  ,  979.5837  ],\n         [ 823.0009  ,  989.7912  ],\n         ...,\n         [ 841.6796  ,  968.4575  ],\n         [ 844.6781  ,  955.7819  ],\n         [ 848.83325 ,  938.32935 ]]],\n\n\n       [[[ 682.5006  ,  767.1878  ],\n         [ 694.0696  ,  751.6518  ],\n         [ 704.2795  ,  747.1867  ],\n...\n         [ 544.6389  ,  124.39598 ],\n         [ 542.1672  ,         nan]]],\n\n\n       [[[1177.5676  ,  626.08044 ],\n         [1165.5343  ,  636.6763  ],\n         [1157.4653  ,  645.02875 ],\n         ...,\n         [1147.251   ,  656.4906  ],\n         [1135.7996  ,  667.9657  ],\n         [1119.3905  ,  680.6386  ]],\n\n        [[ 577.2025  ,   99.16615 ],\n         [ 558.778   ,  112.888275],\n         [ 572.0012  ,   99.46965 ],\n         ...,\n         [ 553.9388  ,  116.50942 ],\n         [ 544.8991  ,  124.15985 ],\n         [ 541.02795 ,  128.59134 ]]]],\n      shape=(900, 2, 7, 2), dtype=float32)Coordinates: (4)time(time)float640.09 0.29 0.49 ... 179.7 179.9array([9.0000e-02, 2.9000e-01, 4.9000e-01, ..., 1.7949e+02, 1.7969e+02,\n       1.7989e+02], shape=(900,))space(space)&lt;U1'x' 'y'array(['x', 'y'], dtype='&lt;U1')keypoints(keypoints)&lt;U9'Nose' 'EarLeft' ... 'TailBase'array(['Nose', 'EarLeft', 'EarRight', 'Neck', 'BodyUpper', 'BodyLower',\n       'TailBase'], dtype='&lt;U9')individuals(individuals)&lt;U1'1' '2'array(['1', '2'], dtype='&lt;U1')Indexes: (4)timePandasIndexPandasIndex(Index([0.09000000000000001,                0.29, 0.49000000000000005,\n        0.6900000000000001,                0.89,                1.09,\n        1.2899999999999998,  1.4899999999999998,  1.6900000000000002,\n                      1.89,\n       ...\n                    178.09,  178.29000000000002,  178.48999999999998,\n                    178.69,  178.89000000000004,              179.09,\n        179.29000000000002,  179.48999999999998,              179.69,\n        179.89000000000004],\n      dtype='float64', name='time', length=900))spacePandasIndexPandasIndex(Index(['x', 'y'], dtype='object', name='space'))keypointsPandasIndexPandasIndex(Index(['Nose', 'EarLeft', 'EarRight', 'Neck', 'BodyUpper', 'BodyLower',\n       'TailBase'],\n      dtype='object', name='keypoints'))individualsPandasIndexPandasIndex(Index(['1', '2'], dtype='object', name='individuals'))Attributes: (0)\n\n\nxarray also provides a rich set of built-in plotting methods for visualising the data.\n\nfrom matplotlib import pyplot as plt\n\ntail_base_pos = poses_ds.sel(keypoints=\"TailBase\").position\ntail_base_pos.plot.line(\n    x=\"time\", row=\"individuals\", hue=\"space\", aspect=2, size=2.5\n)\nplt.show()\n\n\n\n\n\n\n\nFigure¬†4.4: The x,y spatial coordinates of the TailBase keypoint across time\n\n\n\n\n\nYou can also combine those with matplotlib figures.\n\ncolors = plt.cm.tab10.colors\n\nfig, ax = plt.subplots()\nfor kp, color in zip(poses_ds.keypoints, colors):\n    data = poses_ds.confidence.sel(keypoints=kp)\n    data.plot.hist(\n        bins=50, histtype=\"step\", density=True, ax=ax, color=color, label=kp\n    )\nax.set_ylabel(\"Density\")\nax.set_title(\"Confidence histograms per keypoint\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\nFigure¬†4.5: Confidence histograms per keypoint\n\n\n\n\n\nYou may also want to export date to structures you may be more familiar with, such as Pandas DataFrames or NumPy arrays.\nExport the position data array as a pandas DataFrame:\n\nposition_df = poses_ds.position.to_dataframe(\n    dim_order=[\"time\", \"individuals\", \"keypoints\", \"space\"]\n)\nposition_df.head()\n\n\n\n\n\n\n\n\n\n\n\nposition\n\n\ntime\nindividuals\nkeypoints\nspace\n\n\n\n\n\n0.0\n1\nNose\nx\n796.473145\n\n\ny\n843.941101\n\n\nEarLeft\nx\n812.779114\n\n\ny\n853.027039\n\n\nEarRight\nx\n816.932190\n\n\n\n\n\n\n\nExport data variables or coordinates as numpy arrays:\n\nposition_array = poses_ds.position.values\nprint(f\"Position array shape: {position_array.shape}\")\n\ntime_array = poses_ds.time.values\nprint(f\"Time array shape: {time_array.shape}\")\n\nPosition array shape: (9000, 2, 7, 2)\nTime array shape: (9000,)\n\n\nFor saving datasets to disk, we recommend leveraging xarray‚Äôs built-in support for the netCDF file format.\nimport xarray as xr\n\n# To save a dataset to disk\nposes_ds.to_netcdf(\"poses_ds.nc\")\n\n# To load the dataset back from memory\nposes_ds = xr.open_dataset(\"poses_ds.nc\")",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Analysing tracks with movement</span>"
    ]
  },
  {
    "objectID": "04-movement-intro.html#sec-movement-load",
    "href": "04-movement-intro.html#sec-movement-load",
    "title": "4¬† Analysing tracks with movement",
    "section": "4.3 Load and explore data",
    "text": "4.3 Load and explore data\nAs stated above, our goal with movement is to enable pipelines that are input-agnostic, meaning they are not tied to a specific motion tracking tool or data format. Therefore, movement offers input/output functions that facilitate data flows between various motion tracking frameworks and movement‚Äôs own xarray data structure.\nPlease refer to the Input/Output section of the movement documentation for more details, including a full list of supported formats.\n\n\n\n\n\n\nTipExercise A\n\n\n\n\nLoad the predictions you generated in Chapter 3 into a movement dataset. Alternatively, feel free to work with the CalMS21/mouse044_task1_annotator1.slp file from Dropbox (refer to prerequisites¬†A.4) or any of movement‚Äôs sample datasets.\nCompute the overall minimum and maximum x,y positions.\nSelect a narrow time window (e.g.¬†10 seconds) and plot the x, y positions of a certain keypoint across time.\nPlot the centroid trajectory of a given individual across time.\n\nBonus: Overlay the centroid trajectory (task 4) on top of a frame extracted from the video. You may find inspiration in the ‚ÄúPupil tracking‚Äù example.\nUseful resources:\n\nInput/Output guide\nThe ‚ÄúLoad and explore pose tracks‚Äù example\nplot_centroid_trajectory() function\n\n\n\n\n\n\n\n\n\nTipClick to reveal the answers\n\n\n\n\n\nLoading pose tracks from a file:\n\nfrom pathlib import Path\nfrom movement.io import load_poses\n\nfile_name = \"mouse044_task1_annotator1.slp\"\nfile_path = Path.home() / \".movement\" / \"CalMS21\" / file_name\n\nds = load_poses.from_file(file_path, source_software=\"SLEAP\", fps=30)\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 598kB\nDimensions:      (time: 3394, space: 2, keypoints: 7, individuals: 2)\nCoordinates: (4)\nData variables:\n    position     (time, space, keypoints, individuals) float32 380kB 854.4 .....\n    confidence   (time, keypoints, individuals) float32 190kB nan nan ... nan\nAttributes: (5)xarray.DatasetDimensions:time: 3394space: 2keypoints: 7individuals: 2Coordinates: (4)time(time)float640.0 0.03333 0.06667 ... 113.1 113.1array([0.000000e+00, 3.333333e-02, 6.666667e-02, ..., 1.130333e+02,\n       1.130667e+02, 1.131000e+02], shape=(3394,))space(space)&lt;U1'x' 'y'array(['x', 'y'], dtype='&lt;U1')keypoints(keypoints)&lt;U9'nose' 'right_ear' ... 'tail_base'array(['nose', 'right_ear', 'left_ear', 'neck', 'right_hip', 'left_hip',\n       'tail_base'], dtype='&lt;U9')individuals(individuals)&lt;U10'resident_b' 'intruder_w'array(['resident_b', 'intruder_w'], dtype='&lt;U10')Data variables: (2)position(time, space, keypoints, individuals)float32854.4 736.3 807.4 ... 129.1 522.5array([[[[854.4422 , 736.25885],\n         [807.4422 , 752.25885],\n         [833.4422 , 804.25885],\n         ...,\n         [734.4422 , 775.25885],\n         [785.4422 , 779.25885],\n         [721.4422 , 815.25885]],\n\n        [[441.49527, 519.94037],\n         [437.49527, 471.9404 ],\n         [395.49527, 492.9404 ],\n         ...,\n         [405.49527, 420.9404 ],\n         [336.49527, 419.9404 ],\n         [336.49527, 385.9404 ]]],\n\n\n       [[[858.35284, 716.67053],\n         [810.35284, 752.67053],\n         [836.35284, 798.67053],\n...\n         [105.46051, 486.70868],\n         [170.46051, 486.70868],\n         [124.46051, 521.7087 ]]],\n\n\n       [[[162.83447, 892.6651 ],\n         [189.83447, 899.6651 ],\n         [223.83447, 839.6651 ],\n         ...,\n         [304.83447, 908.6651 ],\n         [315.83447, 840.6651 ],\n         [375.83447, 872.6651 ]],\n\n        [[209.14272, 359.4882 ],\n         [159.14272, 424.4882 ],\n         [206.14272, 403.4882 ],\n         ...,\n         [107.14272, 487.4882 ],\n         [173.14272, 487.4882 ],\n         [129.14272, 522.48816]]]], shape=(3394, 2, 7, 2), dtype=float32)confidence(time, keypoints, individuals)float32nan nan nan nan ... nan nan nan nanarray([[[nan, nan],\n        [nan, nan],\n        [nan, nan],\n        ...,\n        [nan, nan],\n        [nan, nan],\n        [nan, nan]],\n\n       [[nan, nan],\n        [nan, nan],\n        [nan, nan],\n        ...,\n        [nan, nan],\n        [nan, nan],\n        [nan, nan]],\n\n       [[nan, nan],\n        [nan, nan],\n        [nan, nan],\n        ...,\n...\n        ...,\n        [nan, nan],\n        [nan, nan],\n        [nan, nan]],\n\n       [[nan, nan],\n        [nan, nan],\n        [nan, nan],\n        ...,\n        [nan, nan],\n        [nan, nan],\n        [nan, nan]],\n\n       [[nan, nan],\n        [nan, nan],\n        [nan, nan],\n        ...,\n        [nan, nan],\n        [nan, nan],\n        [nan, nan]]], shape=(3394, 7, 2), dtype=float32)Indexes: (4)timePandasIndexPandasIndex(Index([                0.0, 0.03333333333333333, 0.06666666666666667,\n                       0.1, 0.13333333333333333, 0.16666666666666666,\n                       0.2, 0.23333333333333334, 0.26666666666666666,\n                       0.3,\n       ...\n                     112.8,  112.83333333333333,  112.86666666666666,\n                     112.9,  112.93333333333334,  112.96666666666667,\n                     113.0,  113.03333333333333,  113.06666666666666,\n                     113.1],\n      dtype='float64', name='time', length=3394))spacePandasIndexPandasIndex(Index(['x', 'y'], dtype='object', name='space'))keypointsPandasIndexPandasIndex(Index(['nose', 'right_ear', 'left_ear', 'neck', 'right_hip', 'left_hip',\n       'tail_base'],\n      dtype='object', name='keypoints'))individualsPandasIndexPandasIndex(Index(['resident_b', 'intruder_w'], dtype='object', name='individuals'))Attributes: (5)source_software :SLEAPds_type :posesfps :30.0time_unit :secondssource_file :/home/runner/.movement/CalMS21/mouse044_task1_annotator1.slp\n\n\nComputing the minimum and maximum x,y positions:\n\nfor space_coord in [\"x\", \"y\"]:\n    min_pos = ds.position.sel(space=space_coord).min().values\n    max_pos = ds.position.sel(space=space_coord).max().values\n    print(f\"Min-Max {space_coord} positions: {min_pos:.2f}-{max_pos:.2f}\")\n\nMin-Max x positions: 83.79-988.99\nMin-Max y positions: 64.72-563.30\n\n\nPlotting the x,y positions of a certain keypoint across time, within a narrow time window:\n\nds.position.sel(keypoints=\"tail_base\", time=slice(0, 10)).plot.line(\n    x=\"time\", row=\"individuals\", hue=\"space\", aspect=2, size=2.5\n)\n\n\n\n\n\n\n\n\nPlotting the centroid trajectory:\n\nfrom movement.plots import plot_centroid_trajectory\n\nfig, ax = plt.subplots(figsize=(8, 4))\nplot_centroid_trajectory(ds.position, individual=\"resident_b\", ax=ax)\nplt.show()\n\n\n\n\n\n\n\n\nAs a bonus, we can also overlay that trajectory on top of a video frame.\n\nimport sleap_io as sio\n\n\nvideo_path = Path.home() / \".movement\" / \"CalMS21\" / \"mouse044_task1_annotator1.mp4\"\nvideo = sio.load_video(video_path)\n\nn_frames, height, width, channels = video.shape\nprint(f\"Number of frames: {n_frames}\")\nprint(f\"Frame size: {width}x{height}\")\nprint(f\"Number of channels: {channels}\\n\")\n\n# Extract the first frame to use as background\nbackground = video[0]\n\nfig, ax = plt.subplots(figsize=(8, 4))\n\n# Plot the first video frame\nax.imshow(background, cmap=\"gray\")\n\n# Plot the centroid trajectory\nplot_centroid_trajectory(\n    ds.position, individual=\"resident_b\", ax=ax, alpha=0.75, s=5,\n)\n\nplt.show()\n\nNumber of frames: 3394\nFrame size: 1024x570\nNumber of channels: 3",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Analysing tracks with movement</span>"
    ]
  },
  {
    "objectID": "04-movement-intro.html#sec-movement-gui",
    "href": "04-movement-intro.html#sec-movement-gui",
    "title": "4¬† Analysing tracks with movement",
    "section": "4.4 Visualise motion tracks interactively",
    "text": "4.4 Visualise motion tracks interactively\nThe movement graphical user interface (GUI), powered by our custom plugin for napari, makes it easy to view and explore motion tracks. Currently, you can use it to visualise 2D movement datasets as points, tracks, and rectangular bounding boxes (if defined) overlaid on video frames.\nWe‚Äôll first demonstrate how the GUI works by using it to explore the CalMS21 dataset and some of the sample data we can fetch with movement.\n\n\n\n\n\n\nFigure¬†4.6: Data from the CalMS21 dataset viewed via the movement plugin for napari\n\n\n\nAfter that you are free to play with the GUI on your own. You may try to explore the predictions you generated in Chapter 3, or any other tracking data you have at hand and is supported by movement.\nConsult the GUI user guide in the movement documentation for more details.\n\n\n\n\n\n\nTipDiscuss\n\n\n\n\nDid you spot any errors in the data you‚Äôve explored?\nWhat kinds of errors do you expect in pose tracks?\nWhat are some common sources of errors?\nHow can such errors be avoided or corrected?",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Analysing tracks with movement</span>"
    ]
  },
  {
    "objectID": "04-movement-intro.html#sec-movement-filtering",
    "href": "04-movement-intro.html#sec-movement-filtering",
    "title": "4¬† Analysing tracks with movement",
    "section": "4.5 Clean motion tracks",
    "text": "4.5 Clean motion tracks\nIn this section, we will walk through the tools movement offers for dealing with errors in motion tracking. These tools are implemented in the movement.filtering module and include functions for:\n\nidentifying and removing outliers\ninterpolating missing data\nsmoothing trajectories over time\n\nWe will go through two notebooks from movement‚Äôs example gallery:\n\nDrop outliers and interpolate\nSmooth pose tracks\n\nTo run these examples as interactive Jupyter notebooks, you can go to end of each example and either:\n\nClick the Download Jupyter Notebook button and open the .ipynb file in your code editor of choice (recommended), or\nClick the launch binder button and run the notebook in your browser.\n\n\n\n\n\n\n\nTipExercise B\n\n\n\nFor this exercise you may continue working with the dataset you used to solve Exercise A in Section 4.3, or load another one. You should store each intermediate output as a data variable within the same dataset.\n\nDrop position values that are below a certain confidence threshold.\nSmooth the data across time using rolling median filter.\nInterpolate missing values across time.\nSave the dataset, including the processed position data, to a netCDF file.\n\nRefer to the filtering module documentation and to the examples we went through above.\nEach filtering step involves selecting one or more parameters. We encourage you to experiment with different parameters values and inspect their effects by plotting the data.\n\n\n\n\n\n\n\n\nTipClick to reveal the answers\n\n\n\n\n\nWe will work with the same ‚ÄúSLEAP_two-mice_octagon.analysis.h5‚Äù dataset we loaded in Section 4.2.1.\n\nds_oct = sample_data.fetch_dataset(\"SLEAP_two-mice_octagon.analysis.h5\")\nds_oct\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 2MB\nDimensions:      (time: 9000, space: 2, keypoints: 7, individuals: 2)\nCoordinates: (4)\nData variables:\n    position     (time, space, keypoints, individuals) float32 1MB 796.5 ... nan\n    confidence   (time, keypoints, individuals) float32 504kB 0.9292 ... 0.0\nAttributes: (6)xarray.DatasetDimensions:time: 9000space: 2keypoints: 7individuals: 2Coordinates: (4)time(time)float640.0 0.02 0.04 ... 179.9 180.0 180.0array([0.0000e+00, 2.0000e-02, 4.0000e-02, ..., 1.7994e+02, 1.7996e+02,\n       1.7998e+02], shape=(9000,))space(space)&lt;U1'x' 'y'array(['x', 'y'], dtype='&lt;U1')keypoints(keypoints)&lt;U9'Nose' 'EarLeft' ... 'TailBase'array(['Nose', 'EarLeft', 'EarRight', 'Neck', 'BodyUpper', 'BodyLower',\n       'TailBase'], dtype='&lt;U9')individuals(individuals)&lt;U1'1' '2'array(['1', '2'], dtype='&lt;U1')Data variables: (2)position(time, space, keypoints, individuals)float32796.5 724.5 812.8 ... 539.9 nanarray([[[[ 796.47314 ,  724.50464 ],\n         [ 812.7791  ,  721.03235 ],\n         [ 816.9322  ,  704.87585 ],\n         ...,\n         [ 840.7488  ,  704.7664  ],\n         [ 860.94714 ,  696.9818  ],\n         [ 891.86694 ,  696.5501  ]],\n\n        [[ 843.9411  , 1013.0088  ],\n         [ 853.02704 ,  984.7694  ],\n         [ 836.6645  ,  992.73926 ],\n         ...,\n         [ 852.37103 ,  969.5139  ],\n         [ 852.46027 ,  956.7811  ],\n         [ 852.8248  ,  940.1626  ]]],\n\n\n       [[[ 784.9976  ,  724.7622  ],\n         [ 804.41455 ,  721.14667 ],\n         [ 808.7286  ,  705.17615 ],\n...\n         [ 548.5738  ,  124.4707  ],\n         [ 539.9726  ,         nan]]],\n\n\n       [[[1168.8281  ,  628.0149  ],\n         [1164.842   ,  636.7952  ],\n         [1149.0544  ,  648.0165  ],\n         ...,\n         [1148.6505  ,  656.44324 ],\n         [1140.6172  ,  667.7969  ],\n         [1124.6349  ,         nan]],\n\n        [[ 612.5953  ,   96.91025 ],\n         [ 588.73206 ,  112.90335 ],\n         [ 596.61346 ,   96.67566 ],\n         ...,\n         [ 572.3625  ,  116.7138  ],\n         [ 552.75    ,  124.54907 ],\n         [ 539.917   ,         nan]]]],\n      shape=(9000, 2, 7, 2), dtype=float32)confidence(time, keypoints, individuals)float320.9292 0.8582 0.9183 ... 0.8343 0.0array([[[0.9292158 , 0.85823977],\n        [0.91834486, 0.9425898 ],\n        [0.9423268 , 1.0327555 ],\n        ...,\n        [1.0818218 , 1.0189738 ],\n        [0.82583773, 1.0566478 ],\n        [0.61231655, 0.7511665 ]],\n\n       [[0.87165236, 0.87277526],\n        [0.9183895 , 0.91004723],\n        [0.8998653 , 1.0173767 ],\n        ...,\n        [1.0842742 , 1.0057921 ],\n        [0.8515047 , 1.0472274 ],\n        [0.76587516, 0.7506779 ]],\n\n       [[0.83031344, 0.8633507 ],\n        [0.8614834 , 0.9128488 ],\n        [0.8628435 , 1.0143896 ],\n        ...,\n...\n        ...,\n        [0.9772766 , 0.85115355],\n        [0.9786657 , 0.61859107],\n        [0.88060284, 0.        ]],\n\n       [[0.8607382 , 0.5399062 ],\n        [0.9647669 , 0.8537044 ],\n        [0.9817054 , 0.8100933 ],\n        ...,\n        [1.0091084 , 0.8482252 ],\n        [0.9706513 , 0.6101817 ],\n        [0.8690833 , 0.        ]],\n\n       [[0.8661687 , 0.53857213],\n        [0.9934075 , 0.8632442 ],\n        [1.0423936 , 0.81270623],\n        ...,\n        [0.9675762 , 0.8551186 ],\n        [0.9172935 , 0.59800303],\n        [0.83427495, 0.        ]]], shape=(9000, 7, 2), dtype=float32)Indexes: (4)timePandasIndexPandasIndex(Index([   0.0,   0.02,   0.04,   0.06,   0.08,    0.1,   0.12,   0.14,   0.16,\n         0.18,\n       ...\n        179.8, 179.82, 179.84, 179.86, 179.88,  179.9, 179.92, 179.94, 179.96,\n       179.98],\n      dtype='float64', name='time', length=9000))spacePandasIndexPandasIndex(Index(['x', 'y'], dtype='object', name='space'))keypointsPandasIndexPandasIndex(Index(['Nose', 'EarLeft', 'EarRight', 'Neck', 'BodyUpper', 'BodyLower',\n       'TailBase'],\n      dtype='object', name='keypoints'))individualsPandasIndexPandasIndex(Index(['1', '2'], dtype='object', name='individuals'))Attributes: (6)source_software :SLEAPds_type :posesfps :50.0time_unit :secondssource_file :/home/runner/.movement/data/poses/SLEAP_two-mice_octagon.analysis.h5frame_path :/home/runner/.movement/data/frames/two-mice_octagon_frame-10sec.png\n\n\nConsulting Figure¬†4.5, we decide on a confidence threshold of 0.8. This decision is always somewhat arbitrary, and confidence scores are not usually comparable across different tracking tools and datasets.\nWe can now drop the position values that are below the threshold:\n\nfrom movement.filtering import (\n    filter_by_confidence,\n    rolling_filter,\n    interpolate_over_time,\n)\n\nconfidence_threshold = 0.8\n\nds_oct[\"position_filtered\"] = filter_by_confidence(\n    ds_oct.position,\n    ds_oct.confidence,\n    threshold=confidence_threshold,\n    print_report=True\n)\n\nMissing points (marked as NaN) in input:\n\nkeypoints                Nose           EarLeft          EarRight              Neck         BodyUpper         BodyLower          TailBase\nindividuals                                                                                                                              \n1            107/9000 (1.19%)     9/9000 (0.1%)     9/9000 (0.1%)     9/9000 (0.1%)     9/9000 (0.1%)     9/9000 (0.1%)   44/9000 (0.49%)\n2            272/9000 (3.02%)  260/9000 (2.89%)  260/9000 (2.89%)  260/9000 (2.89%)  260/9000 (2.89%)  260/9000 (2.89%)  392/9000 (4.36%)\nMissing points (marked as NaN) in output:\n\nkeypoints                  Nose             EarLeft            EarRight              Neck         BodyUpper           BodyLower            TailBase\nindividuals                                                                                                                                        \n1            3972/9000 (44.13%)  1441/9000 (16.01%)  1200/9000 (13.33%)  390/9000 (4.33%)     9/9000 (0.1%)    520/9000 (5.78%)  3337/9000 (37.08%)\n2            5143/9000 (57.14%)  1826/9000 (20.29%)  1324/9000 (14.71%)  489/9000 (5.43%)  524/9000 (5.82%)  1382/9000 (15.36%)  4899/9000 (54.43%)\n\n\nSmoothing the data with a rolling median filter:\n\nds_oct[\"position_smoothed\"] = rolling_filter(\n    ds_oct.position_filtered,\n    window=5,\n    statistic=\"median\",\n    min_periods=2,\n    print_report=True\n)\n\nMissing points (marked as NaN) in input:\n\nkeypoints                  Nose             EarLeft            EarRight              Neck         BodyUpper           BodyLower            TailBase\nindividuals                                                                                                                                        \n1            3972/9000 (44.13%)  1441/9000 (16.01%)  1200/9000 (13.33%)  390/9000 (4.33%)     9/9000 (0.1%)    520/9000 (5.78%)  3337/9000 (37.08%)\n2            5143/9000 (57.14%)  1826/9000 (20.29%)  1324/9000 (14.71%)  489/9000 (5.43%)  524/9000 (5.82%)  1382/9000 (15.36%)  4899/9000 (54.43%)\nMissing points (marked as NaN) in output:\n\nkeypoints                  Nose             EarLeft            EarRight              Neck         BodyUpper          BodyLower            TailBase\nindividuals                                                                                                                                       \n1            3583/9000 (39.81%)  1196/9000 (13.29%)  1019/9000 (11.32%)   270/9000 (3.0%)    4/9000 (0.04%)   349/9000 (3.88%)  2821/9000 (31.34%)\n2            4741/9000 (52.68%)  1493/9000 (16.59%)   994/9000 (11.04%)  314/9000 (3.49%)  341/9000 (3.79%)  970/9000 (10.78%)  4296/9000 (47.73%)\n\n\nInterpolating missing values across time:\n\nds_oct[\"position_interpolated\"] = interpolate_over_time(\n    ds_oct.position_smoothed,\n    method=\"linear\",\n    max_gap=10,\n    print_report=True\n)\n\nMissing points (marked as NaN) in input:\n\nkeypoints                  Nose             EarLeft            EarRight              Neck         BodyUpper          BodyLower            TailBase\nindividuals                                                                                                                                       \n1            3583/9000 (39.81%)  1196/9000 (13.29%)  1019/9000 (11.32%)   270/9000 (3.0%)    4/9000 (0.04%)   349/9000 (3.88%)  2821/9000 (31.34%)\n2            4741/9000 (52.68%)  1493/9000 (16.59%)   994/9000 (11.04%)  314/9000 (3.49%)  341/9000 (3.79%)  970/9000 (10.78%)  4296/9000 (47.73%)\nMissing points (marked as NaN) in output:\n\nkeypoints                  Nose             EarLeft          EarRight             Neck         BodyUpper         BodyLower            TailBase\nindividuals                                                                                                                                   \n1            3394/9000 (37.71%)  1009/9000 (11.21%)  886/9000 (9.84%)  180/9000 (2.0%)     0/9000 (0.0%)  196/9000 (2.18%)  2456/9000 (27.29%)\n2             4500/9000 (50.0%)  1263/9000 (14.03%)  750/9000 (8.33%)  162/9000 (1.8%)  205/9000 (2.28%)  670/9000 (7.44%)  3808/9000 (42.31%)\n\n\nThe dataset now contains all intermediate processing steps.\n\nds_oct\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 5MB\nDimensions:                (time: 9000, space: 2, keypoints: 7, individuals: 2)\nCoordinates: (4)\nData variables:\n    position               (time, space, keypoints, individuals) float32 1MB ...\n    confidence             (time, keypoints, individuals) float32 504kB 0.929...\n    position_filtered      (time, space, keypoints, individuals) float32 1MB ...\n    position_smoothed      (time, space, keypoints, individuals) float32 1MB ...\n    position_interpolated  (time, space, keypoints, individuals) float32 1MB ...\nAttributes: (6)xarray.DatasetDimensions:time: 9000space: 2keypoints: 7individuals: 2Coordinates: (4)time(time)float640.0 0.02 0.04 ... 179.9 180.0 180.0array([0.0000e+00, 2.0000e-02, 4.0000e-02, ..., 1.7994e+02, 1.7996e+02,\n       1.7998e+02], shape=(9000,))space(space)&lt;U1'x' 'y'array(['x', 'y'], dtype='&lt;U1')keypoints(keypoints)&lt;U9'Nose' 'EarLeft' ... 'TailBase'array(['Nose', 'EarLeft', 'EarRight', 'Neck', 'BodyUpper', 'BodyLower',\n       'TailBase'], dtype='&lt;U9')individuals(individuals)&lt;U1'1' '2'array(['1', '2'], dtype='&lt;U1')Data variables: (5)position(time, space, keypoints, individuals)float32796.5 724.5 812.8 ... 539.9 nanarray([[[[ 796.47314 ,  724.50464 ],\n         [ 812.7791  ,  721.03235 ],\n         [ 816.9322  ,  704.87585 ],\n         ...,\n         [ 840.7488  ,  704.7664  ],\n         [ 860.94714 ,  696.9818  ],\n         [ 891.86694 ,  696.5501  ]],\n\n        [[ 843.9411  , 1013.0088  ],\n         [ 853.02704 ,  984.7694  ],\n         [ 836.6645  ,  992.73926 ],\n         ...,\n         [ 852.37103 ,  969.5139  ],\n         [ 852.46027 ,  956.7811  ],\n         [ 852.8248  ,  940.1626  ]]],\n\n\n       [[[ 784.9976  ,  724.7622  ],\n         [ 804.41455 ,  721.14667 ],\n         [ 808.7286  ,  705.17615 ],\n...\n         [ 548.5738  ,  124.4707  ],\n         [ 539.9726  ,         nan]]],\n\n\n       [[[1168.8281  ,  628.0149  ],\n         [1164.842   ,  636.7952  ],\n         [1149.0544  ,  648.0165  ],\n         ...,\n         [1148.6505  ,  656.44324 ],\n         [1140.6172  ,  667.7969  ],\n         [1124.6349  ,         nan]],\n\n        [[ 612.5953  ,   96.91025 ],\n         [ 588.73206 ,  112.90335 ],\n         [ 596.61346 ,   96.67566 ],\n         ...,\n         [ 572.3625  ,  116.7138  ],\n         [ 552.75    ,  124.54907 ],\n         [ 539.917   ,         nan]]]],\n      shape=(9000, 2, 7, 2), dtype=float32)confidence(time, keypoints, individuals)float320.9292 0.8582 0.9183 ... 0.8343 0.0array([[[0.9292158 , 0.85823977],\n        [0.91834486, 0.9425898 ],\n        [0.9423268 , 1.0327555 ],\n        ...,\n        [1.0818218 , 1.0189738 ],\n        [0.82583773, 1.0566478 ],\n        [0.61231655, 0.7511665 ]],\n\n       [[0.87165236, 0.87277526],\n        [0.9183895 , 0.91004723],\n        [0.8998653 , 1.0173767 ],\n        ...,\n        [1.0842742 , 1.0057921 ],\n        [0.8515047 , 1.0472274 ],\n        [0.76587516, 0.7506779 ]],\n\n       [[0.83031344, 0.8633507 ],\n        [0.8614834 , 0.9128488 ],\n        [0.8628435 , 1.0143896 ],\n        ...,\n...\n        ...,\n        [0.9772766 , 0.85115355],\n        [0.9786657 , 0.61859107],\n        [0.88060284, 0.        ]],\n\n       [[0.8607382 , 0.5399062 ],\n        [0.9647669 , 0.8537044 ],\n        [0.9817054 , 0.8100933 ],\n        ...,\n        [1.0091084 , 0.8482252 ],\n        [0.9706513 , 0.6101817 ],\n        [0.8690833 , 0.        ]],\n\n       [[0.8661687 , 0.53857213],\n        [0.9934075 , 0.8632442 ],\n        [1.0423936 , 0.81270623],\n        ...,\n        [0.9675762 , 0.8551186 ],\n        [0.9172935 , 0.59800303],\n        [0.83427495, 0.        ]]], shape=(9000, 7, 2), dtype=float32)position_filtered(time, space, keypoints, individuals)float32796.5 724.5 812.8 ... nan 539.9 nanlog :[\n  {\n    \"operation\": \"filter_by_confidence\",\n    \"datetime\": \"2025-10-01 14:15:00.942167\",\n    \"confidence\": \"&lt;xarray.DataArray 'confidence' (time: 9000, keypoints: 7, individuals: 2)&gt; Size: 504kB\\n0.9292 0.8582 0.9183 0.9426 0.9423 1.033 ... 0.8551 0.9173 0.598 0.8343 0.0\\nCoordinates: (3)\",\n    \"threshold\": \"0.8\",\n    \"print_report\": \"True\"\n  }\n]array([[[[ 796.47314 ,  724.50464 ],\n         [ 812.7791  ,  721.03235 ],\n         [ 816.9322  ,  704.87585 ],\n         ...,\n         [ 840.7488  ,  704.7664  ],\n         [ 860.94714 ,  696.9818  ],\n         [        nan,         nan]],\n\n        [[ 843.9411  , 1013.0088  ],\n         [ 853.02704 ,  984.7694  ],\n         [ 836.6645  ,  992.73926 ],\n         ...,\n         [ 852.37103 ,  969.5139  ],\n         [ 852.46027 ,  956.7811  ],\n         [        nan,         nan]]],\n\n\n       [[[ 784.9976  ,  724.7622  ],\n         [ 804.41455 ,  721.14667 ],\n         [ 808.7286  ,  705.17615 ],\n...\n         [ 548.5738  ,         nan],\n         [ 539.9726  ,         nan]]],\n\n\n       [[[1168.8281  ,         nan],\n         [1164.842   ,  636.7952  ],\n         [1149.0544  ,  648.0165  ],\n         ...,\n         [1148.6505  ,  656.44324 ],\n         [1140.6172  ,         nan],\n         [1124.6349  ,         nan]],\n\n        [[ 612.5953  ,         nan],\n         [ 588.73206 ,  112.90335 ],\n         [ 596.61346 ,   96.67566 ],\n         ...,\n         [ 572.3625  ,  116.7138  ],\n         [ 552.75    ,         nan],\n         [ 539.917   ,         nan]]]],\n      shape=(9000, 2, 7, 2), dtype=float32)position_smoothed(time, space, keypoints, individuals)float32785.0 724.8 804.4 ... nan 539.9 nanlog :[\n  {\n    \"operation\": \"filter_by_confidence\",\n    \"datetime\": \"2025-10-01 14:15:00.942167\",\n    \"confidence\": \"&lt;xarray.DataArray 'confidence' (time: 9000, keypoints: 7, individuals: 2)&gt; Size: 504kB\\n0.9292 0.8582 0.9183 0.9426 0.9423 1.033 ... 0.8551 0.9173 0.598 0.8343 0.0\\nCoordinates: (3)\",\n    \"threshold\": \"0.8\",\n    \"print_report\": \"True\"\n  },\n  {\n    \"operation\": \"rolling_filter\",\n    \"datetime\": \"2025-10-01 14:15:00.971660\",\n    \"window\": \"5\",\n    \"statistic\": \"'median'\",\n    \"min_periods\": \"2\",\n    \"print_report\": \"True\"\n  }\n]array([[[[ 784.9976 ,  724.7622 ],\n         [ 804.41455,  721.14667],\n         [ 808.7286 ,  705.17615],\n         ...,\n         [ 832.52625,  704.7664 ],\n         [ 852.7732 ,  696.9818 ],\n         [ 872.16034,        nan]],\n\n        [[ 840.1928 , 1012.284  ],\n         [ 852.35284,  984.18274],\n         [ 836.1602 ,  992.3926 ],\n         ...,\n         [ 852.06903,  969.25543],\n         [ 852.23706,  956.54346],\n         [ 849.3308 ,        nan]]],\n\n\n       [[[ 784.9976 ,  724.7622 ],\n         [ 804.41455,  721.14667],\n         [ 808.7286 ,  705.17615],\n...\n         [ 564.36743,  116.69415],\n         [ 548.5738 ,        nan],\n         [ 539.9726 ,        nan]]],\n\n\n       [[[1168.8281 ,        nan],\n         [1164.7733 ,  636.7952 ],\n         [1149.0544 ,  644.9634 ],\n         ...,\n         [1148.2814 ,  656.49194],\n         [1139.8783 ,        nan],\n         [1120.4642 ,        nan]],\n\n        [[ 601.106  ,        nan],\n         [ 580.44836,  112.90335],\n         [ 588.3836 ,   99.76018],\n         ...,\n         [ 564.36743,  116.69415],\n         [ 548.5738 ,        nan],\n         [ 539.917  ,        nan]]]], shape=(9000, 2, 7, 2), dtype=float32)position_interpolated(time, space, keypoints, individuals)float32785.0 724.8 804.4 ... nan 539.9 nanlog :[\n  {\n    \"operation\": \"filter_by_confidence\",\n    \"datetime\": \"2025-10-01 14:15:00.942167\",\n    \"confidence\": \"&lt;xarray.DataArray 'confidence' (time: 9000, keypoints: 7, individuals: 2)&gt; Size: 504kB\\n0.9292 0.8582 0.9183 0.9426 0.9423 1.033 ... 0.8551 0.9173 0.598 0.8343 0.0\\nCoordinates: (3)\",\n    \"threshold\": \"0.8\",\n    \"print_report\": \"True\"\n  },\n  {\n    \"operation\": \"rolling_filter\",\n    \"datetime\": \"2025-10-01 14:15:00.971660\",\n    \"window\": \"5\",\n    \"statistic\": \"'median'\",\n    \"min_periods\": \"2\",\n    \"print_report\": \"True\"\n  },\n  {\n    \"operation\": \"interpolate_over_time\",\n    \"datetime\": \"2025-10-01 14:15:01.456842\",\n    \"method\": \"'linear'\",\n    \"max_gap\": \"10\",\n    \"print_report\": \"True\"\n  }\n]array([[[[ 784.9976 ,  724.7622 ],\n         [ 804.41455,  721.14667],\n         [ 808.7286 ,  705.17615],\n         ...,\n         [ 832.52625,  704.7664 ],\n         [ 852.7732 ,  696.9818 ],\n         [ 872.16034,        nan]],\n\n        [[ 840.1928 , 1012.284  ],\n         [ 852.35284,  984.18274],\n         [ 836.1602 ,  992.3926 ],\n         ...,\n         [ 852.06903,  969.25543],\n         [ 852.23706,  956.54346],\n         [ 849.3308 ,        nan]]],\n\n\n       [[[ 784.9976 ,  724.7622 ],\n         [ 804.41455,  721.14667],\n         [ 808.7286 ,  705.17615],\n...\n         [ 564.36743,  116.69415],\n         [ 548.5738 ,        nan],\n         [ 539.9726 ,        nan]]],\n\n\n       [[[1168.8281 ,        nan],\n         [1164.7733 ,  636.7952 ],\n         [1149.0544 ,  644.9634 ],\n         ...,\n         [1148.2814 ,  656.49194],\n         [1139.8783 ,        nan],\n         [1120.4642 ,        nan]],\n\n        [[ 601.106  ,        nan],\n         [ 580.44836,  112.90335],\n         [ 588.3836 ,   99.76018],\n         ...,\n         [ 564.36743,  116.69415],\n         [ 548.5738 ,        nan],\n         [ 539.917  ,        nan]]]], shape=(9000, 2, 7, 2), dtype=float32)Indexes: (4)timePandasIndexPandasIndex(Index([   0.0,   0.02,   0.04,   0.06,   0.08,    0.1,   0.12,   0.14,   0.16,\n         0.18,\n       ...\n        179.8, 179.82, 179.84, 179.86, 179.88,  179.9, 179.92, 179.94, 179.96,\n       179.98],\n      dtype='float64', name='time', length=9000))spacePandasIndexPandasIndex(Index(['x', 'y'], dtype='object', name='space'))keypointsPandasIndexPandasIndex(Index(['Nose', 'EarLeft', 'EarRight', 'Neck', 'BodyUpper', 'BodyLower',\n       'TailBase'],\n      dtype='object', name='keypoints'))individualsPandasIndexPandasIndex(Index(['1', '2'], dtype='object', name='individuals'))Attributes: (6)source_software :SLEAPds_type :posesfps :50.0time_unit :secondssource_file :/home/runner/.movement/data/poses/SLEAP_two-mice_octagon.analysis.h5frame_path :/home/runner/.movement/data/frames/two-mice_octagon_frame-10sec.png\n\n\nWe can even inspect the log of the final position data array:\n\nprint(ds_oct.position_interpolated.log)\n\n[\n  {\n    \"operation\": \"filter_by_confidence\",\n    \"datetime\": \"2025-10-01 14:15:00.942167\",\n    \"confidence\": \"&lt;xarray.DataArray 'confidence' (time: 9000, keypoints: 7, individuals: 2)&gt; Size: 504kB\\n0.9292 0.8582 0.9183 0.9426 0.9423 1.033 ... 0.8551 0.9173 0.598 0.8343 0.0\\nCoordinates: (3)\",\n    \"threshold\": \"0.8\",\n    \"print_report\": \"True\"\n  },\n  {\n    \"operation\": \"rolling_filter\",\n    \"datetime\": \"2025-10-01 14:15:00.971660\",\n    \"window\": \"5\",\n    \"statistic\": \"'median'\",\n    \"min_periods\": \"2\",\n    \"print_report\": \"True\"\n  },\n  {\n    \"operation\": \"interpolate_over_time\",\n    \"datetime\": \"2025-10-01 14:15:01.456842\",\n    \"method\": \"'linear'\",\n    \"max_gap\": \"10\",\n    \"print_report\": \"True\"\n  }\n]\n\n\nLet‚Äôs pick a keypoint and plot its position across time for every processing step. To make the plotting a bit easier, we‚Äôll stack the four position data arrays across a new dimension called step.\n\nposition_all_steps = xr.concat(\n    [\n        ds_oct.position,\n        ds_oct.position_filtered,\n        ds_oct.position_smoothed,\n        ds_oct.position_interpolated\n    ],\n    dim=\"step\"\n).assign_coords(step=[\"original\", \"filtered\", \"smoothed\", \"interpolated\"])\n\nposition_all_steps\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'position' (step: 4, time: 9000, space: 2, keypoints: 7,\n                              individuals: 2)&gt; Size: 4MB\n796.5 724.5 812.8 721.0 816.9 704.9 821.1 ... 564.4 116.7 548.6 nan 539.9 nan\nCoordinates: (5)xarray.DataArray'position'step: 4time: 9000space: 2keypoints: 7individuals: 2796.5 724.5 812.8 721.0 816.9 704.9 ... 116.7 548.6 nan 539.9 nanarray([[[[[ 796.47314 ,  724.50464 ],\n          [ 812.7791  ,  721.03235 ],\n          [ 816.9322  ,  704.87585 ],\n          ...,\n          [ 840.7488  ,  704.7664  ],\n          [ 860.94714 ,  696.9818  ],\n          [ 891.86694 ,  696.5501  ]],\n\n         [[ 843.9411  , 1013.0088  ],\n          [ 853.02704 ,  984.7694  ],\n          [ 836.6645  ,  992.73926 ],\n          ...,\n          [ 852.37103 ,  969.5139  ],\n          [ 852.46027 ,  956.7811  ],\n          [ 852.8248  ,  940.1626  ]]],\n\n\n        [[[ 784.9976  ,  724.7622  ],\n          [ 804.41455 ,  721.14667 ],\n          [ 808.7286  ,  705.17615 ],\n...\n          [ 548.5738  ,         nan],\n          [ 539.9726  ,         nan]]],\n\n\n        [[[1168.8281  ,         nan],\n          [1164.7733  ,  636.7952  ],\n          [1149.0544  ,  644.9634  ],\n          ...,\n          [1148.2814  ,  656.49194 ],\n          [1139.8783  ,         nan],\n          [1120.4642  ,         nan]],\n\n         [[ 601.106   ,         nan],\n          [ 580.44836 ,  112.90335 ],\n          [ 588.3836  ,   99.76018 ],\n          ...,\n          [ 564.36743 ,  116.69415 ],\n          [ 548.5738  ,         nan],\n          [ 539.917   ,         nan]]]]],\n      shape=(4, 9000, 2, 7, 2), dtype=float32)Coordinates: (5)time(time)float640.0 0.02 0.04 ... 179.9 180.0 180.0array([0.0000e+00, 2.0000e-02, 4.0000e-02, ..., 1.7994e+02, 1.7996e+02,\n       1.7998e+02], shape=(9000,))space(space)&lt;U1'x' 'y'array(['x', 'y'], dtype='&lt;U1')keypoints(keypoints)&lt;U9'Nose' 'EarLeft' ... 'TailBase'array(['Nose', 'EarLeft', 'EarRight', 'Neck', 'BodyUpper', 'BodyLower',\n       'TailBase'], dtype='&lt;U9')individuals(individuals)&lt;U1'1' '2'array(['1', '2'], dtype='&lt;U1')step(step)&lt;U12'original' ... 'interpolated'array(['original', 'filtered', 'smoothed', 'interpolated'], dtype='&lt;U12')Indexes: (5)timePandasIndexPandasIndex(Index([   0.0,   0.02,   0.04,   0.06,   0.08,    0.1,   0.12,   0.14,   0.16,\n         0.18,\n       ...\n        179.8, 179.82, 179.84, 179.86, 179.88,  179.9, 179.92, 179.94, 179.96,\n       179.98],\n      dtype='float64', name='time', length=9000))spacePandasIndexPandasIndex(Index(['x', 'y'], dtype='object', name='space'))keypointsPandasIndexPandasIndex(Index(['Nose', 'EarLeft', 'EarRight', 'Neck', 'BodyUpper', 'BodyLower',\n       'TailBase'],\n      dtype='object', name='keypoints'))individualsPandasIndexPandasIndex(Index(['1', '2'], dtype='object', name='individuals'))stepPandasIndexPandasIndex(Index(['original', 'filtered', 'smoothed', 'interpolated'], dtype='object', name='step'))Attributes: (0)\n\n\nLet‚Äôs plot the position of the EarLeft keypoint across time for every step.\n\nposition_all_steps.sel(individuals=\"1\", keypoints=\"EarLeft\").plot.line(\n    x=\"time\", row=\"step\", hue=\"space\", aspect=2, size=2.5\n)\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Analysing tracks with movement</span>"
    ]
  },
  {
    "objectID": "04-movement-intro.html#sec-movement-kinematics",
    "href": "04-movement-intro.html#sec-movement-kinematics",
    "title": "4¬† Analysing tracks with movement",
    "section": "4.6 Quantify motion",
    "text": "4.6 Quantify motion\nIn this section, we will familiarise ourselves with the movement.kinematics module, which provides functions for deriving various useful quantities from motion tracks, such as velocity, acceleration, distances, orientations, and angles.\nAs in the previous section, we will go through a specific example from movement‚Äôs example gallery:\n\nCompute and visualise kinematics\n\nYou can run this notebook interactively in the same way as in Section 4.5.\n\n\n\n\n\n\nTipExercise C\n\n\n\nFor this exercise you may continue working with the dataset you used to solve Exercise B in Section 4.5. In fact, you can use the fully processed position data as your starting point.\n\nTake the mean position across keypoints (each individual‚Äôs centroid).\nCompute and plot the centroid speed across time.\nCompute the distance travelled by each individual within a certain time window.\nCompute the distance between two individuals (or between two keypoints) and plot it across time.\n\nRefer to the kinematics module documentation and to the example we went through above.\n\n\n\n\n\n\n\n\nTipClick to reveal the answers\n\n\n\n\n\nWe will continue working with the ds_oct from the previous exercise, using the position_interpolated data variable as our starting point.\nLet‚Äôs first import the functions we will use:\n\nfrom movement.kinematics import (\n    compute_speed,\n    compute_path_length,\n    compute_pairwise_distances,\n)\n\nTo compute the centroid position and speed:\n\nds_oct[\"centroid_position\"] = ds_oct.position_interpolated.mean(dim=\"keypoints\")\nds_oct[\"centroid_speed\"] = compute_speed(ds_oct.centroid_position)\n\nds_oct.centroid_speed\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'centroid_speed' (time: 9000, individuals: 2)&gt; Size: 72kB\n401.3 23.79 384.8 41.19 508.6 73.54 629.7 ... 282.9 25.0 132.6 2.656 1.139 0.0\nCoordinates: (2)xarray.DataArray'centroid_speed'time: 9000individuals: 2401.3 23.79 384.8 41.19 508.6 73.54 ... 25.0 132.6 2.656 1.139 0.0array([[401.2876   ,  23.7854   ],\n       [384.7978   ,  41.19397  ],\n       [508.56317  ,  73.54425  ],\n       ...,\n       [282.94922  ,  25.000525 ],\n       [132.57616  ,   2.656464 ],\n       [  1.1394137,   0.       ]], shape=(9000, 2), dtype=float32)Coordinates: (2)time(time)float640.0 0.02 0.04 ... 179.9 180.0 180.0array([0.0000e+00, 2.0000e-02, 4.0000e-02, ..., 1.7994e+02, 1.7996e+02,\n       1.7998e+02], shape=(9000,))individuals(individuals)&lt;U1'1' '2'array(['1', '2'], dtype='&lt;U1')Indexes: (2)timePandasIndexPandasIndex(Index([   0.0,   0.02,   0.04,   0.06,   0.08,    0.1,   0.12,   0.14,   0.16,\n         0.18,\n       ...\n        179.8, 179.82, 179.84, 179.86, 179.88,  179.9, 179.92, 179.94, 179.96,\n       179.98],\n      dtype='float64', name='time', length=9000))individualsPandasIndexPandasIndex(Index(['1', '2'], dtype='object', name='individuals'))Attributes: (0)\n\n\nTo plot it across time:\n\nds_oct.centroid_speed.plot.line(x=\"time\", row=\"individuals\", aspect=2, size=2.5)\nplt.show()\n\n\n\n\n\n\n\n\nTo compute the distance travelled by each individual within a certain time window:\n\nds_oct[\"path_length\"] = compute_path_length(\n    ds_oct.centroid_position.sel(time=slice(50, 100))\n)\n\nds_oct.path_length\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'path_length' (individuals: 2)&gt; Size: 8B\n1.047e+04 1.094e+04\nCoordinates: (1)\nAttributes: (1)xarray.DataArray'path_length'individuals: 21.047e+04 1.094e+04array([10465.438, 10936.398], dtype=float32)Coordinates: (1)individuals(individuals)&lt;U1'1' '2'array(['1', '2'], dtype='&lt;U1')Indexes: (1)individualsPandasIndexPandasIndex(Index(['1', '2'], dtype='object', name='individuals'))Attributes: (1)log :[\n  {\n    \"operation\": \"filter_by_confidence\",\n    \"datetime\": \"2025-10-01 14:15:00.942167\",\n    \"confidence\": \"&lt;xarray.DataArray 'confidence' (time: 9000, keypoints: 7, individuals: 2)&gt; Size: 504kB\\n0.9292 0.8582 0.9183 0.9426 0.9423 1.033 ... 0.8551 0.9173 0.598 0.8343 0.0\\nCoordinates: (3)\",\n    \"threshold\": \"0.8\",\n    \"print_report\": \"True\"\n  },\n  {\n    \"operation\": \"rolling_filter\",\n    \"datetime\": \"2025-10-01 14:15:00.971660\",\n    \"window\": \"5\",\n    \"statistic\": \"'median'\",\n    \"min_periods\": \"2\",\n    \"print_report\": \"True\"\n  },\n  {\n    \"operation\": \"interpolate_over_time\",\n    \"datetime\": \"2025-10-01 14:15:01.456842\",\n    \"method\": \"'linear'\",\n    \"max_gap\": \"10\",\n    \"print_report\": \"True\"\n  }\n]\n\n\nWe will compute the distance between the centroids of the two individuals and plot it across time:\n\nds_oct[\"inter_individual_distance\"] = compute_pairwise_distances(\n    ds_oct.centroid_position,\n    dim=\"individuals\",\n    pairs={\"1\": \"2\"}\n)\n\nds_oct.inter_individual_distance\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'inter_individual_distance' (time: 9000)&gt; Size: 72kB\n177.2 172.2 168.7 162.3 159.0 157.3 ... 672.7 674.2 675.6 679.4 682.9 683.0\nCoordinates: (1)\nAttributes: (1)xarray.DataArray'inter_individual_distance'time: 9000177.2 172.2 168.7 162.3 159.0 157.3 ... 674.2 675.6 679.4 682.9 683.0array([177.22497485, 172.19095439, 168.69327442, ..., 679.41812698,\n       682.94437438, 682.95474153], shape=(9000,))Coordinates: (1)time(time)float640.0 0.02 0.04 ... 179.9 180.0 180.0array([0.0000e+00, 2.0000e-02, 4.0000e-02, ..., 1.7994e+02, 1.7996e+02,\n       1.7998e+02], shape=(9000,))Indexes: (1)timePandasIndexPandasIndex(Index([   0.0,   0.02,   0.04,   0.06,   0.08,    0.1,   0.12,   0.14,   0.16,\n         0.18,\n       ...\n        179.8, 179.82, 179.84, 179.86, 179.88,  179.9, 179.92, 179.94, 179.96,\n       179.98],\n      dtype='float64', name='time', length=9000))Attributes: (1)log :[\n  {\n    \"operation\": \"filter_by_confidence\",\n    \"datetime\": \"2025-10-01 14:15:00.942167\",\n    \"confidence\": \"&lt;xarray.DataArray 'confidence' (time: 9000, keypoints: 7, individuals: 2)&gt; Size: 504kB\\n0.9292 0.8582 0.9183 0.9426 0.9423 1.033 ... 0.8551 0.9173 0.598 0.8343 0.0\\nCoordinates: (3)\",\n    \"threshold\": \"0.8\",\n    \"print_report\": \"True\"\n  },\n  {\n    \"operation\": \"rolling_filter\",\n    \"datetime\": \"2025-10-01 14:15:00.971660\",\n    \"window\": \"5\",\n    \"statistic\": \"'median'\",\n    \"min_periods\": \"2\",\n    \"print_report\": \"True\"\n  },\n  {\n    \"operation\": \"interpolate_over_time\",\n    \"datetime\": \"2025-10-01 14:15:01.456842\",\n    \"method\": \"'linear'\",\n    \"max_gap\": \"10\",\n    \"print_report\": \"True\"\n  }\n]\n\n\nTo plot the inter-individual distance across time:\n\nds_oct.inter_individual_distance.plot.line(x=\"time\", aspect=2, size=2.5)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nIf you wish to learn more about quantifying motion with movement, including things we didn‚Äôt cover here, the following example notebooks are good follow-ups:\n\nCompute head direction: a good place to learn about orientations and angles\nPupil tracking: an interesting application of kinematics to analyse mouse eye movements\n\nThe following two chapters‚ÄîChapter 5 and Chapter 6‚Äîconstitute case studies in which we apply movement to some real-world datasets. The two case studies represent quite different applications:\n\nChapter 5: continuous home cage monitoring data tracking mice for months\nChapter 6: collective escape events in a herd of zebras, recorded via a drone",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Analysing tracks with movement</span>"
    ]
  },
  {
    "objectID": "04-movement-intro.html#sec-movement-next",
    "href": "04-movement-intro.html#sec-movement-next",
    "title": "4¬† Analysing tracks with movement",
    "section": "4.7 Join the movement",
    "text": "4.7 Join the movement\nThis chapter does not represent a full list of movement‚Äôs current and future capabilities. The package will continue to evolve as it‚Äôs being actively developed by a core team of engineers (aka the authors of this book) supported by a growing, global community of contributors.\nWe are committed to openness and transparency and always welcome feedback and contributions from the community, especially from practicing animal behaviour researchers, to shape the project‚Äôs direction.\nVisit the movement community page to find about ways to get help and get involved.\n\n\n\n\n\n\nTipDiscuss\n\n\n\n\nWhat do you think of the movement package? What aspects of it could be improved?\nWhat is currently missing? What types of analyses would you like for movement to support?\n\nIf you have ideas, tell us about them on Zulip, open an issue on GitHub, or suggest a project for the hackday!",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Analysing tracks with movement</span>"
    ]
  },
  {
    "objectID": "05-movement-mouse.html",
    "href": "05-movement-mouse.html",
    "title": "5¬† A mouse‚Äôs daily activity log",
    "section": "",
    "text": "5.1 Import libraries\nIn this case study, we‚Äôll be using the movement package to dive into mouse home cage monitoring data acquired in Smart-Kages and tracked with DeepLabCut. We‚Äôll explore how mouse activity levels fluctuate throughout the day.\nBefore you get started, make sure you‚Äôve set up the animals-in-motion-env environment (refer to prerequisites¬†A.3.3) and are using it to run this code. You‚Äôll also need to download the Smart-Kages.zip archive from Dropbox (see prerequisites¬†A.4) and unzip it.\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport xarray as xr\nfrom matplotlib import colormaps\n\nfrom movement import sample_data\nfrom movement.filtering import filter_by_confidence, rolling_filter\nfrom movement.kinematics import compute_speed\nfrom movement.plots import plot_occupancy\nfrom movement.roi import PolygonOfInterest\nfrom movement.transforms import scale\n\nDownloading data from 'https://gin.g-node.org/neuroinformatics/movement-test-data/raw/master/metadata.yaml' to file '/home/runner/.movement/data/temp_metadata.yaml'.\nSHA256 hash of downloaded file: 22100193c76c0e3f274e4c986357c95fc645da1f5c67af41a8acfa91313205f4\nUse this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>A mouse's daily activity log</span>"
    ]
  },
  {
    "objectID": "05-movement-mouse.html#the-smart-kages-dataset",
    "href": "05-movement-mouse.html#the-smart-kages-dataset",
    "title": "5¬† A mouse‚Äôs daily activity log",
    "section": "5.2 The Smart-Kages dataset",
    "text": "5.2 The Smart-Kages dataset\n\n\n\n\n\n\nNoteAcknowledgement\n\n\n\nThis dataset was kindly shared by Dr.¬†Loukia Katsouri from the O‚ÄôKeefe Lab, with permission to use for this workshop.\n\n\nThe Smart-Kages dataset comprises home cage recordings from two mice, each housed in a specialised Smart-Kage (Ho et al. 2023)‚Äîa home cage monitoring system equipped with a camera mounted atop the cage.\nThe camera captures data around the clock at a rate of 2 frames per second, saving a video segment for each hour of the day. A pre-trained DeepLabCut model is then used to predict 8 keypoints on the mouse‚Äôs body.\nLet‚Äôs examine the contents of the downloaded data. You will need to specify the path to the unzipped Smart-Kages folder on your machine.\n\n# Replace with the path to the unzipped Smart-Kages folder on your machine\nsmart_kages_path = Path.home() / \".movement\" / \"Smart-Kages\"\n\n# Let's visualise the contents of the folder\nfiles = [f.name for f in smart_kages_path.iterdir()]\nfiles.sort()\nfor file in files:\n    print(file)\n\n.DS_Store\nkage14.nc\nkage14_background.png\nkage17.nc\nkage17_background.png\n\n\nThe tracking data are stored in two.nc (netCDF) files: kage14 and kage17. netCDF is an HDF5-based file format that can be natively saved/loaded by the xarray library, and is therefore convenient to use with movement.\nApart from these, we also have two .png files: kage14_background.png and kage17_background.png, which constitute frames extracted from the videos.\nLet‚Äôs take a look at them.\n\n\nCode\nkages = [\"kage14\", \"kage17\"]\nimg_paths = [smart_kages_path / f\"{kage}_background.png\" for kage in kages]\nimages = [plt.imread(img_path) for img_path in img_paths]\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(8, 4))\n\nfor i, img in enumerate(images):\n    axes[i].imshow(img)\n    axes[i].set_title(f\"{kages[i]}\")\n    axes[i].axis(\"off\")\n\n\n\n\n\n\n\n\nFigure¬†5.1: Top-down camera views of the Smart-Kage habitats\n\n\n\n\n\n\n\n\n\n\n\nTipQuestions A\n\n\n\n\nWhat objects do you see in the habitat?\nWhat challenges do you anticipate with tracking a mouse in this environment?\nWhat are the trade-offs one has to consider when designing a continuous monitoring system?\n\n\n\n\n\n\n\n\n\nTipClick to reveal the answers\n\n\n\n\n\n\nThe habitat contains several objects, including the mouse‚Äôs nest, a running wheel, a climbing platform, a toilet paper roll, and a corridor with two arms near the top.\nThe presence of these objects is likely to cause occlusions, meaning the mouse or parts of it may frequently be obscured from the camera‚Äôs view.\nThe requirements for animal welfare, such as providing a rich environment, often conflict with the needs of the tracking system. While a single black mouse on a white background would be straightforward to track, such conditions would not support the mouse‚Äôs well-being over time, nor would they yield naturalistic behavior.\n\n\n\n\nLet‚Äôs load and inspect the tracking data:\n\nds_kages = {}  # a dictionary to store kage name -&gt; xarray dataset\n\nfor kage in [\"kage14\", \"kage17\"]:\n    ds_kages[kage] = xr.open_dataset(smart_kages_path / f\"{kage}.nc\")\n\nds_kages[\"kage14\"]   # Change to \"kage17\" to inspect the other dataset\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 1GB\nDimensions:          (time: 5236793, space: 2, keypoints: 8, individuals: 1)\nCoordinates: (5)\nData variables:\n    position         (time, space, keypoints, individuals) float64 670MB ...\n    confidence       (time, keypoints, individuals) float64 335MB ...\nAttributes: (7)xarray.DatasetDimensions:time: 5236793space: 2keypoints: 8individuals: 1Coordinates: (5)space(space)&lt;U1'x' 'y'array(['x', 'y'], dtype='&lt;U1')keypoints(keypoints)&lt;U10'snout' 'leftear' ... 'tailbase'array(['snout', 'leftear', 'rightear', 'neck', 'spine1', 'bodycenter',\n       'spine2', 'tailbase'], dtype='&lt;U10')individuals(individuals)&lt;U12'individual_0'array(['individual_0'], dtype='&lt;U12')time(time)datetime64[ns]2024-04-08T13:55:40 ... 2024-05-...array(['2024-04-08T13:55:40.000000000', '2024-04-08T13:55:40.499044000',\n       '2024-04-08T13:55:40.998088000', ..., '2024-05-10T07:59:58.501344000',\n       '2024-05-10T07:59:59.001274000', '2024-05-10T07:59:59.501205000'],\n      shape=(5236793,), dtype='datetime64[ns]')seconds_elapsed(time)float64...[5236793 values with dtype=float64]Data variables: (2)position(time, space, keypoints, individuals)float64...[83788688 values with dtype=float64]confidence(time, keypoints, individuals)float64...[41894344 values with dtype=float64]Indexes: (4)spacePandasIndexPandasIndex(Index(['x', 'y'], dtype='object', name='space'))keypointsPandasIndexPandasIndex(Index(['snout', 'leftear', 'rightear', 'neck', 'spine1', 'bodycenter',\n       'spine2', 'tailbase'],\n      dtype='object', name='keypoints'))individualsPandasIndexPandasIndex(Index(['individual_0'], dtype='object', name='individuals'))timePandasIndexPandasIndex(DatetimeIndex([       '2024-04-08 13:55:40', '2024-04-08 13:55:40.499044',\n               '2024-04-08 13:55:40.998088', '2024-04-08 13:55:41.497133',\n               '2024-04-08 13:55:41.996177', '2024-04-08 13:55:42.495221',\n               '2024-04-08 13:55:42.994266', '2024-04-08 13:55:43.493310',\n               '2024-04-08 13:55:43.992354', '2024-04-08 13:55:44.491399',\n               ...\n               '2024-05-10 07:59:55.001829', '2024-05-10 07:59:55.501760',\n               '2024-05-10 07:59:56.001691', '2024-05-10 07:59:56.501621',\n               '2024-05-10 07:59:57.001552', '2024-05-10 07:59:57.501482',\n               '2024-05-10 07:59:58.001413', '2024-05-10 07:59:58.501344',\n               '2024-05-10 07:59:59.001274', '2024-05-10 07:59:59.501205'],\n              dtype='datetime64[ns]', name='time', length=5236793, freq=None))Attributes: (7)source_software :DeepLabCutds_type :posesfps :2.0time_unit :datetime64[ns]source_file :/Users/nsirmpilatze/Data/Smart-Kages/kage14/analysis/dlc_output/2024/04/08/kage14_20240408_135536DLC_resnet101_v2Jan17shuffle2_580000.h5kage :kage14kage_start_datetime :2024-04-08T13:55:40\n\n\nWe see that each dataset contains a huge amount of data, but the two datasets are not exactly aligned in time.\n\n\nCode\nstart_times = {\n    name: pd.Timestamp(ds.time.isel(time=0).values)\n    for name, ds in ds_kages.items()\n}\nend_times = {\n    name: pd.Timestamp(ds.time.isel(time=-1).values)\n    for name, ds in ds_kages.items()\n}\n\nfor name in start_times.keys():\n    print(f\"{name}: from {start_times[name]} to {end_times[name]}\")\n\n\nkage14: from 2024-04-08 13:55:40 to 2024-05-10 07:59:59.501205\nkage17: from 2024-04-03 00:00:06 to 2024-05-10 07:59:59.509103",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>A mouse's daily activity log</span>"
    ]
  },
  {
    "objectID": "05-movement-mouse.html#datetime-coordinates",
    "href": "05-movement-mouse.html#datetime-coordinates",
    "title": "5¬† A mouse‚Äôs daily activity log",
    "section": "5.3 Datetime Coordinates",
    "text": "5.3 Datetime Coordinates\nYou might notice something interesting about the time coordinates in these xarray datasets: they‚Äôre given in datetime64[ns] format, which means they‚Äôre precise timestamps expressed in ‚Äúcalendar time‚Äù.\nThis is different from what we‚Äôve seen before in other movement datasets, where time coordinates are expressed as seconds elapsed since the start of the video, or ‚Äúelapsed time‚Äù.\n\n\n\n\n\n\nNoteHow did we get these timestamps?\n\n\n\n\n\nSome recording systems can output timestamps for each video frame. In our case, the raw data from the Smart-Kage system included the start datetime of each 1-hour-long video segment and the precise time difference between the start of each segment and every frame within it.\nUsing this information, we were able to reconstruct precise datetime coordinates for all frames throughout the entire experiment. We then concatenated the DeepLabCut predictions from all video segments and assigned the datetime coordinates to the resulting dataset. If you‚Äôre interested in the details, you can find the code in the smart-kages-movement GitHub repository.\n\n\n\nUsing ‚Äúcalendar time‚Äù is convenient for many applications. For example, we could cross-reference the tracking results against other data sources, such as body weight measurements.\nIt also allows us to easily select time windows by datetime. We will leverage this here to select a time window that‚Äôs common to both kages. Note that we discard the last few days because the experimenter introduced some interventions during that time, which are out of scope for this case study.\n\ncommon_start = \"2024-04-09 00:00:00\"\ncommon_end = \"2024-05-07 00:00:00\"\n\nfor kage in [\"kage14\", \"kage17\"]:\n    ds_kages[kage] = ds_kages[kage].sel(time=slice(common_start, common_end))\n\nBeyond this ability to select time windows by date and time, we will see many other benefits of using datetime coordinates in the rest of this case study.\nThat said, it‚Äôs still useful to also know the total time elapsed since the start of the experiment. In fact, many movement functions will expect ‚Äúelapsed time‚Äù and may not work with datetime coordinates (for now).\nLuckily, it‚Äôs easy to convert datetime coordinates to ‚Äúelapsed time‚Äù by simply subtracting the start datetime of the whole experiment from each timestamp.\n\n\nExpand to see how this can be done\nds_14 = ds_kages[\"kage14\"]\n\n# Get the start datetime the experiment in kage14\nexperiment_start = ds_14.time.isel(time=0).data\n\n# Subtract the start datetime from each timestamp\ntime_elapsed = (ds_14.time.data - np.datetime64(experiment_start))\n\n# Convert to seconds\nseconds_elapsed = time_elapsed / pd.Timedelta(\"1s\")\n\n# Assign the seconds_elapsed coordinate to the \"time\" dimension\nds_14 = ds_14.assign_coords(seconds_elapsed=(\"time\", seconds_elapsed))\n\n\nWe‚Äôve pre-computed this for convenience and stored it in a secondary time coordinate called seconds_elapsed.\n\nprint(ds_14.coords[\"time\"].values[:2])\nprint(ds_14.coords[\"seconds_elapsed\"].values[:2])\n\n['2024-04-09T00:00:06.000000000' '2024-04-09T00:00:06.500000000']\n[0.  0.5]\n\n\nWhenever we want to switch to ‚Äúelapsed time‚Äù mode, we can simply set the seconds_elapsed coordinates as the ‚Äúindex‚Äù of the time dimension. This means that seconds_elapsed will be used as the primary time coordinate, allowing us to select data by it.\n\nds_14.set_index(time=\"seconds_elapsed\").sel(time=slice(0, 1800))\n\n\n\n\n\n\n\nTipExercise A\n\n\n\nFor each of the two kages:\n\nPlot the x-axis position of the mouse‚Äôs body center over time, for the week starting on April 15th. What do you notice?\nPlot the median confidence of the body center for each day, over the entire duration of the experiment.\n\n\n\n\n\n\n\n\n\nTipClick to reveal the answers\n\n\n\n\n\n\nfig, axes = plt.subplots(nrows=2, ncols=1, figsize=(8, 5), sharex=True)\n\nfor i, kage in enumerate([\"kage14\", \"kage17\"]):\n    ds = ds_kages[kage].squeeze()  # remove redundant \"individuals\" dimension\n    body_center = ds.position.sel(\n        keypoints=\"bodycenter\",\n        time=slice(\"2024-04-15 00:00:00\", \"2024-04-21 23:59:59\"),\n        space=\"x\"\n    )\n    body_center.plot.line(ax=axes[i])\n    axes[i].set_title(f\"{kage} body center x\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nfig, axes = plt.subplots(nrows=2, ncols=1, figsize=(8, 5), sharex=True)\n\nfor i, kage in enumerate([\"kage14\", \"kage17\"]):\n    ds = ds_kages[kage].squeeze()\n    ds_daily = ds.resample(time=\"1D\").median()\n    ds_daily.confidence.sel(keypoints=\"bodycenter\").plot.line(ax=axes[i])\n    axes[i].set_title(f\"{kage} body center\")\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>A mouse's daily activity log</span>"
    ]
  },
  {
    "objectID": "05-movement-mouse.html#cleaning-the-data",
    "href": "05-movement-mouse.html#cleaning-the-data",
    "title": "5¬† A mouse‚Äôs daily activity log",
    "section": "5.4 Cleaning the data",
    "text": "5.4 Cleaning the data\nLet‚Äôs examine the range of confidence values for each keypoint.\n\n\nCode\nkage = \"kage14\"\nconfidence = ds_kages[kage].confidence.squeeze()\n\nfig, ax = plt.subplots(figsize=(8, 3))\nconfidence.quantile(q=0.25, dim=\"time\").plot.line(\"o--\", color=\"gray\", ax=ax, label=\"25% quantile\")\nconfidence.quantile(q=0.75, dim=\"time\").plot.line(\"o--\", color=\"gray\", ax=ax, label=\"75% quantile\")\nconfidence.median(dim=\"time\").plot.line(\"o-\", color=\"black\", ax=ax, label=\"median\")\n\nax.legend()\nax.set_title(f\"{kage} confidence range\")\nplt.show()\n\n\n\n\n\n\n\n\nFigure¬†5.2: Confidence range by keypoint\n\n\n\n\n\nIt looks like the ‚Äúneck‚Äù, ‚Äúbodycenter‚Äù, ‚Äúspine1‚Äù, and ‚Äúspine2‚Äù keypoints are the most confidently detected. Let us define a list of ‚Äúreliable‚Äù keypoints for later use. These are all on the mouse‚Äôs body.\n\nreliable_keypoints = [\"neck\", \"bodycenter\", \"spine1\", \"spine2\"]\n\nWe can filter out low-confidence predictions.\n\nconfidence_threshold = 0.95\n\nfor kage, ds in ds_kages.items():\n    ds[\"position_filtered\"] = filter_by_confidence(\n        ds.position,\n        ds.confidence,\n        threshold=confidence_threshold,\n    )\n\n\n\n\n\n\n\nTipExercise B\n\n\n\nLet‚Äôs smooth the data with a rolling median filter.\nHint: Remember doing this in Chapter 4 ?\n\n\n\n\n\n\n\n\nTipClick to reveal the answers\n\n\n\n\n\n\nwindow_size = 3  # frames\n\nfor kage, ds in ds_kages.items():\n    ds[\"position_smoothed\"] = rolling_filter(\n        ds.position_filtered,\n        window_size,\n        statistic=\"median\",\n    )",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>A mouse's daily activity log</span>"
    ]
  },
  {
    "objectID": "05-movement-mouse.html#plot-the-mouses-speed-over-time",
    "href": "05-movement-mouse.html#plot-the-mouses-speed-over-time",
    "title": "5¬† A mouse‚Äôs daily activity log",
    "section": "5.5 Plot the mouse‚Äôs speed over time",
    "text": "5.5 Plot the mouse‚Äôs speed over time\nLet‚Äôs define a single-point representation of the mouse‚Äôs position, which we‚Äôll call the body_centroid. We derive this by taking the mean of the 4 reliable keypoints, using their smoothed positions.\n\nfor kage, ds in ds_kages.items():\n    ds[\"body_centroid\"] = ds.position_filtered.sel(\n        individuals=\"individual_0\",  # the only individual in the dataset\n        keypoints=reliable_keypoints\n    ).mean(dim=\"keypoints\")\n\nNext, we‚Äôll compute the body centroid‚Äôs speed in cm/sec via the following steps:\n\nConvert the body centroid position data to cm units using scale().\nTemporarily switch to ‚Äúelapsed time‚Äù mode, because compute_speed() does not (yet) support datetime coordinates.\nCompute the speed in cm/sec\nRestore the original datetime coordinates to the speed data.\n\n\nPIXELS_PER_CM = 10\n\nfor kage, ds in ds_kages.items():\n    # Scale from pixels to cm using a known conversion factor\n    body_centroid_cm = scale(\n        ds.body_centroid, factor=1 / PIXELS_PER_CM, space_unit=\"cm\"\n    )\n\n    # Compute the speed in cm/sec\n    ds[\"body_centroid_speed\"] = compute_speed(\n       body_centroid_cm.set_index(time=\"seconds_elapsed\")  # switch time coords\n    ).assign_coords(time=body_centroid_cm.time)            # restore datetime\n\nds_kages[\"kage14\"].body_centroid_speed\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'body_centroid_speed' (time: 4600675)&gt; Size: 37MB\nnan nan 0.05137 0.001921 0.02156 ... 0.01652 0.0186 0.02228 0.01102 0.03168\nCoordinates: (2)xarray.DataArray'body_centroid_speed'time: 4600675nan nan 0.05137 0.001921 0.02156 ... 0.0186 0.02228 0.01102 0.03168array([       nan,        nan, 0.05137345, ..., 0.02227513, 0.01102056,\n       0.03167816], shape=(4600675,))Coordinates: (2)time(time)datetime64[ns]2024-04-09T00:00:06 ... 2024-05-...array(['2024-04-09T00:00:06.000000000', '2024-04-09T00:00:06.500000000',\n       '2024-04-09T00:00:07.000000000', ..., '2024-05-06T23:59:58.497744000',\n       '2024-05-06T23:59:58.997674000', '2024-05-06T23:59:59.497604000'],\n      shape=(4600675,), dtype='datetime64[ns]')seconds_elapsed(time)float643.627e+04 3.627e+04 ... 2.455e+06array([  36266.      ,   36266.5     ,   36267.      , ..., 2455458.497744,\n       2455458.997674, 2455459.497604], shape=(4600675,))Indexes: (1)timePandasIndexPandasIndex(DatetimeIndex([       '2024-04-09 00:00:06', '2024-04-09 00:00:06.500000',\n                      '2024-04-09 00:00:07', '2024-04-09 00:00:07.500000',\n                      '2024-04-09 00:00:08', '2024-04-09 00:00:08.500000',\n                      '2024-04-09 00:00:09', '2024-04-09 00:00:09.500000',\n                      '2024-04-09 00:00:10', '2024-04-09 00:00:10.500000',\n               ...\n               '2024-05-06 23:59:54.998233', '2024-05-06 23:59:55.498164',\n               '2024-05-06 23:59:55.998094', '2024-05-06 23:59:56.498024',\n               '2024-05-06 23:59:56.997954', '2024-05-06 23:59:57.497884',\n               '2024-05-06 23:59:57.997814', '2024-05-06 23:59:58.497744',\n               '2024-05-06 23:59:58.997674', '2024-05-06 23:59:59.497604'],\n              dtype='datetime64[ns]', name='time', length=4600675, freq=None))Attributes: (0)\n\n\nLet‚Äôs plot the speed over time.\n\nfig, axes = plt.subplots(\n    nrows=2, ncols=1, figsize=(8, 5), sharex=True, sharey=True\n)\n\nfor i, kage in enumerate([\"kage14\", \"kage17\"]):\n    ds_kages[kage].body_centroid_speed.plot.line(ax=axes[i])\n    axes[i].set_title(f\"{kage} body centroid\")\n    axes[i].set_ylabel(\"speed (cm/sec)\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure¬†5.3: Body centroid speed\n\n\n\n\n\n\n\n\n\n\n\nTipQuestions B\n\n\n\n\nWhat are potential sources of error in the speed calculation?\nWhat do you notice about the overall speed fluctuations over time? What do you think is the reason for this?\nDo you notice any differences between the two kages? Feel free to ‚Äúzoom in‚Äù on specific time windows to investigate this.\n\n\n\n\n\n\n\n\n\nTipClick to reveal the answers\n\n\n\n\n\nPotential sources of error in speed calculation: The accuracy of speed computation is heavily reliant on precise position data. Any inaccuracies in predicting keypoint positions (i.e., the discrepancy between actual and estimated positions) can be exacerbated when calculating speed, as it is derived from the differences between consecutive positions. Additionally, we have assumed a fixed conversion factor of 10 pixels per cm. While this value may be accurate for the habitat‚Äôs floor, the environment is not entirely flat. It includes objects for climbing, which means the conversion factor will vary depending on the mouse‚Äôs vertical position (distance from the camera).\nThe speed seems to fluctuate in a circadian pattern, especially apparent in kage14. This will become clearer if we zoom in on a time window of a few days.\n\n\nCode\ntime_window = slice(\"2024-04-22\", \"2024-04-26\")\n\nfig, axes = plt.subplots(\n    nrows=2, ncols=1, figsize=(7.5, 5), sharex=True, sharey=True\n)\n\nfor i, kage in enumerate([\"kage14\", \"kage17\"]):\n    speed = ds_kages[kage].body_centroid_speed.sel(time=time_window)\n    speed.plot.line(ax=axes[i])\n    axes[i].set_title(f\"{kage} body centroid\")\n    axes[i].set_ylabel(\"speed (pixels/sec)\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nWe also note that the circadian pattern is more pronounced in kage14, and seems to be somewhat disrupted in kage17. The mouse in kage17 seems to be more active overall, both during the day and at night.\n\n\n\n\n\n\n\n\n\nNoteA side-note on 3D pose estimation\n\n\n\n\n\nWith only a single top-down view of the mouse, we are limited to 2D pose estimation. This means the estimated keypoint coordinates are simply projections of the true 3D coordinates onto the 2D image plane.\nThis is the most common approach to pose estimation, but it cannot accurately measure true dimensions. Any conversion from pixels to physical units (e.g.¬†centimetres) will be imprecise, and sometimes significantly so.\nThis limitation can be overcome by using multiple cameras from different viewpoints and performing 3D pose estimation. There are two main markerless approaches:\n\nThe first approach is to do ‚Äòregular‚Äô 2D pose estimation in each camera view, then triangulate across camera views to estimate 3D pose. The triangulation relies on known parameters about the cameras and their relative positions and orientations. Anipose (Karashchuk et al. 2021) is a popular open-source toolkit that implements this approach.\n\n\n\nSource: Pereira, Shaevitz, and Murthy (2020)\n\n\nThe second approach, implemented in DANNCE (Dunn et al. 2021), is to use a fully 3D convolutional neural network (CNN) that can learn about 3D image features and how cameras and landmarks relate to one another in 3D space.\n\n\n\nSource: https://github.com/spoonsso/dannce\n\n\n\nSome prompts for discussion:\n\nWhat are the pros and cons of 2D vs 3D markerless pose estimation?\nIn which scenarios would you prefer one over the other?",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>A mouse's daily activity log</span>"
    ]
  },
  {
    "objectID": "05-movement-mouse.html#sec-actograms",
    "href": "05-movement-mouse.html#sec-actograms",
    "title": "5¬† A mouse‚Äôs daily activity log",
    "section": "5.6 Plot actograms",
    "text": "5.6 Plot actograms\nAn actogram is a visualisation of the mouse‚Äôs activity level over time.\nAs a measure of activity we‚Äôll use the cumulative distance traversed by the mouse‚Äôs body centroid in a given time bin‚Äîin this case, 10 minutes. Since we have already computed the speed (cm/sec), we can multiply that by the time bin duration in seconds to get the distance (cm).\n\nfor kage in [\"kage14\", \"kage17\"]:\n    time_diff = ds_kages[kage].coords[\"seconds_elapsed\"].diff(dim=\"time\")\n    ds_kages[kage][\"distance\"] = ds_kages[kage].body_centroid_speed * time_diff\n\nThen we can sum the distance traversed in each time bin to get the activity level.\n\ntime_bin_minutes = 10\ntime_bin_duration = pd.Timedelta(f\"{time_bin_minutes}min\")\n\nfig, ax = plt.subplots(\n    nrows=2, ncols=1, figsize=(8, 5), sharex=True, sharey=True\n)\n\nactivity_dict = {}  # Dictionary to store the activity levels for each kage\n\nfor i, kage in enumerate([\"kage14\", \"kage17\"]):\n    activity = ds_kages[kage].distance.resample(time=time_bin_duration).sum()\n    activity.plot.line(ax=ax[i])\n    ax[i].set_title(f\"{kage} activity\")\n    ax[i].set_ylabel(\"distance (cm)\")\n    ax[i].set_xlabel(\"time\")\n\n    activity_dict[kage] = activity\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure¬†5.4: Activity over time\n\n\n\n\n\nTo make any circadian patterns more apparent, we will stack days vertically and indicate the light cycle with gray areas. For this particular experiment, the light cycle is as follows:\n\nlights off at 9:30\ndawn at 20:30\nlights on at 21:30\n\n\n# Define light cycle (in minutes since midnight)\nlights_off = 9 * 60 + 30  # 9:30 AM in minutes\ndawn = 20 * 60 + 30       # 8:30 PM in minutes  \nlights_on = 21 * 60 + 30  # 9:30 PM in minutes\n\nn_bins_in_day = int(24 * 60 / time_bin_minutes)\n\nactogram_dict = {}  # Dictionary to store the 2D actogram for each kage\n\nfor i, kage in enumerate([\"kage14\", \"kage17\"]):\n    activity = activity_dict[kage]\n    days = list(activity.groupby(\"time.date\").groups.keys())\n\n    # Create an empty 2D actogram with dims (date, time_bin)\n    actogram = xr.DataArray(\n        np.zeros((len(days), n_bins_in_day)),\n        dims=[\"date\", \"time_of_day\"],\n        coords={\n            \"date\": days,\n            \"time_of_day\": np.arange(\n                time_bin_minutes/2, 24 * 60, time_bin_minutes\n            )\n        },\n    )\n    # Populate 2D actogram per day\n    for date, day_activity in activity.groupby(\"time.date\"):\n        actogram.loc[dict(date=date)] = day_activity.values\n\n    # Store the actogram in the dictionary for later use\n    actogram_dict[kage] = actogram\n\nactogram_dict[\"kage14\"]  # Replace with kage17 to see the actogram for kage17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray (date: 28, time_of_day: 144)&gt; Size: 32kB\n21.94 12.34 15.7 40.82 51.96 359.7 337.1 ... 59.47 65.09 69.83 41.08 50.58 95.62\nCoordinates: (2)xarray.DataArraydate: 28time_of_day: 14421.94 12.34 15.7 40.82 51.96 359.7 ... 65.09 69.83 41.08 50.58 95.62array([[ 21.94353274,  12.3359559 ,  15.69606955, ..., 370.01643336,\n        148.41509776,  43.63468815],\n       [ 24.34439421,  28.70382676,  35.21278474, ...,  37.0943919 ,\n         32.72694742, 170.30008811],\n       [165.96110319,  22.25839305,  22.3546828 , ...,  13.99343428,\n         10.45916989,  17.76600485],\n       ...,\n       [ 17.13844639,  19.77411278,  42.93937937, ...,  10.3900106 ,\n         21.68248825, 166.8786525 ],\n       [268.03459651,  65.77269862,  72.07467688, ..., 211.74754295,\n         71.80358134,  57.01412942],\n       [ 50.11294109,  30.36584632,  91.0799254 , ...,  41.08254947,\n         50.5819582 ,  95.6220443 ]], shape=(28, 144))Coordinates: (2)date(date)object2024-04-09 ... 2024-05-06array([datetime.date(2024, 4, 9), datetime.date(2024, 4, 10),\n       datetime.date(2024, 4, 11), datetime.date(2024, 4, 12),\n       datetime.date(2024, 4, 13), datetime.date(2024, 4, 14),\n       datetime.date(2024, 4, 15), datetime.date(2024, 4, 16),\n       datetime.date(2024, 4, 17), datetime.date(2024, 4, 18),\n       datetime.date(2024, 4, 19), datetime.date(2024, 4, 20),\n       datetime.date(2024, 4, 21), datetime.date(2024, 4, 22),\n       datetime.date(2024, 4, 23), datetime.date(2024, 4, 24),\n       datetime.date(2024, 4, 25), datetime.date(2024, 4, 26),\n       datetime.date(2024, 4, 27), datetime.date(2024, 4, 28),\n       datetime.date(2024, 4, 29), datetime.date(2024, 4, 30),\n       datetime.date(2024, 5, 1), datetime.date(2024, 5, 2),\n       datetime.date(2024, 5, 3), datetime.date(2024, 5, 4),\n       datetime.date(2024, 5, 5), datetime.date(2024, 5, 6)], dtype=object)time_of_day(time_of_day)float645.0 15.0 ... 1.425e+03 1.435e+03array([   5.,   15.,   25.,   35.,   45.,   55.,   65.,   75.,   85.,   95.,\n        105.,  115.,  125.,  135.,  145.,  155.,  165.,  175.,  185.,  195.,\n        205.,  215.,  225.,  235.,  245.,  255.,  265.,  275.,  285.,  295.,\n        305.,  315.,  325.,  335.,  345.,  355.,  365.,  375.,  385.,  395.,\n        405.,  415.,  425.,  435.,  445.,  455.,  465.,  475.,  485.,  495.,\n        505.,  515.,  525.,  535.,  545.,  555.,  565.,  575.,  585.,  595.,\n        605.,  615.,  625.,  635.,  645.,  655.,  665.,  675.,  685.,  695.,\n        705.,  715.,  725.,  735.,  745.,  755.,  765.,  775.,  785.,  795.,\n        805.,  815.,  825.,  835.,  845.,  855.,  865.,  875.,  885.,  895.,\n        905.,  915.,  925.,  935.,  945.,  955.,  965.,  975.,  985.,  995.,\n       1005., 1015., 1025., 1035., 1045., 1055., 1065., 1075., 1085., 1095.,\n       1105., 1115., 1125., 1135., 1145., 1155., 1165., 1175., 1185., 1195.,\n       1205., 1215., 1225., 1235., 1245., 1255., 1265., 1275., 1285., 1295.,\n       1305., 1315., 1325., 1335., 1345., 1355., 1365., 1375., 1385., 1395.,\n       1405., 1415., 1425., 1435.])Indexes: (2)datePandasIndexPandasIndex(Index([2024-04-09, 2024-04-10, 2024-04-11, 2024-04-12, 2024-04-13, 2024-04-14,\n       2024-04-15, 2024-04-16, 2024-04-17, 2024-04-18, 2024-04-19, 2024-04-20,\n       2024-04-21, 2024-04-22, 2024-04-23, 2024-04-24, 2024-04-25, 2024-04-26,\n       2024-04-27, 2024-04-28, 2024-04-29, 2024-04-30, 2024-05-01, 2024-05-02,\n       2024-05-03, 2024-05-04, 2024-05-05, 2024-05-06],\n      dtype='object', name='date'))time_of_dayPandasIndexPandasIndex(Index([   5.0,   15.0,   25.0,   35.0,   45.0,   55.0,   65.0,   75.0,   85.0,\n         95.0,\n       ...\n       1345.0, 1355.0, 1365.0, 1375.0, 1385.0, 1395.0, 1405.0, 1415.0, 1425.0,\n       1435.0],\n      dtype='float64', name='time_of_day', length=144))Attributes: (0)\n\n\nLet‚Äôs now visualise the actograms, with the light cycle marked.\n\n\nCode\nmax_activity = max(\n    actogram_dict[kage].max().values for kage in [\"kage14\", \"kage17\"]\n)\n\nfig, axes = plt.subplots(\n    nrows=2, ncols=1, figsize=(8, 8), sharex=True, sharey=True,\n)\n\nfor i, kage in enumerate([\"kage14\", \"kage17\"]):\n    actogram = actogram_dict[kage]\n\n    # Create a colormap for the actogram\n    cmap = colormaps.get_cmap(\"Greys\")\n    cmap.set_bad(color=\"brown\", alpha=0.5)  # Set bad values to red\n\n    # Plot the actogram\n    ax = axes[i]\n    actogram.plot(\n        ax=ax, yincrease=False, vmin=0, vmax=max_activity, cmap=cmap\n    )\n\n    # Assign x-tick labels every 4 hours formatted as HH:MM\n    ax.set_xticks(np.arange(0, 24 * 60 + 1, 4 * 60))\n    ax.set_xticklabels([f\"{i:02d}:00\" for i in np.arange(0, 25, 4)])\n\n    # Mark light cycle\n    ax.axvline(lights_off, color=\"red\", alpha=0.7, lw=2, linestyle=\"-\", label=\"lights off\")\n    ax.axvline(dawn, color=\"blue\", alpha=0.7, lw=2, linestyle=\"--\", label=\"dawn\")\n    ax.axvline(lights_on, color=\"blue\", alpha=0.7, lw=2, linestyle=\"-\", label=\"lights on\")\n    ax.legend(loc=\"lower left\")\n\n    # Set title and axis labels\n    ax.set_title(f\"{kage} actogram\")\n    ax.set_xlabel(\"Time of day\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure¬†5.5: Actograms\n\n\n\n\n\n\n\n\n\n\n\nTipExercise C\n\n\n\n\nHow would you describe the observed differences between the two mice?\nCompute the mean activity profile (across days) for each mouse.\nPlot the mean activity profile for each mouse, with the light cycle marked.\n\nHint: Start with the actogram_dict dictionary and also make use of the lights_off and lights_on variables defined above.\n\n\n\n\n\n\n\n\nTipClick to reveal the answers\n\n\n\n\n\nThe mouse in kage17 shows higher overall activity levels, stays constantly active during dark periods and also exhibits many activity bursts during the day (at least more than kage14 does).\n\nfig, ax = plt.subplots(figsize=(8, 4))\n\n# Mark light cycle\nax.axvspan(lights_off, lights_on, color=\"0.8\", label=\"Dark period\")\n\nfor kage, actogram in actogram_dict.items():    \n    actogram.mean(dim=\"date\").plot.line(ax=ax, label=kage, lw=3)\n\nax.set_title(f\"Mean daily activity profile\")\nax.set_ylabel(f\"Activity\")\nax.set_xlabel(\"Time of day\")\n\n# Assign x-tick labels every 4 hours formatted as HH:MM\nax.set_xticks(np.arange(0, 24 * 60 + 1, 4 * 60))\nax.set_xticklabels([f\"{i:02d}:00\" for i in np.arange(0, 25, 4)])\n\nax.legend()\nplt.show()\n\n\n\n\n\n\n\nFigure¬†5.6: Mean daily activity profiles\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteWhat underlies the differences in activity?\n\n\n\n\n\nThe mouse in kage17 is a genetically modified model of Down Syndrome. The syndrome, also known as trisomy 21, is caused in humans by the presence of all or part of a third copy of chromosome 21. The mouse in kage17 is genetically modified with a triplication of mouse chromosome 16, which carries about 150 genes homologues to the human chromosome 21.\nLoukia is currently investigating why this particular mouse model exhibits a ‚Äúrestless‚Äù behavioural phenotype.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>A mouse's daily activity log</span>"
    ]
  },
  {
    "objectID": "05-movement-mouse.html#space-occupancy",
    "href": "05-movement-mouse.html#space-occupancy",
    "title": "5¬† A mouse‚Äôs daily activity log",
    "section": "5.7 Space occupancy",
    "text": "5.7 Space occupancy\nApart from quantifying how active the mice were over time, we might also be interested in which parts of the habitat they tend to frequent.\nmovement provides a plot_occupancy() function to help us visualise the space occupancy.\n\nfig, axes = plt.subplots(\n    nrows=1, ncols=2, figsize=(8, 4), sharex=True, sharey=True\n)\n\nplt.suptitle(\"Body centroid occupancy (log scale)\")\n\nfor i, kage in enumerate([\"kage14\", \"kage17\"]):\n    img = images[i]\n    height, width = img.shape[:2]\n\n    axes[i].imshow(img)\n    plot_occupancy(\n        # Setting the time coordinates to \"elapsed time\" is necessary\n        # for the log scale to work properly.\n        ds_kages[kage].body_centroid.set_index(time=\"seconds_elapsed\"),\n        ax=axes[i],\n        cmap=\"turbo\",\n        norm=\"log\",  # log scale the colormap\n        vmax=10**6,\n        alpha=0.6,   # some transparency\n    )\n    # Make axes match the image dimensions\n    axes[i].set_ylim([height - 1, 0])\n    axes[i].set_xlim([0, width])\n    axes[i].set_title(kage)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure¬†5.7: Occupancy heatmaps\n\n\n\n\n\nWe see some clear hotspots, such as the nest and climbing platform. But not all hotspots are created equal. For example, the nest should be occupied when the mouse is stationary but not when it is moving.\nTo investigate this, let‚Äôs choose an arbitrary speed limit of 4 cm/sec, below which we consider the mouse to be stationary.\n\n\nCode\nfig, ax = plt.subplots(figsize=(8, 4))\n\nspeed_threshold = 4\n\nfor kage in [\"kage14\", \"kage17\"]:\n    ds_kages[kage].body_centroid_speed.plot.hist(\n        ax=ax, bins=50, alpha=0.5, label=kage, histtype=\"step\",\n    )\n\nax.axvline(speed_threshold, linestyle=\"--\", color=\"red\", label=\"Speed threshold\")\n\nax.set_title(\"Body centroid speed\")\nax.set_xlabel(\"Speed (cm/sec)\")\nax.set_ylabel(\"Count\")\nax.set_yscale(\"log\")\nax.legend()\nplt.show()\n\n\n\n\n\n\n\n\nFigure¬†5.8: Body centroid speed histogram (log scale)\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe use the log scale because speeds tend to be exponentially distributed, i.e.¬†the mouse spends far more time at low speeds than at high speeds. Try commenting out the ax.set_yscale(\"log\") line and see what happens.\n\n\nWe will generate separate occupancy heatmaps for when the mouse is stationary vs moving. We can do this by masking with where().\n\nstationary_mask = ds_kages[\"kage14\"].body_centroid_speed &lt; 4\nstationary_mask\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'body_centroid_speed' (time: 4600675)&gt; Size: 5MB\nFalse False True True True True True True ... True True True True True True True\nCoordinates: (2)xarray.DataArray'body_centroid_speed'time: 4600675False False True True True True True ... True True True True True Truearray([False, False,  True, ...,  True,  True,  True], shape=(4600675,))Coordinates: (2)time(time)datetime64[ns]2024-04-09T00:00:06 ... 2024-05-...array(['2024-04-09T00:00:06.000000000', '2024-04-09T00:00:06.500000000',\n       '2024-04-09T00:00:07.000000000', ..., '2024-05-06T23:59:58.497744000',\n       '2024-05-06T23:59:58.997674000', '2024-05-06T23:59:59.497604000'],\n      shape=(4600675,), dtype='datetime64[ns]')seconds_elapsed(time)float643.627e+04 3.627e+04 ... 2.455e+06array([  36266.      ,   36266.5     ,   36267.      , ..., 2455458.497744,\n       2455458.997674, 2455459.497604], shape=(4600675,))Indexes: (1)timePandasIndexPandasIndex(DatetimeIndex([       '2024-04-09 00:00:06', '2024-04-09 00:00:06.500000',\n                      '2024-04-09 00:00:07', '2024-04-09 00:00:07.500000',\n                      '2024-04-09 00:00:08', '2024-04-09 00:00:08.500000',\n                      '2024-04-09 00:00:09', '2024-04-09 00:00:09.500000',\n                      '2024-04-09 00:00:10', '2024-04-09 00:00:10.500000',\n               ...\n               '2024-05-06 23:59:54.998233', '2024-05-06 23:59:55.498164',\n               '2024-05-06 23:59:55.998094', '2024-05-06 23:59:56.498024',\n               '2024-05-06 23:59:56.997954', '2024-05-06 23:59:57.497884',\n               '2024-05-06 23:59:57.997814', '2024-05-06 23:59:58.497744',\n               '2024-05-06 23:59:58.997674', '2024-05-06 23:59:59.497604'],\n              dtype='datetime64[ns]', name='time', length=4600675, freq=None))Attributes: (0)\n\n\nA ‚Äúmask‚Äù is just a boolean array of the same shape as the original data. It‚Äôs True where the condition is met, and False otherwise.\n\nstationary_position = ds_kages[\"kage14\"].body_centroid.where(stationary_mask)\nstationary_position\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'body_centroid' (time: 4600675, space: 2)&gt; Size: 74MB\nnan nan nan nan 198.7 177.6 198.7 ... 44.09 128.6 43.99 128.7 43.94 128.5 44.0\nCoordinates: (3)\nAttributes: (1)xarray.DataArray'body_centroid'time: 4600675space: 2nan nan nan nan 198.7 177.6 ... 128.6 43.99 128.7 43.94 128.5 44.0array([[         nan,          nan],\n       [         nan,          nan],\n       [198.68329239, 177.6133194 ],\n       ...,\n       [128.63087654,  43.9933548 ],\n       [128.66922379,  43.94464111],\n       [128.520895  ,  44.00013161]], shape=(4600675, 2))Coordinates: (3)space(space)&lt;U1'x' 'y'array(['x', 'y'], dtype='&lt;U1')time(time)datetime64[ns]2024-04-09T00:00:06 ... 2024-05-...array(['2024-04-09T00:00:06.000000000', '2024-04-09T00:00:06.500000000',\n       '2024-04-09T00:00:07.000000000', ..., '2024-05-06T23:59:58.497744000',\n       '2024-05-06T23:59:58.997674000', '2024-05-06T23:59:59.497604000'],\n      shape=(4600675,), dtype='datetime64[ns]')seconds_elapsed(time)float643.627e+04 3.627e+04 ... 2.455e+06array([  36266.      ,   36266.5     ,   36267.      , ...,\n       2455458.497744, 2455458.997674, 2455459.497604], shape=(4600675,))Indexes: (2)spacePandasIndexPandasIndex(Index(['x', 'y'], dtype='object', name='space'))timePandasIndexPandasIndex(DatetimeIndex([       '2024-04-09 00:00:06', '2024-04-09 00:00:06.500000',\n                      '2024-04-09 00:00:07', '2024-04-09 00:00:07.500000',\n                      '2024-04-09 00:00:08', '2024-04-09 00:00:08.500000',\n                      '2024-04-09 00:00:09', '2024-04-09 00:00:09.500000',\n                      '2024-04-09 00:00:10', '2024-04-09 00:00:10.500000',\n               ...\n               '2024-05-06 23:59:54.998233', '2024-05-06 23:59:55.498164',\n               '2024-05-06 23:59:55.998094', '2024-05-06 23:59:56.498024',\n               '2024-05-06 23:59:56.997954', '2024-05-06 23:59:57.497884',\n               '2024-05-06 23:59:57.997814', '2024-05-06 23:59:58.497744',\n               '2024-05-06 23:59:58.997674', '2024-05-06 23:59:59.497604'],\n              dtype='datetime64[ns]', name='time', length=4600675, freq=None))Attributes: (1)log :[\n  {\n    \"operation\": \"filter_by_confidence\",\n    \"datetime\": \"2025-10-01 14:15:27.765684\",\n    \"confidence\": \"&lt;xarray.DataArray 'confidence' (time: 4600675, keypoints: 8, individuals: 1)&gt; Size: 294MB\\narray([[[         nan],\\n        [         nan],\\n        ...,\\n        [         nan],\\n        [         nan]],\\n\\n       [[1.728711e-02],\\n        [9.850310e-01],\\n        ...,\\n        [9.821828e-01],\\n        [8.725649e-04]],\\n\\n       ...,\\n\\n       [[9.718522e-01],\\n        [9.982640e-01],\\n        ...,\\n        [9.880463e-01],\\n        [9.681368e-01]],\\n\\n       [[9.761935e-01],\\n        [9.980691e-01],\\n        ...,\\n        [9.910011e-01],\\n        [9.745104e-01]]], shape=(4600675, 8, 1))\\nCoordinates: (4)\",\n    \"threshold\": \"0.95\",\n    \"print_report\": \"False\"\n  }\n]\n\n\nWe see that the where() method returns a new xarray.DataArray with the same dimensions as the original, but with the data values replaced by NaN where the condition (mask) is False.\nUsing this approach we can generate separate the body centroid position arrays into states where the mouse is stationary vs moving, and then plot the occupancy heatmaps for each state.\n\n\nCode\nfig, axes = plt.subplots(\n    nrows=2, ncols=2, figsize=(8, 8), sharex=True, sharey=True\n)\n\nplt.suptitle(\"Body centroid occupancy (log scale)\")\n\nfor i, kage in enumerate([\"kage14\", \"kage17\"]):\n    img = images[i]\n    height, width = img.shape[:2]\n\n    masks = {\n        \"stationary\": ds_kages[kage].body_centroid_speed &lt; 4,\n        \"moving\": ds_kages[kage].body_centroid_speed &gt;= 4,\n    }\n\n    for j, (mask_name, mask) in enumerate(masks.items()):\n        ax = axes[i, j]\n        ax.imshow(img)\n        plot_occupancy(\n            ds_kages[kage].body_centroid.where(mask).set_index(time=\"seconds_elapsed\"),\n            ax=ax,\n            cmap=\"turbo\",\n            norm=\"log\",\n            vmax=10**6,\n            alpha=0.6,\n        )\n        ax.set_title(f\"{kage} {mask_name}\")\n        ax.set_ylim([height - 1, 0])\n        ax.set_xlim([0, width])\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure¬†5.9: Occupancy heatmaps (stationary vs active)\n\n\n\n\n\nWe see some expected patterns like the nest being a ‚Äúhotspot‚Äù during stationary periods, and going ‚Äúdark‚Äù during active periods. But we also see some puzzling patterns: for example, the running wheel is occupied during both periods, including when the mouse is ‚Äústationary‚Äù.\nIs that because the mouse appears to be stationary to the camera as it‚Äôs running ‚Äúin-place‚Äù on the wheel (like on a treadmill)? Or maybe the mouse spends some of its downtime resting on the wheel?",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>A mouse's daily activity log</span>"
    ]
  },
  {
    "objectID": "05-movement-mouse.html#region-of-interest-occupancy",
    "href": "05-movement-mouse.html#region-of-interest-occupancy",
    "title": "5¬† A mouse‚Äôs daily activity log",
    "section": "5.8 Region of interest occupancy",
    "text": "5.8 Region of interest occupancy\nLet us precisely measure running wheel occupancy, i.e.¬†the proportion of time the mouse spends on the running wheel. Here we‚Äôll use movement‚Äôs functionality for defining regions of interest (ROIs).\nLet us create a circular ‚Äúrunning wheel‚Äù ROI for each of the two kages.\n\ncentres = {\n    \"kage14\": np.array([145, 260]),  # (x, y)\n    \"kage17\": np.array([385, 210]),\n}\nradius = 70\n\n# Create a unit circle\nn_points = 32\nangles = np.linspace(0, 2 * np.pi, n_points)\nunit_circle = np.column_stack([np.cos(angles), np.sin(angles)])\n\n# Create ROIs by scaling and shifting the unit circle\nrois = {}\nfor kage in [\"kage14\", \"kage17\"]:\n    points = centres[kage] + radius * unit_circle\n    roi = PolygonOfInterest(points, name=f\"{kage} running wheel\")\n    rois[kage] = roi\n\n\n\n\n\n\n\nNote\n\n\n\nAdmittedly, this is not the most precise or convenient way to define the running wheel ROI. It would be better to directly draw shapes on the video frames.\nWe are actively working on a widget in napari that will enable this. Stay tuned for updates in movement by joining the ‚Äúmovement‚Äù channel on Zulip.\n\n\nNow that we have the ROIs defined as PolygonOfInterest objects, we can use some of their built-in methods.\nFor example, we can use .plot() to visualise the ROIs and verify that they are roughly in the right place.\n\n\nCode\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(8, 8))\n\nfor i, kage in enumerate([\"kage14\", \"kage17\"]):\n    img = images[i]\n    ax[i].imshow(img)\n    rois[kage].plot(ax=ax[i], alpha=0.25, facecolor=\"red\")\n    ax[i].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure¬†5.10: Running wheel ROIs\n\n\n\n\n\nWe can also use .contains_point() to check if a point is inside the ROI. If we pass it a whole xarray.DataArray of points positions, e.g. the positions of the mouse‚Äôs body centroid over time, the check is performed for each point in the array.\n\n\n\n\n\n\nWarning\n\n\n\nThe following code cell will take a while to run, probably a few minutes. That‚Äôs because we have a large amount of data and the .contains_point() method is not fully optimised yet.\nIf you are an experienced Python programmer this could be a cool project for the hackday.\n\n\n\nroi_occupancy = {\n    kage: rois[kage].contains_point(ds_kages[kage].body_centroid)\n    for kage in [\"kage14\", \"kage17\"]\n}\n\nroi_occupancy[\"kage14\"]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'body_centroid' (time: 4600675)&gt; Size: 5MB\nFalse False False False False False ... False False False False False False\nCoordinates: (2)\nAttributes: (1)xarray.DataArray'body_centroid'time: 4600675False False False False False False ... False False False False Falsearray([False, False, False, ..., False, False, False], shape=(4600675,))Coordinates: (2)time(time)datetime64[ns]2024-04-09T00:00:06 ... 2024-05-...array(['2024-04-09T00:00:06.000000000', '2024-04-09T00:00:06.500000000',\n       '2024-04-09T00:00:07.000000000', ..., '2024-05-06T23:59:58.497744000',\n       '2024-05-06T23:59:58.997674000', '2024-05-06T23:59:59.497604000'],\n      shape=(4600675,), dtype='datetime64[ns]')seconds_elapsed(time)float643.627e+04 3.627e+04 ... 2.455e+06array([  36266.      ,   36266.5     ,   36267.      , ..., 2455458.497744,\n       2455458.997674, 2455459.497604], shape=(4600675,))Indexes: (1)timePandasIndexPandasIndex(DatetimeIndex([       '2024-04-09 00:00:06', '2024-04-09 00:00:06.500000',\n                      '2024-04-09 00:00:07', '2024-04-09 00:00:07.500000',\n                      '2024-04-09 00:00:08', '2024-04-09 00:00:08.500000',\n                      '2024-04-09 00:00:09', '2024-04-09 00:00:09.500000',\n                      '2024-04-09 00:00:10', '2024-04-09 00:00:10.500000',\n               ...\n               '2024-05-06 23:59:54.998233', '2024-05-06 23:59:55.498164',\n               '2024-05-06 23:59:55.998094', '2024-05-06 23:59:56.498024',\n               '2024-05-06 23:59:56.997954', '2024-05-06 23:59:57.497884',\n               '2024-05-06 23:59:57.997814', '2024-05-06 23:59:58.497744',\n               '2024-05-06 23:59:58.997674', '2024-05-06 23:59:59.497604'],\n              dtype='datetime64[ns]', name='time', length=4600675, freq=None))Attributes: (1)log :[\n  {\n    \"operation\": \"filter_by_confidence\",\n    \"datetime\": \"2025-10-01 14:15:27.765684\",\n    \"confidence\": \"&lt;xarray.DataArray 'confidence' (time: 4600675, keypoints: 8, individuals: 1)&gt; Size: 294MB\\narray([[[         nan],\\n        [         nan],\\n        ...,\\n        [         nan],\\n        [         nan]],\\n\\n       [[1.728711e-02],\\n        [9.850310e-01],\\n        ...,\\n        [9.821828e-01],\\n        [8.725649e-04]],\\n\\n       ...,\\n\\n       [[9.718522e-01],\\n        [9.982640e-01],\\n        ...,\\n        [9.880463e-01],\\n        [9.681368e-01]],\\n\\n       [[9.761935e-01],\\n        [9.980691e-01],\\n        ...,\\n        [9.910011e-01],\\n        [9.745104e-01]]], shape=(4600675, 8, 1))\\nCoordinates: (4)\",\n    \"threshold\": \"0.95\",\n    \"print_report\": \"False\"\n  }\n]\n\n\nThe ROI occupancy data is a boolean array which is True when a point (in this case the mouse‚Äôs body centroid at each time point) is inside the ROI, and False otherwise.\n\n\nCode\nfig, axes = plt.subplots(\n    nrows=2, ncols=1, figsize=(7.5, 5), sharex=True, sharey=True\n)\n\nfor i, kage in enumerate([\"kage14\", \"kage17\"]):\n    ax = axes[i]\n    roi_occupancy[kage].sel(time=slice(None, \"2024-04-09 23:59:59\")).plot.line(\n        \"-\", ax=ax, lw=0.1\n    )\n    ax.set_title(kage)\n    ax.set_ylabel(\"Occupancy\")\n    ax.set_yticks([0, 1])\n    ax.set_yticklabels([\"False\", \"True\"])\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure¬†5.11: Running wheel occupancy during April 9, 2024\n\n\n\n\n\nWe can also compute the % of time the mouse spends on the running wheel.\n\nfor kage in [\"kage14\", \"kage17\"]:\n    # Count the ratio of True values in the array\n    on_wheel_ratio = roi_occupancy[kage].mean(dim=\"time\").values\n    # Convert to percentage\n    pct_on_wheel = float(100 * on_wheel_ratio)\n    print(f\"{kage} spends {pct_on_wheel:.1f}% of its time on the running wheel\")\n\nkage14 spends 7.7% of its time on the running wheel\nkage17 spends 5.6% of its time on the running wheel\n\n\nBut how does the running wheel occupancy relate to the light cycle? We can segment the ROI occupancy data into dark and light periods for each day and compute the % of time spent on the running wheel during each period.\n\n\nCode\ndays = list(\n    roi_occupancy[\"kage14\"].dropna(dim=\"time\").groupby(\"time.date\").groups.keys()\n)\nn_days = len(days)\n\n# Create a new DataArray of NaNs with shape (kage, date, period)\ndaily_occupancy = xr.DataArray(\n    np.full((2, n_days, 2), np.nan),\n    dims=[\"kage\", \"date\", \"period\"],\n    coords={\n        \"kage\": [\"kage14\", \"kage17\"],\n        \"date\": days,\n        \"period\": [\"dark\", \"light\"]\n    },\n)\n\nfor kage in [\"kage14\", \"kage17\"]:\n    for date, day_ds in roi_occupancy[kage].dropna(dim=\"time\").groupby(\"time.date\"):\n        dark_period = slice(f\"{date} 09:30:00\", f\"{date} 21:30:00\")\n        light_period1 = slice(f\"{date} 00:00:00\", f\"{date} 09:30:00\")\n        light_period2 = slice(f\"{date} 21:30:00\", f\"{date} 23:59:59\")\n\n        dark_occupancy = 100 * day_ds.sel(time=dark_period).mean()\n        light_occupancy = 100 * (\n            day_ds.sel(time=light_period1).mean() +\n            day_ds.sel(time=light_period2).mean()\n        )\n\n        daily_occupancy.loc[dict(kage=kage, date=date, period=\"dark\")] = dark_occupancy\n        daily_occupancy.loc[dict(kage=kage, date=date, period=\"light\")] = light_occupancy\n\n\n\ndaily_occupancy\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray (kage: 2, date: 27, period: 2)&gt; Size: 864B\n18.14 0.5802 14.61 0.2946 15.77 0.05681 ... 7.642 1.184 6.061 1.822 5.416 1.465\nCoordinates: (3)xarray.DataArraykage: 2date: 27period: 218.14 0.5802 14.61 0.2946 15.77 ... 1.184 6.061 1.822 5.416 1.465array([[[18.13721182,  0.58020834],\n        [14.61429664,  0.29461052],\n        [15.77476872,  0.05681437],\n        [17.50275346,  0.09224193],\n        [16.79477815,  0.16400123],\n        [21.09387679,  0.34003517],\n        [22.30209155,  0.05797538],\n        [25.09129694,  0.09135919],\n        [19.21257652,  0.44861916],\n        [20.14724638,  0.42807444],\n        [10.57427028,  0.35374141],\n        [20.48226293,  0.41311089],\n        [17.57491451,  0.37366155],\n        [23.04687047,  0.22957797],\n        [24.79366625,  0.52732957],\n        [22.83723788,  0.83547609],\n        [13.37413962,  0.47727011],\n        [10.37548834,  1.68838776],\n        [ 7.56987353,  0.        ],\n        [10.09540149,  2.25066261],\n...\n        [ 4.60557584,  0.72872661],\n        [ 2.87145126,  0.7931199 ],\n        [        nan,         nan],\n        [14.65730083,  0.09372208],\n        [12.79994436,  1.024094  ],\n        [11.59018712,  1.25811336],\n        [12.3581439 ,  0.88033789],\n        [17.19648488,  1.35126619],\n        [15.02411091,  4.8502329 ],\n        [14.7872957 ,  0.8731539 ],\n        [ 6.47423636,  2.6038989 ],\n        [ 8.67453399, 16.23910604],\n        [10.69304865,  0.7644518 ],\n        [ 9.61627637,  0.48341291],\n        [13.3663309 ,  2.00887393],\n        [ 9.25234728,  3.90692875],\n        [14.20903758,  0.9260435 ],\n        [ 7.64156958,  1.18385996],\n        [ 6.06127346,  1.82213469],\n        [ 5.41580692,  1.4653501 ]]])Coordinates: (3)kage(kage)&lt;U6'kage14' 'kage17'array(['kage14', 'kage17'], dtype='&lt;U6')date(date)object2024-04-09 ... 2024-05-06array([datetime.date(2024, 4, 9), datetime.date(2024, 4, 10),\n       datetime.date(2024, 4, 11), datetime.date(2024, 4, 13),\n       datetime.date(2024, 4, 14), datetime.date(2024, 4, 15),\n       datetime.date(2024, 4, 16), datetime.date(2024, 4, 17),\n       datetime.date(2024, 4, 18), datetime.date(2024, 4, 19),\n       datetime.date(2024, 4, 20), datetime.date(2024, 4, 21),\n       datetime.date(2024, 4, 22), datetime.date(2024, 4, 23),\n       datetime.date(2024, 4, 24), datetime.date(2024, 4, 25),\n       datetime.date(2024, 4, 26), datetime.date(2024, 4, 27),\n       datetime.date(2024, 4, 28), datetime.date(2024, 4, 29),\n       datetime.date(2024, 4, 30), datetime.date(2024, 5, 1),\n       datetime.date(2024, 5, 2), datetime.date(2024, 5, 3),\n       datetime.date(2024, 5, 4), datetime.date(2024, 5, 5),\n       datetime.date(2024, 5, 6)], dtype=object)period(period)&lt;U5'dark' 'light'array(['dark', 'light'], dtype='&lt;U5')Indexes: (3)kagePandasIndexPandasIndex(Index(['kage14', 'kage17'], dtype='object', name='kage'))datePandasIndexPandasIndex(Index([2024-04-09, 2024-04-10, 2024-04-11, 2024-04-13, 2024-04-14, 2024-04-15,\n       2024-04-16, 2024-04-17, 2024-04-18, 2024-04-19, 2024-04-20, 2024-04-21,\n       2024-04-22, 2024-04-23, 2024-04-24, 2024-04-25, 2024-04-26, 2024-04-27,\n       2024-04-28, 2024-04-29, 2024-04-30, 2024-05-01, 2024-05-02, 2024-05-03,\n       2024-05-04, 2024-05-05, 2024-05-06],\n      dtype='object', name='date'))periodPandasIndexPandasIndex(Index(['dark', 'light'], dtype='object', name='period'))Attributes: (0)\n\n\nLet‚Äôs visualise the % of time spent on the running wheel during the light and dark periods of each day.\n\n\nCode\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(8, 4), sharex=True, sharey=True)\n\nmax_occupancy = daily_occupancy.max()\n\nfor i, kage in enumerate([\"kage14\", \"kage17\"]):\n    daily_occupancy.sel(kage=kage).plot(\n        ax=axes[i], lw=3, vmax=daily_occupancy.max(), yincrease=False\n    )\n\n    axes[i].set_title(f\"{kage} running wheel occupancy\")\n    axes[i].set_xticks([0.25, 0.75])\n    axes[i].set_ylabel(\"Occupancy (%)\")\n    axes[i].set_xlabel(\"Period\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure¬†5.12: Running wheel occupancy during light and dark periods\n\n\n\n\n\nAs we would expect, both mice tend to spend more time on the running wheel during the dark (active) periods.\nEven though the mouse in kage17 is more active overall, as we saw in Section 5.6, it spends less time on the running wheel compared to the mouse in kage14.\n\n\n\n\nDunn, Timothy W., Jesse D. Marshall, Kyle S. Severson, Diego E. Aldarondo, David G. C. Hildebrand, Selmaan N. Chettih, William L. Wang, et al. 2021. ‚ÄúGeometric Deep Learning Enables 3D Kinematic Profiling Across Species and Environments.‚Äù Nature Methods 18 (5): 564‚Äì73. https://doi.org/10.1038/s41592-021-01106-6.\n\n\nHo, Hinze, Nejc Kejzar, Hiroki Sasaguri, Takashi Saito, Takaomi C. Saido, Bart De Strooper, Marius Bauza, and Julija Krupic. 2023. ‚ÄúA Fully Automated Home Cage for Long-Term Continuous Phenotyping of Mouse Cognition and Behavior.‚Äù Cell Reports Methods 3 (7): 100532. https://doi.org/10.1016/j.crmeth.2023.100532.\n\n\nKarashchuk, Pierre, Katie L. Rupp, Evyn S. Dickinson, Sarah Walling-Bell, Elischa Sanders, Eiman Azim, Bingni W. Brunton, and John C. Tuthill. 2021. ‚ÄúAnipose: A Toolkit for Robust Markerless 3D Pose Estimation.‚Äù Cell Reports 36 (13): 109730. https://doi.org/10.1016/j.celrep.2021.109730.\n\n\nPereira, Talmo D., Joshua W. Shaevitz, and Mala Murthy. 2020. ‚ÄúQuantifying Behavior to Understand the Brain.‚Äù Nature Neuroscience 23 (12): 1537‚Äì49. https://doi.org/10.1038/s41593-020-00734-z.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>A mouse's daily activity log</span>"
    ]
  },
  {
    "objectID": "06-movement-zebras.html",
    "href": "06-movement-zebras.html",
    "title": "6¬† Zebra escape trajectories",
    "section": "",
    "text": "6.1 Dataset description\nIn this case study we will be using movement to quantify the collective behaviour of a herd of zebras in escape response. The data we will use is part of a larger dataset collected in Mpala (Kenya), in which researchers simulated a predation event to study the group response of the animals. We will demonstrate how we can compute useful metrics for this analysis using movement.\nThe 3.5-min dataset presented here consists of 44 trajectories of zebras (Equus quagga) expressed in a coordinate system fixed to the ground. Each individual has two keypoints (head and tail). The data was obtained as follows: first, a trained SLEAP model was run on a video clip recorded from a camera drone, to obtain the trajectories of the zebras in a coordinate system linked to the drone. Then, the trajectories were expressed in a coordinate system fixed to the ground using Structure-from-motion (with OpenSFM and OpenDroneMap). This transformation between camera and world coordinate systems is necessary to be able to disentangle the motion of the zebras from the movement of the camera drone. After this coordinate transformation, the data was cleaned by removing low-confidence keypoints and implausible data points.\nYou can find a detailed description of the approach as a collection of notebooks in this repository. Further details about the dataset and the prototype pipeline can be found in Duporge, Mi√±ano et al (2025).",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Zebra escape trajectories</span>"
    ]
  },
  {
    "objectID": "06-movement-zebras.html#dataset-description",
    "href": "06-movement-zebras.html#dataset-description",
    "title": "6¬† Zebra escape trajectories",
    "section": "",
    "text": "NoteAcknowledgement\n\n\n\nThe sample video and original SLEAP trajectories were kindly shared by Dr.¬†Isla Duporge from the Rubenstein Lab at Princeton University, with permission to use for this workshop. The trajectories in world coordinate system were computed by Sof√≠a Mi√±ano, Niko Sirmpilatze and Igor Tatarnikov.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Zebra escape trajectories</span>"
    ]
  },
  {
    "objectID": "06-movement-zebras.html#load-and-explore-the-dataset",
    "href": "06-movement-zebras.html#load-and-explore-the-dataset",
    "title": "6¬† Zebra escape trajectories",
    "section": "6.2 Load and explore the dataset",
    "text": "6.2 Load and explore the dataset\nFirst, let‚Äôs load and explore the dataset\n\nimport matplotlib.pyplot as plt\n\nimport numpy as np\nimport xarray as xr\n\nfrom movement import sample_data\nfrom movement.kinematics import compute_pairwise_distances, compute_speed\nfrom movement.transforms import scale\nfrom movement.utils.vector import compute_norm, convert_to_unit\nfrom movement.utils.reports import report_nan_values\n\nDownloading data from 'https://gin.g-node.org/neuroinformatics/movement-test-data/raw/master/metadata.yaml' to file '/home/runner/.movement/data/temp_metadata.yaml'.\nSHA256 hash of downloaded file: 22100193c76c0e3f274e4c986357c95fc645da1f5c67af41a8acfa91313205f4\nUse this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n\n\n\nds = sample_data.fetch_dataset(\"SLEAP_OSFM_zebras_drone.h5\")\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 7MB\nDimensions:      (time: 6294, space: 2, keypoints: 2, individuals: 44)\nCoordinates:\n  * time         (time) float64 50kB 0.0 0.03337 0.06673 ... 209.9 209.9 210.0\n  * space        (space) &lt;U1 8B 'x' 'y'\n  * keypoints    (keypoints) &lt;U1 8B 'H' 'T'\n  * individuals  (individuals) &lt;U8 1kB 'track_0' 'track_1' ... 'track_43'\nData variables:\n    position     (time, space, keypoints, individuals) float32 4MB 1.035e+03 ...\n    confidence   (time, keypoints, individuals) float32 2MB 0.9987 ... 1.006\nAttributes:\n    source_software:  SLEAP\n    ds_type:          poses\n    fps:              29.97\n    time_unit:        seconds\n    source_file:      /home/runner/.movement/data/poses/SLEAP_OSFM_zebras_dro...xarray.DatasetDimensions:time: 6294space: 2keypoints: 2individuals: 44Coordinates: (4)time(time)float640.0 0.03337 0.06673 ... 209.9 210.0array([0.000000e+00, 3.336670e-02, 6.673340e-02, ..., 2.099099e+02,\n       2.099433e+02, 2.099766e+02], shape=(6294,))space(space)&lt;U1'x' 'y'array(['x', 'y'], dtype='&lt;U1')keypoints(keypoints)&lt;U1'H' 'T'array(['H', 'T'], dtype='&lt;U1')individuals(individuals)&lt;U8'track_0' 'track_1' ... 'track_43'array(['track_0', 'track_1', 'track_10', 'track_11', 'track_12', 'track_13',\n       'track_14', 'track_15', 'track_16', 'track_17', 'track_18', 'track_19',\n       'track_2', 'track_20', 'track_21', 'track_22', 'track_23', 'track_24',\n       'track_25', 'track_26', 'track_27', 'track_28', 'track_29', 'track_3',\n       'track_30', 'track_31', 'track_32', 'track_33', 'track_34', 'track_35',\n       'track_36', 'track_37', 'track_38', 'track_39', 'track_4', 'track_40',\n       'track_41', 'track_42', 'track_5', 'track_6', 'track_7', 'track_8',\n       'track_9', 'track_43'], dtype='&lt;U8')Data variables: (2)position(time, space, keypoints, individuals)float321.035e+03 1.043e+03 ... 680.6 743.0array([[[[1035.0875 , 1042.7065 , 1305.2988 , ..., 1288.8965 ,\n          1075.2968 ,        nan],\n         [1055.7279 , 1056.1145 , 1287.899  , ..., 1272.5366 ,\n          1061.8053 ,        nan]],\n\n        [[1170.7526 , 1198.7987 , 1207.2275 , ..., 1196.4867 ,\n          1274.6531 ,        nan],\n         [1175.2821 , 1195.3256 , 1216.0687 , ..., 1210.1553 ,\n          1258.3647 ,        nan]]],\n\n\n       [[[1036.3914 , 1047.7836 , 1306.7142 , ..., 1290.4705 ,\n          1076.7327 ,        nan],\n         [1056.8123 , 1057.0919 , 1289.186  , ..., 1274.0607 ,\n          1062.8341 ,        nan]],\n\n        [[1170.8033 , 1197.9618 , 1207.1444 , ..., 1196.7869 ,\n          1274.542  ,        nan],\n         [1175.3003 , 1195.433  , 1216.0906 , ..., 1210.2795 ,\n          1258.3342 ,        nan]]],\n...\n          5037.79   , 4924.202  ],\n         [       nan, 4916.7803 ,        nan, ..., 5216.492  ,\n          5016.4253 , 4944.2705 ]],\n\n        [[       nan,  731.70557,        nan, ...,  994.94434,\n           689.243  ,  738.3227 ],\n         [       nan,  731.16486,        nan, ...,  975.09174,\n           680.5491 ,  743.00867]]],\n\n\n       [[[       nan, 4897.603  ,        nan, ..., 5213.083  ,\n          5037.834  , 4924.2344 ],\n         [       nan, 4916.813  ,        nan, ..., 5216.505  ,\n          5016.4824 , 4944.314  ]],\n\n        [[       nan,  731.7324 ,        nan, ...,  994.9447 ,\n           689.2539 ,  738.3456 ],\n         [       nan,  731.19904,        nan, ...,  975.0833 ,\n           680.563  ,  743.02875]]]],\n      shape=(6294, 2, 2, 44), dtype=float32)confidence(time, keypoints, individuals)float320.9987 0.9696 ... 0.9946 1.006array([[[0.9986721 , 0.96957946, 0.97588646, ..., 0.9835917 ,\n         0.99211425,        nan],\n        [1.006721  , 1.0021206 , 0.99066794, ..., 0.99203897,\n         1.0121652 ,        nan]],\n\n       [[0.9872179 , 0.97352976, 0.97786933, ..., 0.98417157,\n         0.99206424,        nan],\n        [1.0023106 , 1.0009985 , 0.9916664 , ..., 0.9919399 ,\n         0.9985494 ,        nan]],\n\n       [[1.002729  , 1.0058758 , 0.9846221 , ..., 0.9828853 ,\n         0.9683708 ,        nan],\n        [1.0021431 , 0.9725971 , 0.9884142 , ..., 0.99417496,\n         0.9847228 ,        nan]],\n\n       ...,\n\n       [[       nan, 0.99612695,        nan, ..., 0.98065114,\n         0.98576516, 0.99649894],\n        [       nan, 0.99007297,        nan, ..., 0.99566007,\n         0.9955021 , 1.0055838 ]],\n\n       [[       nan, 0.9971069 ,        nan, ..., 0.98106676,\n         0.98654073, 0.9961379 ],\n        [       nan, 0.9902725 ,        nan, ..., 0.9969691 ,\n         0.9945675 , 1.0057868 ]],\n\n       [[       nan, 0.9971091 ,        nan, ..., 0.9822346 ,\n         0.9864464 , 0.99721533],\n        [       nan, 0.9900413 ,        nan, ..., 0.9947883 ,\n         0.9946445 , 1.0055223 ]]], shape=(6294, 2, 44), dtype=float32)Indexes: (4)timePandasIndexPandasIndex(Index([                 0.0, 0.033366700033366704,  0.06673340006673341,\n         0.1001001001001001,  0.13346680013346682,  0.16683350016683351,\n         0.2002002002002002,   0.2335669002335669,  0.26693360026693363,\n         0.3003003003003003,\n       ...\n         209.67634300967634,   209.70970970970973,   209.74307640974308,\n         209.77644310977644,   209.80980980980982,   209.84317650984318,\n         209.87654320987656,    209.9099099099099,    209.9432766099433,\n         209.97664330997665],\n      dtype='float64', name='time', length=6294))spacePandasIndexPandasIndex(Index(['x', 'y'], dtype='object', name='space'))keypointsPandasIndexPandasIndex(Index(['H', 'T'], dtype='object', name='keypoints'))individualsPandasIndexPandasIndex(Index(['track_0', 'track_1', 'track_10', 'track_11', 'track_12', 'track_13',\n       'track_14', 'track_15', 'track_16', 'track_17', 'track_18', 'track_19',\n       'track_2', 'track_20', 'track_21', 'track_22', 'track_23', 'track_24',\n       'track_25', 'track_26', 'track_27', 'track_28', 'track_29', 'track_3',\n       'track_30', 'track_31', 'track_32', 'track_33', 'track_34', 'track_35',\n       'track_36', 'track_37', 'track_38', 'track_39', 'track_4', 'track_40',\n       'track_41', 'track_42', 'track_5', 'track_6', 'track_7', 'track_8',\n       'track_9', 'track_43'],\n      dtype='object', name='individuals'))Attributes: (5)source_software :SLEAPds_type :posesfps :29.97time_unit :secondssource_file :/home/runner/.movement/data/poses/SLEAP_OSFM_zebras_drone.h5\n\n\nWe can see the poses dataset ds is made up of two data arrays, position and confidence. In this example, we will use the position data array only, which spans four dimensions: time, space, keypoints and individuals. We can verify there are 44 individuals in this dataset (track_0 to track_43) and two keypoints per individual, labelled H (head) and T (tailbase). The data was collected at 29.97 frames per second, and the dataloader used this information to automatically express the time dimension in seconds.\n\n\n\n\n\n\nNotePosition units\n\n\n\nThe position data in ds is expressed in arbitrary units. This is because no GPS data was available for georeferencing or defining ground control points in the structure-from-motion (SfM) analysis. As a result, the scale factor remains a free parameter in the reconstruction of the world coordinates.\nNote however that this will not be a problem for our analysis, since the relative positions between the individuals are still correct. Moreover, we will use the median zebra body length to scale the data to more informative units. For more details on the coordinate systems involved in SfM analysis see the OpenSfM documentation.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Zebra escape trajectories</span>"
    ]
  },
  {
    "objectID": "06-movement-zebras.html#compute-the-body-length-per-individual",
    "href": "06-movement-zebras.html#compute-the-body-length-per-individual",
    "title": "6¬† Zebra escape trajectories",
    "section": "6.3 Compute the body length per individual",
    "text": "6.3 Compute the body length per individual\nWe define the body vector for each individual as the vector going from the T keypoint (tail) to the H keypoint (head).\n\nbody_vector = ds.position.sel(keypoints=\"H\") - ds.position.sel(keypoints=\"T\")\n\nbody_vector\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'position' (time: 6294, space: 2, individuals: 44)&gt; Size: 2MB\n-20.64 -13.41 17.4 8.969 21.4 -4.405 ... -0.1141 3.393 4.625 19.86 8.691 -4.683\nCoordinates:\n  * time         (time) float64 50kB 0.0 0.03337 0.06673 ... 209.9 209.9 210.0\n  * space        (space) &lt;U1 8B 'x' 'y'\n  * individuals  (individuals) &lt;U8 1kB 'track_0' 'track_1' ... 'track_43'xarray.DataArray'position'time: 6294space: 2individuals: 44-20.64 -13.41 17.4 8.969 21.4 ... 3.393 4.625 19.86 8.691 -4.683array([[[-20.64038   , -13.407959  ,  17.39978   , ...,  16.359863  ,\n          13.491455  ,          nan],\n        [ -4.529541  ,   3.4731445 ,  -8.841187  , ..., -13.668579  ,\n          16.28833   ,          nan]],\n\n       [[-20.420898  ,  -9.30835   ,  17.528198  , ...,  16.40979   ,\n          13.89856   ,          nan],\n        [ -4.4969482 ,   2.5288086 ,  -8.946167  , ..., -13.492676  ,\n          16.207764  ,          nan]],\n\n       [[-20.80188   , -17.699585  ,  17.320312  , ...,  16.389404  ,\n          14.212769  ,          nan],\n        [ -4.6690674 ,   4.485962  ,  -8.845093  , ..., -13.594238  ,\n          15.8845215 ,          nan]],\n\n       ...,\n\n       [[         nan, -19.214844  ,          nan, ...,  -3.4248047 ,\n          21.367676  , -20.073242  ],\n        [         nan,   0.55041504,          nan, ...,  19.87909   ,\n           8.734253  ,  -4.6884766 ]],\n\n       [[         nan, -19.21045   ,          nan, ...,  -3.4033203 ,\n          21.364746  , -20.06836   ],\n        [         nan,   0.54071045,          nan, ...,  19.8526    ,\n           8.693909  ,  -4.685974  ]],\n\n       [[         nan, -19.20996   ,          nan, ...,  -3.421875  ,\n          21.351562  , -20.07959   ],\n        [         nan,   0.53338623,          nan, ...,  19.86139   ,\n           8.690918  ,  -4.6831665 ]]], shape=(6294, 2, 44), dtype=float32)Coordinates: (3)time(time)float640.0 0.03337 0.06673 ... 209.9 210.0array([0.000000e+00, 3.336670e-02, 6.673340e-02, ..., 2.099099e+02,\n       2.099433e+02, 2.099766e+02], shape=(6294,))space(space)&lt;U1'x' 'y'array(['x', 'y'], dtype='&lt;U1')individuals(individuals)&lt;U8'track_0' 'track_1' ... 'track_43'array(['track_0', 'track_1', 'track_10', 'track_11', 'track_12', 'track_13',\n       'track_14', 'track_15', 'track_16', 'track_17', 'track_18', 'track_19',\n       'track_2', 'track_20', 'track_21', 'track_22', 'track_23', 'track_24',\n       'track_25', 'track_26', 'track_27', 'track_28', 'track_29', 'track_3',\n       'track_30', 'track_31', 'track_32', 'track_33', 'track_34', 'track_35',\n       'track_36', 'track_37', 'track_38', 'track_39', 'track_4', 'track_40',\n       'track_41', 'track_42', 'track_5', 'track_6', 'track_7', 'track_8',\n       'track_9', 'track_43'], dtype='&lt;U8')Indexes: (3)timePandasIndexPandasIndex(Index([                 0.0, 0.033366700033366704,  0.06673340006673341,\n         0.1001001001001001,  0.13346680013346682,  0.16683350016683351,\n         0.2002002002002002,   0.2335669002335669,  0.26693360026693363,\n         0.3003003003003003,\n       ...\n         209.67634300967634,   209.70970970970973,   209.74307640974308,\n         209.77644310977644,   209.80980980980982,   209.84317650984318,\n         209.87654320987656,    209.9099099099099,    209.9432766099433,\n         209.97664330997665],\n      dtype='float64', name='time', length=6294))spacePandasIndexPandasIndex(Index(['x', 'y'], dtype='object', name='space'))individualsPandasIndexPandasIndex(Index(['track_0', 'track_1', 'track_10', 'track_11', 'track_12', 'track_13',\n       'track_14', 'track_15', 'track_16', 'track_17', 'track_18', 'track_19',\n       'track_2', 'track_20', 'track_21', 'track_22', 'track_23', 'track_24',\n       'track_25', 'track_26', 'track_27', 'track_28', 'track_29', 'track_3',\n       'track_30', 'track_31', 'track_32', 'track_33', 'track_34', 'track_35',\n       'track_36', 'track_37', 'track_38', 'track_39', 'track_4', 'track_40',\n       'track_41', 'track_42', 'track_5', 'track_6', 'track_7', 'track_8',\n       'track_9', 'track_43'],\n      dtype='object', name='individuals'))Attributes: (0)\n\n\nWe can compute the body length of each individual by computing the norm of the body vector.\n\n# Compute body length per individual\nbody_length = compute_norm(body_vector)\n\nIt would be useful to check if there are missing values in the body length array. We can quickly inspect this using movement‚Äôs report_nan_values function.\n\nprint(report_nan_values(body_length))\n\nMissing points (marked as NaN) in position:\n\nindividuals\ntrack_0       234/6294 (3.72%)\ntrack_1       286/6294 (4.54%)\ntrack_10      614/6294 (9.76%)\ntrack_11      369/6294 (5.86%)\ntrack_12      519/6294 (8.25%)\ntrack_13      420/6294 (6.67%)\ntrack_14      353/6294 (5.61%)\ntrack_15       434/6294 (6.9%)\ntrack_16     827/6294 (13.14%)\ntrack_17      241/6294 (3.83%)\ntrack_18      483/6294 (7.67%)\ntrack_19      196/6294 (3.11%)\ntrack_2      645/6294 (10.25%)\ntrack_20      557/6294 (8.85%)\ntrack_21      576/6294 (9.15%)\ntrack_22        88/6294 (1.4%)\ntrack_23     633/6294 (10.06%)\ntrack_24       85/6294 (1.35%)\ntrack_25       43/6294 (0.68%)\ntrack_26     813/6294 (12.92%)\ntrack_27       315/6294 (5.0%)\ntrack_28    1208/6294 (19.19%)\ntrack_29      127/6294 (2.02%)\ntrack_3       224/6294 (3.56%)\ntrack_30      235/6294 (3.73%)\ntrack_31       91/6294 (1.45%)\ntrack_32        25/6294 (0.4%)\ntrack_33      362/6294 (5.75%)\ntrack_34       447/6294 (7.1%)\ntrack_35     742/6294 (11.79%)\ntrack_36     741/6294 (11.77%)\ntrack_37     766/6294 (12.17%)\ntrack_38      415/6294 (6.59%)\ntrack_39    1254/6294 (19.92%)\ntrack_4        17/6294 (0.27%)\ntrack_40      523/6294 (8.31%)\ntrack_41      263/6294 (4.18%)\ntrack_42      236/6294 (3.75%)\ntrack_5        93/6294 (1.48%)\ntrack_6      718/6294 (11.41%)\ntrack_7       432/6294 (6.86%)\ntrack_8      731/6294 (11.61%)\ntrack_9       388/6294 (6.16%)\ntrack_43      548/6294 (8.71%)\n\n\nThe output shows that the number of missing values per individual varies between 0.27% and 19.92%. This is not necessarily a problem for our analysis, but it is something to keep in mind when interpreting the results. These missing points are likely due to imperfect tracking of one or both of the keypoints required to compute the body vector.\nLet‚Äôs compute some basic statistics to get a sense of the distribution of the body length values.\n\n# Compute basic statistics\nbody_length_std = body_length.std()\nbody_length_mean = body_length.mean()\nbody_length_median = body_length.median()\n\nprint(f\"Body length mean: {body_length_mean:.2f} a.u.\")  # a.u.: arbitrary units\nprint(f\"Body length median: {body_length_median:.2f} a.u.\")\nprint(f\"Body length std: {body_length_std:.2f} a.u.\")\n\nBody length mean: 20.45 a.u.\nBody length median: 20.43 a.u.\nBody length std: 2.32 a.u.\n\n\nWe can also plot the distribution of body lengths.\n\n\nCode\nfig, ax = plt.subplots()\n\n# plot histogram of body length values\ncounts, bins, _ = body_length.plot.hist(bins=100)\n\n# add reference lines for mean and mean +- 2 stds\nax.vlines(\n    body_length_mean,\n    ymin=0,\n    ymax=np.max(counts),\n    color=\"red\",\n    linestyle=\"-\",\n    label=\"mean body length\",\n)\nlower_bound = body_length_mean - 2 * body_length_std\nupper_bound = body_length_mean + 2 * body_length_std\nfor bound in [lower_bound, upper_bound]:\n    ax.vlines(\n        bound,\n        ymin=0,\n        ymax=np.max(counts),\n        color=\"red\",\n        linestyle=\"--\",\n        label=\"mean +- 2 std\",\n    )\nax.set_ylim(0, np.max(counts))\nax.set_xlabel(\"body length (a.u.)\")\nax.set_ylabel(\"counts\")\nax.legend()\n\n\n\n\n\n\n\n\nFigure¬†6.1: Distribution of zebra body lengths.\n\n\n\n\n\nWe can see there is some variability in the body lengths per individual. Part of it may reflect the diversity across individuals, but from visual inspection of the video we expect the majority of it to be due to imperfect tracking of the keypoints. To remove some of these outliers, we continue the analysis considering only body vectors that are within 2 standard deviations of the mean.\n\nbody_vector_filtered = body_vector.where(\n    np.logical_and(\n        body_length &lt;= body_length_mean + 2 * body_length_std,\n        body_length &gt;= body_length_mean - 2 * body_length_std,\n    )\n)",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Zebra escape trajectories</span>"
    ]
  },
  {
    "objectID": "06-movement-zebras.html#compute-polarization",
    "href": "06-movement-zebras.html#compute-polarization",
    "title": "6¬† Zebra escape trajectories",
    "section": "6.4 Compute polarization",
    "text": "6.4 Compute polarization\nWe would now like to inspect the orientation of each individual in relation to the group while the simulated escape events take place.\nFor this, we first compute each animal‚Äôs unit body vector. These are a scaled version of the body vectors we just computed, normalised to have unit length. movement provides a convenience function to do this:\n\nbody_vector_filtered_unit = convert_to_unit(body_vector_filtered)\n\nWe can quickly check if their norms are now equal to 1.\n\nprint(compute_norm(body_vector_filtered_unit)) \n\n&lt;xarray.DataArray 'position' (time: 6294, individuals: 44)&gt; Size: 1MB\n1.0 nan 1.0 1.0 1.0 1.0 1.0 1.0 1.0 nan ... 1.0 1.0 nan 1.0 1.0 1.0 1.0 1.0 1.0\nCoordinates:\n  * time         (time) float64 50kB 0.0 0.03337 0.06673 ... 209.9 209.9 210.0\n  * individuals  (individuals) &lt;U8 1kB 'track_0' 'track_1' ... 'track_43'\n\n\nWe now define the herd vector as the mean of the unit body vectors across all individuals. The mean vector of a set of \\(n\\) vectors is the sum of all the vectors (i.e., the resultant vector) scaled by \\(1/n\\).\n\nherd_vector = body_vector_filtered_unit.mean(\"individuals\")\nprint(herd_vector) \n\n&lt;xarray.DataArray 'position' (time: 6294, space: 2)&gt; Size: 50kB\n0.5835 -0.4923 0.5589 -0.4968 0.5252 ... 0.4129 0.3367 0.4131 0.3156 0.4332\nCoordinates:\n  * time     (time) float64 50kB 0.0 0.03337 0.06673 ... 209.9 209.9 210.0\n  * space    (space) &lt;U1 8B 'x' 'y'\n\n\nThe resulting array has (time, space) dimensions, which means that we have a single herd vector defined at each timestep.\nThe norm of the herd vector already gives us an intuition of how aligned the whole herd is. When its norm is close to 1, it means that the majority of the unit body vectors are aligned. When its norm is close to 0, it means that the unit body vectors are dispersed. The norm of the herd vector is sometimes called polarization.\n\npolarization = compute_norm(herd_vector)\n\nWe can plot the evolution of the polarization over time to get a sense of how the herd‚Äôs alignment changes.\n\nfig, ax = plt.subplots()\nax.plot(herd_vector.time, polarization)\nax.set_ylabel(\"polarization\")\nax.set_xlabel(\"time (s)\")\nax.grid()\n\n\n\n\n\n\n\nFigure¬†6.2: Evolution of the herd‚Äôs polarization over time.\n\n\n\n\n\nThe plot suggests that the herd alternates between periods of higher and lower polarization.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Zebra escape trajectories</span>"
    ]
  },
  {
    "objectID": "06-movement-zebras.html#compute-average-speed-of-the-herd",
    "href": "06-movement-zebras.html#compute-average-speed-of-the-herd",
    "title": "6¬† Zebra escape trajectories",
    "section": "6.5 Compute average speed of the herd",
    "text": "6.5 Compute average speed of the herd\nWe would also like to inspect how the speed of the herd changes over the course of the simulated escape events.\nFirst, let‚Äôs scale the position data to express it in units of body lengths (BL). This will make the results more interpretable. We can use movement‚Äôs scale function to do this.\n\nposition_scaled = scale(\n    ds.position, \n    factor=1 / body_length_median.item(), \n    space_unit=\"body_length\",\n)\n\nFor simplicity, we would also like to reduce the position of each individual to a single point. A good candidate for this is the centroid, which is the mean of all the keypoints per individual. In our case, the centroid will be the midpoint between the head and tail keypoints.\n\ncentroid = position_scaled.mean(\"keypoints\")\n\n\n\n\n\n\n\nTipExercise A\n\n\n\nUse the centroid data array to:\n\nCompute centroid_speed, the speed of each individual‚Äôs centroid.\nCompute herd_speed, the average speed across all individuals.\nPlot the evolution of herd_speed over time.\n\n\n\n\n\n\n\n\n\nTipClick to reveal the answers\n\n\n\n\n\n\n# Compute speed of each zebra's centroid\ncentroid_speed = compute_speed(centroid)\n\n# Compute the average speed across all individuals\nherd_speed = centroid_speed.mean(\"individuals\")\n\n# Plot the evolution of the herd speed over time\nfig, ax = plt.subplots()\nax.plot(herd_speed.time, herd_speed)\nax.set_ylabel(\"herd speed (BL/s)\")\nax.set_xlabel(\"time (s)\")\nax.grid()\n\n\n\n\n\n\n\n\nWe can see that there are four periods in the dataset in which the speed of the herd surpasses 2 BL/s for about 10 to 20 seconds.\n\n\n\nWe can also inspect how the speed of each individual changes over time.\n\n\nCode\nfig, ax = plt.subplots()\nim = ax.matshow(\n    centroid_speed,\n    aspect=\"auto\",\n    cmap=\"viridis\",\n)\n\n# convert frames to seconds in y-axis\ntime_ticks_step = 1498\ntime_ticks = np.arange(0, len(centroid_speed.time), time_ticks_step) \ntime_labels = [f\"{t:.0f}\" for t in centroid_speed.time.values[0:-1:time_ticks_step]]\nax.set_yticks(time_ticks)\nax.set_yticklabels(time_labels)\nax.tick_params(axis='x', bottom=True, top=False, labelbottom=True, labeltop=False)\n\nax.set_xlabel(\"individual\")\nax.set_ylabel(\"time (s)\") \n\n# add colorbar\ncbar = plt.colorbar(im)\ncbar.set_label(\"speed (BL/s)\")\nax.get_images()[0].set_clim(0, 6) # cap values at 6 BL/s\n\n\n\n\n\n\n\n\nFigure¬†6.3: Evolution of the speed of each individual over time.\n\n\n\n\n\nThe plot suggests that the individuals are quite coordinated also in their change of speed. We can clearly see the four periods of higher speed, which correspond to the four simulated escape events.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Zebra escape trajectories</span>"
    ]
  },
  {
    "objectID": "06-movement-zebras.html#polarization-vs-speed",
    "href": "06-movement-zebras.html#polarization-vs-speed",
    "title": "6¬† Zebra escape trajectories",
    "section": "6.6 Polarization vs speed",
    "text": "6.6 Polarization vs speed\nLet‚Äôs put it all together and inspect how polarization changes with the speed of the herd.\nWe will use the logarithm of the speed to focus on the higher range of speeds.\n\nlog10_herd_speed = np.log10(herd_speed)\n\nWe can now plot the polarization in time, colouring the points by the logarithm of the speed.\n\nfig, ax = plt.subplots()\nsc = ax.scatter(\n    x=polarization.time,\n    y=polarization,\n    c=log10_herd_speed,\n    s=5,\n    cmap=\"turbo\",\n    # rescale color map to 1st and 99th percentiles\n    vmin=log10_herd_speed.quantile(0.01).item(),\n    vmax=log10_herd_speed.quantile(0.99).item(),\n)\n\n\nax.set_xlabel(\"time (s)\")\nax.set_ylabel(\"polarization\")\n\ncbar = plt.colorbar(sc)\ncbar.set_label(\"log10 herd speed (BL/s)\")\n\n\n\n\n\n\n\nFigure¬†6.4: Periods of highest polarization are associated with higher speeds\n\n\n\n\n\nThe plot shows that for this dataset, the periods of highest polarization are associated with higher speeds. This is consistent with the interpretation that the zebras become more aligned when escaping at speed, and more dispersed when they are at rest.\n\n\n\n\nDuporge, Isla, Sofia Minano, Nikoloz Sirmpilatze, Igor Tatarnikov, Scott Wolf, Adam L. Tyson, and Daniel Rubenstein. 2025. ‚ÄúTracking the Flight: Exploring a Computational Framework for Analyzing Escape Responses in Plains Zebra (Equus Quagga).‚Äù arXiv. https://doi.org/10.48550/arXiv.2505.16882.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Zebra escape trajectories</span>"
    ]
  },
  {
    "objectID": "prerequisites.html",
    "href": "prerequisites.html",
    "title": "Appendix A ‚Äî Prerequisites",
    "section": "",
    "text": "A.1 Knowledge\nWe assume basic familiarity with Python, ideally including its core scientific libraries such as NumPy, Pandas, Matplotlib, and Jupyter.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Prerequisites</span>"
    ]
  },
  {
    "objectID": "prerequisites.html#hardware",
    "href": "prerequisites.html#hardware",
    "title": "Appendix A ‚Äî Prerequisites",
    "section": "A.2 Hardware",
    "text": "A.2 Hardware\nThis is a hands-on course, so please bring your own laptop and charger.\nA mouse is strongly recommended, especially for tasks like image annotation.\nA dedicated GPU is not required, though it may speed up some computations.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Prerequisites</span>"
    ]
  },
  {
    "objectID": "prerequisites.html#software",
    "href": "prerequisites.html#software",
    "title": "Appendix A ‚Äî Prerequisites",
    "section": "A.3 Software",
    "text": "A.3 Software\nYou‚Äôll need both general tools for Python programming and specific software required for the course, as detailed below.\n\nA.3.1 General development tools\n\n\n\n\n\n\nNote\n\n\n\nIf you already have a working Anaconda or Miniconda installation and have used it to run Python scripts or Jupyter notebooks, you can likely skip the steps below.\n\n\nTo prepare your computer for Python development, we recommend following the Software Carpentries installation instructions, in particular:\n\nBash Shell, to run terminal commands\nGit, including a GitHub account\nPython, via the conda-forge installer. Please make sure you install a Python version &gt;= 3.12 (e.g.¬†3.12 is fine, 3.10 is not).\n\nYou‚Äôll also need a code editor (IDE) configured for Python.\nIf you already have one you‚Äôre comfortable with, feel free to use it. Otherwise, we recommend:\n\nVisual Studio Code with the Python extension\nJupyterLab\n\n\n\nA.3.2 For the SLEAP tutorial\nPlease install SLEAP following the legacy installation instructions.\n\n\n\n\n\n\nNote\n\n\n\nFor this workshop, use SLEAP version 1.3.4. Be sure to replace the default version number (e.g.¬†1.4.1) in the instructions with 1.3.4.\n\n\nThis should create a conda environment named sleap with the necessary dependencies. You can verify the installation by running:\nconda activate sleap\nsleap-label\nThis should launch the SLEAP graphical user interface (GUI).\n\n\nA.3.3 For the interactive notebooks\nYou will also need a separate conda environment with everything required for the interactive exercises, including the movement and jupyter packages.\nWe recommend cloning this workshop‚Äôs repository and creating the environment using the provided environment.yaml file:\ngit clone https://github.com/neuroinformatics-unit/course-animals-in-motion.git\ncd course-animals-in-motion\nconda env create -n animals-in-motion-env -f environment.yaml\nTo test your setup, run:\nconda activate animals-in-motion-env\nmovement launch\nThis should open the movement GUI, i.e.¬†the napari image viewer with the movement plugin docked on the right.\n\n\n\n\n\n\nNote\n\n\n\nThere are other ways to install the movement package.\nHowever, for this workshop, we recommend using the environment.yaml file to ensure that all necessary dependencies, including those beyond movement, are included.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Prerequisites</span>"
    ]
  },
  {
    "objectID": "prerequisites.html#sec-data",
    "href": "prerequisites.html#sec-data",
    "title": "Appendix A ‚Äî Prerequisites",
    "section": "A.4 Data",
    "text": "A.4 Data\nBringing your own data is encouraged but not required. This could include video recordings of animal behaviour and/or motion tracking data you‚Äôve previously generated.\nWe also provide some example datasets for you to use during the workshop. Please download these from Dropbox before the workshop starts (they are a few GB in size).\nThe Dropbox folder is structured as follows:\nAnimals-in-Motion_2025-08/\n‚îú‚îÄ‚îÄ CalMS21/\n‚îÇ   ‚îú‚îÄ‚îÄ better_model/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 250806_174722.centroid.n=679/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 250807_162146.multi_class_topdown.n=679/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ predictions/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ mouse044_task1_annotator1.test.pkg.slp\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ mouse044_task1_annotator1.train.pkg.slp\n‚îÇ   ‚îú‚îÄ‚îÄ calms21_task1_train.json\n‚îÇ   ‚îú‚îÄ‚îÄ mouse044_task1_annotator1.mp4\n‚îÇ   ‚îú‚îÄ‚îÄ mouse044_task1_annotator1.slp\n‚îÇ   ‚îî‚îÄ‚îÄ readme.md\n‚îî‚îÄ‚îÄ Smart-Kages.zip\n\nCalMS21: Contains an example video from the Caltech Mouse Social Interactions (CalMS21) Dataset (Sun et al. 2021), SLEAP labels and trained models.\n\nbetter_model: SLEAP multi-animal top-down ID model trained on 679 labelled frames.\n\n250806_174722.centroid.n=679: Centroid model.\n250807_162146.multi_class_topdown.n=679: Top-down ID model.\npredictions: Model predictions on the full video.\nmouse044_task1_annotator1.test.pkg.slp: Held-out test set containing 2715 labelled frames (80%) randomly sampled from the full annotation file.\nmouse044_task1_annotator1.train.pkg.slp: Training set containing 679 labelled frames (20%) randomly sampled from the full annotation file.\n\ncalms21_task1_train.json: MARS (Segalin et al. 2021) pose estimates provided in the CalMS21 dataset.\nmouse044_task1_annotator1.mp4: Video file used in Chapter 3.\nmouse044_task1_annotator1.slp: Fully annotated SLEAP labels file containing 3394 labelled frames used in Chapter 4 (converted from calms21_task1_train.json). The train/test splits above were derived from this file using a 20:80 random partition.\nreadme.md: Information about the dataset (from the CalMS21 dataset).\n\nSmart-Kages.zip: Compressed dataset used in Chapter 5.\n\n\n\n\n\nSegalin, Cristina, Jalani Williams, Tomomi Karigo, May Hui, Moriel Zelikowsky, Jennifer J Sun, Pietro Perona, David J Anderson, and Ann Kennedy. 2021. ‚ÄúThe Mouse Action Recognition System (MARS) Software Pipeline for Automated Analysis of Social Behaviors in Mice.‚Äù Edited by Gordon J Berman, Kate M Wassum, and Asaf Gal. eLife 10 (November): e63720. https://doi.org/10.7554/eLife.63720.\n\n\nSun, Jennifer J., Tomomi Karigo, David J. Anderson, Pietro Perona, Yisong Yue, and Ann Kennedy. 2021. ‚ÄúCaltech Mouse Social Interactions (CalMS21) Dataset.‚Äù CaltechDATA. https://doi.org/10.22002/D1.1991.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Prerequisites</span>"
    ]
  },
  {
    "objectID": "contributing.html",
    "href": "contributing.html",
    "title": "Appendix B ‚Äî Contributing",
    "section": "",
    "text": "B.1 Setting up the development environment\nThank you for considering contributing to the Animals In Motion project! We welcome contributions in various forms, including bug reports, requests for content improvement, as well as new tutorials or case studies.\nBegin by cloning the repository and navigating to its root directory:\nWe use conda to manage dependencies. First, create a development environment using the environment-dev.yaml file, and activate it:\nTo enable the pre-commit hooks, run the following command once:\nThis is a Quarto book project, with its source code located in the book/ directory. We refer you to the Quarto documentation for more information on how books are structured and configured.\nTo render/preview the book locally, you‚Äôll need the Quarto CLI installed, as well as the VSCode Quarto extension\nYou will also need to make sure that the QUARTO_PYTHON environment variable is set to the path of the python executable in the development conda environment. This guarantees that the Quarto CLI will use the correct Python interpreter when rendering the book.\nThen, you can render the book using:\nYou can view the rendered book by opening the book/_book/index.html file in your browser.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>¬† <span class='chapter-title'>Contributing</span>"
    ]
  },
  {
    "objectID": "contributing.html#setting-up-the-development-environment",
    "href": "contributing.html#setting-up-the-development-environment",
    "title": "Appendix B ‚Äî Contributing",
    "section": "",
    "text": "git clone https://github.com/neuroinformatics-unit/course-animals-in-motion.git\ncd course-animals-in-motion\n\nconda env create -n animals-in-motion-dev -f environment-dev.yaml\nconda activate animals-in-motion-dev\n\npre-commit install\n\n\n\nexport QUARTO_PYTHON=$(which python)\n\nquarto render book\n# or if you want to run executable code blocks before rendering to html\nquarto render book --execute",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>¬† <span class='chapter-title'>Contributing</span>"
    ]
  },
  {
    "objectID": "contributing.html#authoring-content",
    "href": "contributing.html#authoring-content",
    "title": "Appendix B ‚Äî Contributing",
    "section": "B.2 Authoring content",
    "text": "B.2 Authoring content\nBook chapters are written primarily as Quarto Markdown files (.qmd). These can contain a mix of narrative and interactive content, such as code exercises. See Quarto computations &gt; Using Python to learn more about executable code blocks.\nWe recommend using the Quarto VSCode extension for authoring and previewing content.\nAlternatively, you may also use JupyterLab, with Jupyter Notebooks (.ipynb) as source files‚Äîsee Quarto tools &gt; JupyterLab for more information.\nThe chapter source files reside in the book/ directory and have to be linked in the book/_quarto.yml file for them to show up. See Book Crossrefs on how to reference other chapters.\nBibliographical references should be added to the book/references.bib file in BibTeX format. See Quarto authoring &gt; Citations for more information.\nIn general, cross-referencing objects (e.g.¬†figures, tables, chapters, equations, citations, etc.) should be done using the @ref syntax, e.g.¬†See @fig-overview for more details.\n\nB.2.1 Adding answers to exercises\nThis book is configured to be rendered with or without answers to exercises, using Quarto profiles.\n\nThe _quarto.yml file defines the ‚Äúdefault‚Äù profile for the book, which does not show the answers to exercises.\nThe _quarto-answers.yml file defines the ‚Äúanswers‚Äù profile, which is identical to the ‚Äúdefault‚Äù profile, but also includes solutions to code exercises.\n\nTo add answers to code exercises, please enclose them in a block of the following form:\n::: {.content-visible when-profile=\"answers\"}\n\n::: {.callout-tip title=\"Click to reveal the answers\" collapse=\"true\"}\n\nWrite your solution here.\n\n:::\n\n:::\nThen you can control whether the answers are shown or not by passing the appropriate Quarto profile to the quarto render command:\nquarto render book --execute --profile default  # equivalent to no profile\nquarto render book --execute --profile answers\nYou can achieve the same effect by setting the QUARTO_PROFILE environment variable before rendering the book:\nexport QUARTO_PROFILE=answers\nquarto render book --execute\nIn general, it‚Äôs most convenient to show the answers while you are developing the content, and then hide them to preview the book as a student would see it.\nSee the Section B.4 for more information on how to create releases with or without answers.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>¬† <span class='chapter-title'>Contributing</span>"
    ]
  },
  {
    "objectID": "contributing.html#pre-commit-hooks",
    "href": "contributing.html#pre-commit-hooks",
    "title": "Appendix B ‚Äî Contributing",
    "section": "B.3 Pre-commit hooks",
    "text": "B.3 Pre-commit hooks\nWe use pre-commit to run checks on the codebase before committing.\nCurrent hooks include:\n\ncodespell for catching common spelling mistakes.\nmarkdownlint for (Quarto) Markdown linting and formatting.\nruff for code linting and formatting.\n\nThese will prevent code from being committed if any of these hooks fail. To run all the hooks before committing:\npre-commit run  # for staged files\npre-commit run -a  # for all files in the repository",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>¬† <span class='chapter-title'>Contributing</span>"
    ]
  },
  {
    "objectID": "contributing.html#sec-versioning",
    "href": "contributing.html#sec-versioning",
    "title": "Appendix B ‚Äî Contributing",
    "section": "B.4 Versioning and releasing",
    "text": "B.4 Versioning and releasing\nWe use Calendar Versioning (CalVer) and specifically the YYYY.0M scheme (e.g.¬†2025.08 for August 2025).\nTo create a new release, first update the book/index.qmd file. Specifically, add two rows like the following to the ‚ÄúVersions‚Äù table:\n| `v2025.08` | version used for the inaugural workshop in August 2025 |\n| `v2025.08-answers` | same as `v2025.08` but with answers to exercises |\nYou also need to create a new tag in the vYYYY.0M format (e.g.¬†v2025.08) and push it to the repository. Don‚Äôt forget the v prefix for the tag name!\nFor example:\ngit tag v2025.08\ngit push origin --tags",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>¬† <span class='chapter-title'>Contributing</span>"
    ]
  },
  {
    "objectID": "contributing.html#continuous-integration-ci",
    "href": "contributing.html#continuous-integration-ci",
    "title": "Appendix B ‚Äî Contributing",
    "section": "B.5 Continuous integration (CI)",
    "text": "B.5 Continuous integration (CI)\nThe CI workflow is defined in the .github/workflows/build_and_deploy.yaml file and can be triggered by:\n\nPushes to the main branch\nPull requests\nReleases, i.e.¬†tags starting with v (e.g., v2025.08)\nManual dispatches\n\nThe workflow is built using GitHub actions and includes three jobs:\n\nlinting: running the pre-commit hooks;\nbuild: rendering the Quarto book with and without answers, and uploading the rendered artifacts;\ndeploy: deploying the book artifact(s) to the gh-pages branch (only for pushes to the main branch and releases).\n\nEach release version is deployed to a folder in the gh-pages branch, with the same name as the release tag (e.g., v2025.08). This is accompanied by a vYYYY.0M-answers folder containing a version of the book with answers to exercises (e.g.¬†v2025.08-answers).\nThere‚Äôs also a special folder called dev that is deployed for pushes to the main branch. This folder always includes the answers to exercises.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>¬† <span class='chapter-title'>Contributing</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Appendix C ‚Äî References",
    "section": "",
    "text": "Anderson, David J., and Pietro Perona. 2014. ‚ÄúToward a\nScience of Computational\nEthology.‚Äù Neuron 84 (1): 18‚Äì31. https://doi.org/10.1016/j.neuron.2014.09.005.\n\n\nBeane, Glen, Brian Q. Geuther, Thomas J. Sproule, Anshul Choudhary,\nJarek Trapszo, Leinani Hession, Vivek Kohar, and Vivek Kumar. 2023.\n‚ÄúJAX Animal Behavior\nSystem (JABS): A Video-Based\nPhenotyping Platform for the Laboratory Mouse.‚Äù bioRxiv. https://doi.org/10.1101/2022.01.13.476229.\n\n\nBerman, Gordon J., Daniel M. Choi, William Bialek, and Joshua W.\nShaevitz. 2014. ‚ÄúMapping the Stereotyped Behaviour of Freely\nMoving Fruit Flies.‚Äù Journal of The Royal Society\nInterface 11 (99): 20140672. https://doi.org/10.1098/rsif.2014.0672.\n\n\nBiderman, Dan, Matthew R. Whiteway, Cole Hurwitz, Nicholas Greenspan,\nRobert S. Lee, Ankit Vishnubhotla, Richard Warren, et al. 2024.\n‚ÄúLightning Pose: Improved Animal Pose Estimation via\nSemi-Supervised Learning, Bayesian Ensembling and\nCloud-Native Open-Source Tools.‚Äù Nature Methods 21 (7):\n1316‚Äì28. https://doi.org/10.1038/s41592-024-02319-1.\n\n\nBlau, Ari, Evan S. Schaffer, Neeli Mishra, Nathaniel J. Miska,\nInternational Brain Laboratory, Liam Paninski, and Matthew R. Whiteway.\n2024. ‚ÄúA Study of Animal Action Segmentation Algorithms Across\nSupervised, Unsupervised, and Semi-Supervised Learning\nParadigms.‚Äù Neurons, Behavior, Data Analysis, and\nTheory, December, 1‚Äì46. https://doi.org/10.51628/001c.127770.\n\n\nBohnslav, James P, Nivanthika K Wimalasena, Kelsey J Clausing, Yu Y Dai,\nDavid A Yarmolinsky, Tom√°s Cruz, Adam D Kashlan, et al. 2021.\n‚ÄúDeepEthogram, a Machine Learning Pipeline for\nSupervised Behavior Classification from Raw Pixels.‚Äù Edited by\nMackenzie W Mathis, Timothy E Behrens, Mackenzie W Mathis, and Johannes\nBohacek. eLife 10 (September): e63377. https://doi.org/10.7554/eLife.63377.\n\n\nBradski, G. 2000. ‚ÄúThe OpenCV Library.‚Äù\nDr. Dobb‚Äôs Journal of Software Tools.\n\n\nChindemi, Giuseppe, Benoit Girard, and Camilla Bellone. 2023.\n‚ÄúLISBET: A Machine Learning Model for the Automatic Segmentation\nof Social Behavior Motifs.‚Äù https://arxiv.org/abs/2311.04069.\n\n\n‚ÄúCRISPR Ants Lose Ability to Smell.‚Äù 2017.\nNature 548 (7667): 263‚Äì63. https://doi.org/10.1038/d41586-017-02337-4.\n\n\nDatta, Sandeep Robert, David J. Anderson, Kristin Branson, Pietro\nPerona, and Andrew Leifer. 2019. ‚ÄúComputational\nNeuroethology: A Call to\nAction.‚Äù Neuron 104 (1): 11‚Äì24. https://doi.org/10.1016/j.neuron.2019.09.038.\n\n\nDe Almeida, Tulio Fernandes, Bruno Guedes Spinelli, Ram√≥n Hypolito Lima,\nMaria Carolina Gonzalez, and Abner Cardoso Rodrigues. 2022.\n‚ÄúPyRAT: An\nOpen-Source Python\nLibrary for Animal Behavior\nAnalysis.‚Äù Frontiers in Neuroscience 16. https://www.frontiersin.org/articles/10.3389/fnins.2022.779106.\n\n\nDunn, Timothy W., Jesse D. Marshall, Kyle S. Severson, Diego E.\nAldarondo, David G. C. Hildebrand, Selmaan N. Chettih, William L. Wang,\net al. 2021. ‚ÄúGeometric Deep Learning Enables 3D\nKinematic Profiling Across Species and Environments.‚Äù Nature\nMethods 18 (5): 564‚Äì73. https://doi.org/10.1038/s41592-021-01106-6.\n\n\nDuporge, Isla, Sofia Minano, Nikoloz Sirmpilatze, Igor Tatarnikov, Scott\nWolf, Adam L. Tyson, and Daniel Rubenstein. 2025. ‚ÄúTracking the\nFlight: Exploring a Computational\nFramework for Analyzing Escape\nResponses in Plains Zebra\n(Equus Quagga).‚Äù arXiv. https://doi.org/10.48550/arXiv.2505.16882.\n\n\nFriard, Olivier, and Marco Gamba. 2016. ‚ÄúBORIS: A\nFree, Versatile Open-Source Event-Logging Software for Video/Audio\nCoding and Live Observations.‚Äù Methods in Ecology and\nEvolution 7 (11): 1325‚Äì30. https://doi.org/10.1111/2041-210X.12584.\n\n\nGallois, Benjamin, and Rapha√´l Candelier. 2021.\n‚ÄúFastTrack: An Open-Source Software for\nTracking Varying Numbers of Deformable Objects.‚Äù PLOS\nComputational Biology 17 (2): e1008697. https://doi.org/10.1371/journal.pcbi.1008697.\n\n\nGoodwin, Nastacia L., Jia J. Choong, Sophia Hwang, Kayla Pitts, Liana\nBloom, Aasiya Islam, Yizhe Y. Zhang, et al. 2024. ‚ÄúSimple\nBehavioral Analysis (SimBA) as a\nPlatform for Explainable Machine Learning in Behavioral\nNeuroscience.‚Äù Nature Neuroscience, May, 1‚Äì14. https://doi.org/10.1038/s41593-024-01649-9.\n\n\nGraser, Anita. 2019. ‚ÄúMovingPandas:\nEfficient Structures for Movement\nData in Python.‚Äù GI_Forum\n2019, Volume 7, (June): 54‚Äì68. https://doi.org/10.1553/giscience2019_01_s54.\n\n\nGraving, Jacob M, Daniel Chae, Hemal Naik, Liang Li, Benjamin Koger,\nBlair R Costelloe, and Iain D Couzin. 2019.\n‚ÄúDeepPoseKit, a Software Toolkit for Fast and Robust\nAnimal Pose Estimation Using Deep Learning.‚Äù Edited by Ian T\nBaldwin, Josh W Shaevitz, Josh W Shaevitz, and Greg Stephens.\neLife 8 (October): e47994. https://doi.org/10.7554/eLife.47994.\n\n\nG√ºnel, Semih, Helge Rhodin, Daniel Morales, Jo√£o Campagnolo, Pavan\nRamdya, and Pascal Fua. 2019. ‚ÄúDeepFly3D, a Deep\nLearning-Based Approach for 3D Limb and Appendage Tracking\nin Tethered, Adult Drosophila.‚Äù Edited by Timothy\nO‚ÄôLeary, Ronald L Calabrese, and Josh W Shaevitz. eLife 8\n(October): e48571. https://doi.org/10.7554/eLife.48571.\n\n\nHo, Hinze, Nejc Kejzar, Hiroki Sasaguri, Takashi Saito, Takaomi C.\nSaido, Bart De Strooper, Marius Bauza, and Julija Krupic. 2023. ‚ÄúA\nFully Automated Home Cage for Long-Term Continuous Phenotyping of Mouse\nCognition and Behavior.‚Äù Cell Reports Methods 3 (7):\n100532. https://doi.org/10.1016/j.crmeth.2023.100532.\n\n\nHsu, Alexander I., and Eric A. Yttri. 2021. ‚ÄúB-SOiD,\nan Open-Source Unsupervised Algorithm for Identification and Fast\nPrediction of Behaviors.‚Äù Nature Communications 12 (1):\n5188. https://doi.org/10.1038/s41467-021-25420-x.\n\n\nHu, Yujia, Carrie R. Ferrario, Alexander D. Maitland, Rita B. Ionides,\nAnjesh Ghimire, Brendon Watson, Kenichi Iwasaki, et al. 2023.\n‚ÄúLabGym: Quantification of User-Defined\nAnimal Behaviors Using Learning-Based Holistic Assessment.‚Äù\nCell Reports Methods 0 (0). https://doi.org/10.1016/j.crmeth.2023.100415.\n\n\nKabra, Mayank, Alice A. Robie, Marta Rivera-Alba, Steven Branson, and\nKristin Branson. 2013. ‚ÄúJAABA: Interactive Machine\nLearning for Automatic Annotation of Animal Behavior.‚Äù Nature\nMethods 10 (1): 64‚Äì67. https://doi.org/10.1038/nmeth.2281.\n\n\nKarashchuk, Pierre, Katie L. Rupp, Evyn S. Dickinson, Sarah\nWalling-Bell, Elischa Sanders, Eiman Azim, Bingni W. Brunton, and John\nC. Tuthill. 2021. ‚ÄúAnipose: A Toolkit for Robust\nMarkerless 3D Pose Estimation.‚Äù Cell\nReports 36 (13): 109730. https://doi.org/10.1016/j.celrep.2021.109730.\n\n\nKoolhaas Jaap M., de Boer Sietse F., Coppens Caroline M. 2013.\n‚ÄúThe Resident-Intruder Paradigm: A Standardized Test for\nAggression, Violence and Social Stress.‚Äù JoVE, no. 77:\ne4367. https://doi.org/doi:10.3791/4367.\n\n\nKrakauer, John W., Asif A. Ghazanfar, Alex Gomez-Marin, Malcolm A.\nMacIver, and David Poeppel. 2017. ‚ÄúNeuroscience Needs\nBehavior: Correcting a\nReductionist Bias.‚Äù Neuron 93\n(3): 480‚Äì90. https://doi.org/10.1016/j.neuron.2016.12.041.\n\n\nLauer, Jessy, Mu Zhou, Shaokai Ye, William Menegas, Steffen Schneider,\nTanmay Nath, Mohammed Mostafizur Rahman, et al. 2022.\n‚ÄúMulti-Animal Pose Estimation, Identification and Tracking with\nDeepLabCut.‚Äù Nature Methods 19 (4):\n496‚Äì504. https://doi.org/10.1038/s41592-022-01443-0.\n\n\nLevitis, Daniel A., William Z. Lidicker, and Glenn Freund. 2009.\n‚ÄúBehavioural Biologists Don‚Äôt Agree on What Constitutes\nBehaviour.‚Äù Animal Behaviour 78 (1): 103‚Äì10. https://doi.org/10.1016/j.anbehav.2009.03.018.\n\n\nLopes, Gon√ßalo, Niccol√≤ Bonacchi, Jo√£o Fraz√£o, Joana P. Neto, Bassam V.\nAtallah, Sofia Soares, Lu√≠s Moreira, et al. 2015. ‚ÄúBonsai: An\nEvent-Based Framework for Processing and Controlling Data\nStreams.‚Äù Frontiers in Neuroinformatics 9. https://www.frontiersin.org/articles/10.3389/fninf.2015.00007.\n\n\nLuxem, Kevin, Petra Mocellin, Falko Fuhrmann, Johannes K√ºrsch, Stephanie\nR. Miller, Jorge J. Palop, Stefan Remy, and Pavol Bauer. 2022.\n‚ÄúIdentifying Behavioral Structure from Deep Variational Embeddings\nof Animal Motion.‚Äù Communications Biology 5 (1): 1‚Äì15.\nhttps://doi.org/10.1038/s42003-022-04080-7.\n\n\nLuxem, Kevin, Jennifer J Sun, Sean P Bradley, Keerthi Krishnan, Eric\nYttri, Jan Zimmermann, Talmo D Pereira, and Mark Laubach. 2023.\n‚ÄúOpen-Source Tools for Behavioral Video Analysis:\nSetup, Methods, and Best Practices.‚Äù Edited by\nDenise J Cai and Laura L Colgin. eLife 12 (March): e79305. https://doi.org/10.7554/eLife.79305.\n\n\nMartinez, Romain, Benjamin Michaud, and Mickael Begon. 2020.\n‚Äú‚ÄòPyomeca‚Äò: An Open-Source Framework for Biomechanical\nAnalysis.‚Äù Journal of Open Source Software 5 (53): 2431.\nhttps://doi.org/10.21105/joss.02431.\n\n\nMathis, Alexander, Pranav Mamidanna, Kevin M. Cury, Taiga Abe, Venkatesh\nN. Murthy, Mackenzie Weygandt Mathis, and Matthias Bethge. 2018.\n‚ÄúDeepLabCut: Markerless Pose Estimation of\nUser-Defined Body Parts with Deep Learning.‚Äù Nature\nNeuroscience 21 (9): 1281‚Äì89. https://doi.org/10.1038/s41593-018-0209-y.\n\n\nMathis, Alexander, Steffen Schneider, Jessy Lauer, and Mackenzie\nWeygandt Mathis. 2020. ‚ÄúA Primer on\nMotion Capture with Deep\nLearning: Principles, Pitfalls,\nand Perspectives.‚Äù Neuron 108 (1): 44‚Äì65.\nhttps://doi.org/10.1016/j.neuron.2020.09.017.\n\n\nMiranda, Lucas, Joeri Bordes, Benno P√ºtz, Mathias V. Schmidt, and\nBertram M√ºller-Myhsok. 2023. ‚ÄúDeepOF: A Python Package for\nSupervised and Unsupervised Pattern Recognition in Mice Motion Tracking\nData.‚Äù Journal of Open Source Software 8 (86): 5394. https://doi.org/10.21105/joss.05394.\n\n\nOverall, Rupert W. 2024. ColonyTrack: Analysis of Multi-Subject\nTracking Data from Interconnected Cage Networks.\n\n\nOverall, Rupert W, Sara Zocher, Alexander Garthe, and Gerd Kempermann.\n2020. ‚ÄúRtrack: A Software Package for Reproducible Automated Water\nMaze Analysis.‚Äù bioRxiv 2020.02.27.967372. https://doi.org/10.1101/2020.02.27.967372.\n\n\nPappalardo, Luca, Filippo Simini, Gianni Barlacchi, and Roberto\nPellungrini. 2022. ‚ÄúScikit-Mobility: A Python Library for the\nAnalysis, Generation, and Risk Assessment of Mobility Data.‚Äù\nJournal of Statistical Software 103 (1): 1‚Äì38. https://doi.org/10.18637/jss.v103.i04.\n\n\nPereira, Talmo D., Joshua W. Shaevitz, and Mala Murthy. 2020.\n‚ÄúQuantifying Behavior to Understand the Brain.‚Äù Nature\nNeuroscience 23 (12): 1537‚Äì49. https://doi.org/10.1038/s41593-020-00734-z.\n\n\nPereira, Talmo D., Nathaniel Tabris, Arie Matsliah, David M. Turner,\nJunyu Li, Shruthi Ravindranath, Eleni S. Papadoyannis, et al. 2022.\n‚ÄúSLEAP: A Deep Learning System for\nMulti-Animal Pose Tracking.‚Äù Nature Methods 19 (4):\n486‚Äì95. https://doi.org/10.1038/s41592-022-01426-1.\n\n\nRoald-Arb√∏l, Mikkel. 2024. ‚ÄúAnimovement: An r Toolbox for\nAnalysing Animal Movement Across Space and Time.‚Äù http://www.roald-arboel.com/animovement/.\n\n\nRomero-Ferrero, Francisco, Mattia G. Bergomi, Robert C. Hinz, Francisco\nJ. H. Heras, and Gonzalo G. de Polavieja. 2019. ‚ÄúIdtracker.ai:\nTracking All Individuals in Small or Large Collectives of Unmarked\nAnimals.‚Äù Nature Methods 16 (2): 179‚Äì82. https://doi.org/10.1038/s41592-018-0295-5.\n\n\nSchweihoff, Jens F., Alexander I. Hsu, Martin K. Schwarz, and Eric A.\nYttri. 2022. ‚ÄúA-SOiD, an Active Learning Platform for\nExpert-Guided, Data Efficient Discovery of Behavior.‚Äù bioRxiv. https://doi.org/10.1101/2022.11.04.515138.\n\n\nSegalin, Cristina, Jalani Williams, Tomomi Karigo, May Hui, Moriel\nZelikowsky, Jennifer J Sun, Pietro Perona, David J Anderson, and Ann\nKennedy. 2021. ‚ÄúThe Mouse Action Recognition System (MARS)\nSoftware Pipeline for Automated Analysis of Social Behaviors in\nMice.‚Äù Edited by Gordon J Berman, Kate M Wassum, and Asaf Gal.\neLife 10 (November): e63720. https://doi.org/10.7554/eLife.63720.\n\n\nSirmpilatze, Niko, Sof√≠a Mi√±ano, Chang Huan Lo, Adam Tyson, Will Graham,\nStella Prins, Brandon Peri, et al. 2025.\n‚ÄúNeuroinformatics-Unit/Movement: V0.9.0.‚Äù Zenodo. https://doi.org/10.5281/zenodo.16754905.\n\n\nSun, Jennifer J., Tomomi Karigo, David J. Anderson, Pietro Perona,\nYisong Yue, and Ann Kennedy. 2021. ‚ÄúCaltech Mouse Social\nInteractions (CalMS21) Dataset.‚Äù CaltechDATA. https://doi.org/10.22002/D1.1991.\n\n\nTinbergen, Niko. 1951. The Study of\nInstinct. Clarendon Press.\n\n\nTomar, Suramya. 2006. ‚ÄúConverting Video Formats with\nFFmpeg.‚Äù Linux Journal 2006 (146): 10.\n\n\nWalter, Tristan, and Iain D Couzin. 2021. ‚ÄúTRex, a\nFast Multi-Animal Tracking System with Markerless Identification, and\n2D Estimation of Posture and Visual Fields.‚Äù Edited\nby David Lentink, Christian Rutz, and Sergi Pujades. eLife 10\n(February): e64000. https://doi.org/10.7554/eLife.64000.\n\n\nWeinreb, Caleb. 2024. ‚ÄúKeypoint-MoSeq: Parsing\nBehavior by Linking Point Tracking to Pose Dynamics.‚Äù Nature\nMethods 21.\n\n\nWiltschko, Alexander B., Matthew J. Johnson, Giuliano Iurilli, Ralph E.\nPeterson, Jesse M. Katon, Stan L. Pashkovski, Victoria E. Abraira, Ryan\nP. Adams, and Sandeep Robert Datta. 2015. ‚ÄúMapping\nSub-Second Structure in\nMouse Behavior.‚Äù Neuron 88\n(6): 1121‚Äì35. https://doi.org/10.1016/j.neuron.2015.11.031.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>¬† <span class='chapter-title'>References</span>"
    ]
  }
]