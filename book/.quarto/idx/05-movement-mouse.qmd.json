{"title":"A mouse's daily activity log","markdown":{"headingText":"A mouse's daily activity log","headingAttr":{"id":"sec-movement-mouse","classes":[],"keyvalue":[]},"containsRefs":false,"markdown":"\nIn this case study, we'll be using the [`movement` package](https://movement.neuroinformatics.dev/) to dive into mouse home cage monitoring data acquired in [Smart-Kages](https://cambridgephenotyping.com/products) and tracked with [DeepLabCut](https://www.deeplabcut.org/). We'll explore how mouse activity levels fluctuate throughout the day.\n\nBefore you get started, make sure you've set up the `animals-in-motion-env` environment (refer to [prerequisites @sec-install-movement]) and are using it to run this code. You'll also need to download the `Smart-Kages.zip` archive from [Dropbox](https://www.dropbox.com/scl/fo/81ug5hoy9msc7v7bteqa0/AH32RLdbZqWZJstIeR4YHZY?rlkey=blgagtaizw8aac5areja6h7q1&st=w1zueyi9&dl=0) (see [prerequisites @sec-data]) and unzip it.\n\n## Import libraries\n\n```{python}\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport xarray as xr\nfrom matplotlib import colormaps\n\nfrom movement import sample_data\nfrom movement.filtering import filter_by_confidence, rolling_filter\nfrom movement.kinematics import compute_speed\nfrom movement.plots import plot_occupancy\nfrom movement.roi import PolygonOfInterest\nfrom movement.transforms import scale\n\n```\n\n```{python}\n#| include: false\n\nxr.set_options(\n    display_expand_attrs=False,\n    display_expand_coords=False,\n    keep_attrs=True,\n)\n```\n\n## The Smart-Kages dataset\n\n::: {.callout-note title=\"Acknowledgement\"}\nThis dataset was kindly shared by [Dr. Loukia Katsouri](https://www.sainsburywellcome.org/web/people/loukia-katsouri) from the [O'Keefe Lab](https://www.sainsburywellcome.org/web/groups/okeefe-lab), with permission to use for this workshop.\n:::\n\nThe Smart-Kages dataset comprises home cage recordings from two mice,\neach housed in a specialised [Smart-Kage](https://cambridgephenotyping.com/products) [@ho_fully_2023]—a home cage\nmonitoring system equipped with a camera mounted atop the cage.\n\nThe camera captures data around the clock at a rate of 2 frames per second, saving a video\nsegment for each hour of the day. A pre-trained DeepLabCut model is then used\nto predict 8 keypoints on the mouse's body.\n\nLet's examine the contents of the downloaded data.\nYou will need to specify the path to the unzipped `Smart-Kages` folder on your machine.\n\n```{python}\n# Replace with the path to the unzipped Smart-Kages folder on your machine\nsmart_kages_path = Path.home() / \".movement\" / \"Smart-Kages\"\n\n# Let's visualise the contents of the folder\nfiles = [f.name for f in smart_kages_path.iterdir()]\nfiles.sort()\nfor file in files:\n    print(file)\n```\n\nThe tracking data are stored in two`.nc` (netCDF) files: `kage14` and `kage17`.\nnetCDF is an HDF5-based file format that can be natively saved/loaded by the `xarray` library, and is therefore [convenient to use with `movement`](https://movement.neuroinformatics.dev/user_guide/input_output.html#native-saving-and-loading-with-netcdf).\n\nApart from these, we also have two `.png` files: `kage14_background.png` and `kage17_background.png`, which constitute frames extracted from the videos.\n\nLet's take a look at them.\n\n```{python}\n#| label: fig-background-frames\n#| fig-cap: \"Top-down camera views of the Smart-Kage habitats\"\n#| code-fold: true\n\nkages = [\"kage14\", \"kage17\"]\nimg_paths = [smart_kages_path / f\"{kage}_background.png\" for kage in kages]\nimages = [plt.imread(img_path) for img_path in img_paths]\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(8, 4))\n\nfor i, img in enumerate(images):\n    axes[i].imshow(img)\n    axes[i].set_title(f\"{kages[i]}\")\n    axes[i].axis(\"off\")\n\n```\n\n::: {.callout-tip title=\"Questions A\"}\n\n1. What objects do you see in the habitat?\n2. What challenges do you anticipate with tracking a mouse in this environment?\n3. What are the trade-offs one has to consider when designing a continuous monitoring system?\n:::\n\n::: {.content-visible when-profile=\"answers\"}\n\n::: {.callout-tip title=\"Click to reveal the answers\" collapse=\"true\"}\n\n1. The habitat contains several objects, including the mouse's nest, a running wheel, a climbing platform, a toilet paper roll, and a corridor with two arms near the top.\n2. The presence of these objects is likely to cause occlusions, meaning the mouse or parts of it may frequently be obscured from the camera's view.\n3. The requirements for animal welfare, such as providing a rich environment, often conflict with the needs of the tracking system. While a single black mouse on a white background would be straightforward to track, such conditions would not support the mouse's well-being over time, nor would they yield naturalistic behavior.\n:::\n\n:::\n\nLet's load and inspect the tracking data:\n\n```{python}\nds_kages = {}  # a dictionary to store kage name -> xarray dataset\n\nfor kage in [\"kage14\", \"kage17\"]:\n    ds_kages[kage] = xr.open_dataset(smart_kages_path / f\"{kage}.nc\")\n\nds_kages[\"kage14\"]   # Change to \"kage17\" to inspect the other dataset\n```\n\nWe see that each dataset contains a huge amount of data, but the two datasets are\nnot exactly aligned in time.\n\n```{python}\n#| code-fold: true\n\nstart_times = {\n    name: pd.Timestamp(ds.time.isel(time=0).values)\n    for name, ds in ds_kages.items()\n}\nend_times = {\n    name: pd.Timestamp(ds.time.isel(time=-1).values)\n    for name, ds in ds_kages.items()\n}\n\nfor name in start_times.keys():\n    print(f\"{name}: from {start_times[name]} to {end_times[name]}\")\n```\n\n## Datetime Coordinates\n\nYou might notice something interesting about the time coordinates in these `xarray` datasets: they're given in `datetime64[ns]` format, which means they're precise timestamps expressed in \"calendar time\".\n\nThis is different from what we've seen before in other `movement` datasets, where time coordinates are expressed as seconds elapsed since the start of the video, or \"elapsed time\".\n\n::: {.callout-note title=\"How did we get these timestamps?\" collapse=\"true\"}\n\nSome recording systems can output timestamps for each video frame. In our case, the raw data from the Smart-Kage system included the start datetime of each 1-hour-long video segment and the precise time difference between the start of each segment and every frame within it.\n\nUsing this information, we were able to reconstruct precise datetime coordinates for all frames throughout the entire experiment. We then concatenated the DeepLabCut predictions from all video segments and assigned the datetime coordinates to the resulting dataset. If you're interested in the details, you can find the code in the [smart-kages-movement GitHub repository](https://github.com/neuroinformatics-unit/smart-kages-movement).\n\n:::\n\nUsing \"calendar time\" is convenient for many applications.\nFor example, we could cross-reference the tracking results against other data sources, such as body weight measurements.\n\nIt also allows us to easily select time windows by datetime.\nWe will leverage this here to select a time window that's common to both kages.\nNote that we discard the last few days because the experimenter introduced\nsome interventions during that time, which are out of scope for this case study.\n\n```{python}\n#| output: false\n\ncommon_start = \"2024-04-09 00:00:00\"\ncommon_end = \"2024-05-07 00:00:00\"\n\nfor kage in [\"kage14\", \"kage17\"]:\n    ds_kages[kage] = ds_kages[kage].sel(time=slice(common_start, common_end))\n```\n\nBeyond this ability to select time windows by date and time,\nwe will see many other benefits of using datetime coordinates in the rest of this case study.\n\nThat said, it's still useful to also know the total time elapsed since the start of the experiment.\nIn fact, many `movement` functions will expect \"elapsed time\" and may not work with datetime coordinates (for now).\n\nLuckily, it's easy to convert datetime coordinates to \"elapsed time\" by simply subtracting the start datetime of the whole experiment from each timestamp.\n\n```{python}\n#| code-fold: true\n#| code-summary: \"Expand to see how this can be done\"\n\nds_14 = ds_kages[\"kage14\"]\n\n# Get the start datetime the experiment in kage14\nexperiment_start = ds_14.time.isel(time=0).data\n\n# Subtract the start datetime from each timestamp\ntime_elapsed = (ds_14.time.data - np.datetime64(experiment_start))\n\n# Convert to seconds\nseconds_elapsed = time_elapsed / pd.Timedelta(\"1s\")\n\n# Assign the seconds_elapsed coordinate to the \"time\" dimension\nds_14 = ds_14.assign_coords(seconds_elapsed=(\"time\", seconds_elapsed))\n```\n\nWe've pre-computed this for convenience and stored it in a secondary time coordinate called `seconds_elapsed`.\n\n```{python}\nprint(ds_14.coords[\"time\"].values[:2])\nprint(ds_14.coords[\"seconds_elapsed\"].values[:2])\n```\n\nWhenever we want to switch to \"elapsed time\" mode, we can simply set the `seconds_elapsed` coordinates as the \"index\" of the `time` dimension. This means that `seconds_elapsed` will be used as the primary time coordinate, allowing us to select data by it.\n\n```{python}\n#| output: false\n\nds_14.set_index(time=\"seconds_elapsed\").sel(time=slice(0, 1800))\n```\n\n::: {.callout-tip title=\"Exercise A\"}\n\nFor each of the two kages:\n\n1. Plot the x-axis position of the mouse's body center over time, for the week starting on April 15th. What do you notice?\n2. Plot the median confidence of the body center for each day, over the entire duration of the experiment.\n:::\n\n::: {.content-visible when-profile=\"answers\"}\n\n::: {.callout-tip title=\"Click to reveal the answers\" collapse=\"true\"}\n\n```{python}\nfig, axes = plt.subplots(nrows=2, ncols=1, figsize=(8, 5), sharex=True)\n\nfor i, kage in enumerate([\"kage14\", \"kage17\"]):\n    ds = ds_kages[kage].squeeze()  # remove redundant \"individuals\" dimension\n    body_center = ds.position.sel(\n        keypoints=\"bodycenter\",\n        time=slice(\"2024-04-15 00:00:00\", \"2024-04-21 23:59:59\"),\n        space=\"x\"\n    )\n    body_center.plot.line(ax=axes[i])\n    axes[i].set_title(f\"{kage} body center x\")\n\nplt.tight_layout()\nplt.show()\n```\n\n```{python}\nfig, axes = plt.subplots(nrows=2, ncols=1, figsize=(8, 5), sharex=True)\n\nfor i, kage in enumerate([\"kage14\", \"kage17\"]):\n    ds = ds_kages[kage].squeeze()\n    ds_daily = ds.resample(time=\"1D\").median()\n    ds_daily.confidence.sel(keypoints=\"bodycenter\").plot.line(ax=axes[i])\n    axes[i].set_title(f\"{kage} body center\")\n\nplt.tight_layout()\nplt.show()\n```\n\n:::\n\n:::\n\n## Cleaning the data\n\nLet's examine the range of confidence values for each keypoint.\n\n```{python}\n#| label: fig-confidence-range\n#| fig-cap: \"Confidence range by keypoint\"\n#| code-fold: true\n\nkage = \"kage14\"\nconfidence = ds_kages[kage].confidence.squeeze()\n\nfig, ax = plt.subplots(figsize=(8, 3))\nconfidence.quantile(q=0.25, dim=\"time\").plot.line(\"o--\", color=\"gray\", ax=ax, label=\"25% quantile\")\nconfidence.quantile(q=0.75, dim=\"time\").plot.line(\"o--\", color=\"gray\", ax=ax, label=\"75% quantile\")\nconfidence.median(dim=\"time\").plot.line(\"o-\", color=\"black\", ax=ax, label=\"median\")\n\nax.legend()\nax.set_title(f\"{kage} confidence range\")\nplt.show()\n```\n\nIt looks like the \"neck\", \"bodycenter\", \"spine1\", and \"spine2\" keypoints\nare the most confidently detected.\nLet us define a list of \"reliable\" keypoints for later use.\nThese are all on the mouse's body.\n\n```{python}\nreliable_keypoints = [\"neck\", \"bodycenter\", \"spine1\", \"spine2\"]\n```\n\nWe can filter out low-confidence predictions.\n\n```{python}\nconfidence_threshold = 0.95\n\nfor kage, ds in ds_kages.items():\n    ds[\"position_filtered\"] = filter_by_confidence(\n        ds.position,\n        ds.confidence,\n        threshold=confidence_threshold,\n    )\n```\n\n::: {.callout-tip title=\"Exercise B\"}\n\nLet's smooth the data with a rolling median filter.\n\n**Hint**: Remember doing this in @sec-movement-intro ?\n:::\n\n::: {.content-visible when-profile=\"answers\"}\n\n::: {.callout-tip title=\"Click to reveal the answers\" collapse=\"true\"}\n\n```{python}\nwindow_size = 3  # frames\n\nfor kage, ds in ds_kages.items():\n    ds[\"position_smoothed\"] = rolling_filter(\n        ds.position_filtered,\n        window_size,\n        statistic=\"median\",\n    )\n```\n\n:::\n\n:::\n\n## Plot the mouse's speed over time\n\nLet's define a single-point representation of the mouse's position, which we'll call the `body_centroid`.\nWe derive this by taking the mean of the 4 reliable keypoints, using their smoothed positions.\n\n```{python}\nfor kage, ds in ds_kages.items():\n    ds[\"body_centroid\"] = ds.position_filtered.sel(\n        individuals=\"individual_0\",  # the only individual in the dataset\n        keypoints=reliable_keypoints\n    ).mean(dim=\"keypoints\")\n```\n\nNext, we'll compute the body centroid's speed in cm/sec via the following steps:\n\n1. Convert the body centroid position data to cm units using [`scale()`](https://movement.neuroinformatics.dev/api/movement.transforms.scale.html).\n2. Temporarily switch to \"elapsed time\" mode, because [`compute_speed()`](https://movement.neuroinformatics.dev/api/movement.kinematics.compute_speed.html) does not (yet) support datetime coordinates.\n3. Compute the speed in cm/sec\n4. Restore the original datetime coordinates to the speed data.\n\n```{python}\nPIXELS_PER_CM = 10\n\nfor kage, ds in ds_kages.items():\n    # Scale from pixels to cm using a known conversion factor\n    body_centroid_cm = scale(\n        ds.body_centroid, factor=1 / PIXELS_PER_CM, space_unit=\"cm\"\n    )\n\n    # Compute the speed in cm/sec\n    ds[\"body_centroid_speed\"] = compute_speed(\n       body_centroid_cm.set_index(time=\"seconds_elapsed\")  # switch time coords\n    ).assign_coords(time=body_centroid_cm.time)            # restore datetime\n\nds_kages[\"kage14\"].body_centroid_speed\n```\n\nLet's plot the speed over time.\n\n```{python}\n#| label: fig-body-speed\n#| fig-cap: \"Body centroid speed\"\n\nfig, axes = plt.subplots(\n    nrows=2, ncols=1, figsize=(8, 5), sharex=True, sharey=True\n)\n\nfor i, kage in enumerate([\"kage14\", \"kage17\"]):\n    ds_kages[kage].body_centroid_speed.plot.line(ax=axes[i])\n    axes[i].set_title(f\"{kage} body centroid\")\n    axes[i].set_ylabel(\"speed (cm/sec)\")\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.callout-tip title=\"Questions B\"}\n\n1. What are potential sources of error in the speed calculation?\n2. What do you notice about the overall speed fluctuations over time? What do you think is the reason for this?\n3. Do you notice any differences between the two kages? Feel free to \"zoom in\" on specific time windows to investigate this.\n:::\n\n::: {.content-visible when-profile=\"answers\"}\n\n::: {.callout-tip title=\"Click to reveal the answers\" collapse=\"true\"}\n\n**Potential sources of error in speed calculation:** The accuracy of speed computation is heavily reliant on precise position data. Any inaccuracies in predicting keypoint positions (i.e., the discrepancy between actual and estimated positions) can be exacerbated when calculating speed, as it is derived from the differences between consecutive positions. Additionally, we have assumed a fixed conversion factor of 10 pixels per cm. While this value may be accurate for the habitat's floor, the environment is not entirely flat. It includes objects for climbing, which means the conversion factor will vary depending on the mouse's vertical position (distance from the camera).\n\nThe speed seems to fluctuate in a circadian pattern, especially apparent in kage14.\nThis will become clearer if we zoom in on a time window of a few days.\n\n```{python}\n#| code-fold: true\n\ntime_window = slice(\"2024-04-22\", \"2024-04-26\")\n\nfig, axes = plt.subplots(\n    nrows=2, ncols=1, figsize=(7.5, 5), sharex=True, sharey=True\n)\n\nfor i, kage in enumerate([\"kage14\", \"kage17\"]):\n    speed = ds_kages[kage].body_centroid_speed.sel(time=time_window)\n    speed.plot.line(ax=axes[i])\n    axes[i].set_title(f\"{kage} body centroid\")\n    axes[i].set_ylabel(\"speed (pixels/sec)\")\n\nplt.tight_layout()\nplt.show()\n\n```\n\nWe also note that the circadian pattern is more pronounced in kage14,\nand seems to be somewhat disrupted in kage17. The mouse in kage17\nseems to be more active overall, both during the day and at night.\n:::\n\n:::\n\n::: {.callout-note title=\"A side-note on 3D pose estimation\" collapse=\"true\"}\n\nWith only a single top-down view of the mouse, we are limited to 2D pose estimation.\nThis means the estimated keypoint coordinates are simply projections of the true 3D coordinates onto the 2D image plane.\n\nThis is the most common approach to pose estimation, but it cannot accurately measure true dimensions.\nAny conversion from pixels to physical units (e.g. centimetres) will be imprecise, and sometimes significantly so.\n\nThis limitation can be overcome by using multiple cameras from different viewpoints and performing 3D pose estimation.\nThere are two main markerless approaches:\n\n1. The first approach is to do 'regular' 2D pose estimation in each camera view,\n   then triangulate across camera views to estimate 3D pose. The triangulation\n   relies on known parameters about the cameras and their relative positions\n   and orientations. [Anipose](https://anipose.readthedocs.io/en/latest/)\n   [@karashchuk_anipose_2021] is a popular open-source toolkit that\n   implements this approach.\n\n   ![Source: @pereira_quantifying_2020](img/pose_estimation_3D.png)\n\n2. The second approach, implemented in [DANNCE](https://github.com/spoonsso/dannce)\n   [@dunn_geometric_2021], is to use a fully 3D convolutional neural network (CNN) that\n   can learn about 3D image features and how cameras and landmarks\n   relate to one another in 3D space.\n\n   ![Source: <https://github.com/spoonsso/dannce>](img/DANNCE.png)\n\nSome **prompts for discussion**:\n\n- What are the pros and cons of 2D vs 3D markerless pose estimation?\n- In which scenarios would you prefer one over the other?\n\n:::\n\n## Plot actograms {#sec-actograms}\n\nAn actogram is a visualisation of the mouse's activity level over time.\n\nAs a measure of activity we'll use the cumulative distance traversed\nby the mouse's body centroid in a given time bin—in this case, 10 minutes.\nSince we have already computed the speed (cm/sec), we can multiply that by the time bin duration in seconds to get the distance (cm).\n\n```{python}\nfor kage in [\"kage14\", \"kage17\"]:\n    time_diff = ds_kages[kage].coords[\"seconds_elapsed\"].diff(dim=\"time\")\n    ds_kages[kage][\"distance\"] = ds_kages[kage].body_centroid_speed * time_diff\n\n```\n\nThen we can sum the distance traversed in each time bin to get the activity level.\n\n```{python}\n#| label: fig-activity-levels\n#| fig-cap: \"Activity over time\"\n\ntime_bin_minutes = 10\ntime_bin_duration = pd.Timedelta(f\"{time_bin_minutes}min\")\n\nfig, ax = plt.subplots(\n    nrows=2, ncols=1, figsize=(8, 5), sharex=True, sharey=True\n)\n\nactivity_dict = {}  # Dictionary to store the activity levels for each kage\n\nfor i, kage in enumerate([\"kage14\", \"kage17\"]):\n    activity = ds_kages[kage].distance.resample(time=time_bin_duration).sum()\n    activity.plot.line(ax=ax[i])\n    ax[i].set_title(f\"{kage} activity\")\n    ax[i].set_ylabel(\"distance (cm)\")\n    ax[i].set_xlabel(\"time\")\n\n    activity_dict[kage] = activity\n\nplt.tight_layout()\nplt.show()\n```\n\nTo make any circadian patterns more apparent, we will stack days vertically\nand indicate the light cycle with gray areas.\nFor this particular experiment, the light cycle is as follows:\n\n- lights off at 9:30\n- dawn at 20:30\n- lights on at 21:30\n\n```{python}\n# Define light cycle (in minutes since midnight)\nlights_off = 9 * 60 + 30  # 9:30 AM in minutes\ndawn = 20 * 60 + 30       # 8:30 PM in minutes  \nlights_on = 21 * 60 + 30  # 9:30 PM in minutes\n\nn_bins_in_day = int(24 * 60 / time_bin_minutes)\n\nactogram_dict = {}  # Dictionary to store the 2D actogram for each kage\n\nfor i, kage in enumerate([\"kage14\", \"kage17\"]):\n    activity = activity_dict[kage]\n    days = list(activity.groupby(\"time.date\").groups.keys())\n\n    # Create an empty 2D actogram with dims (date, time_bin)\n    actogram = xr.DataArray(\n        np.zeros((len(days), n_bins_in_day)),\n        dims=[\"date\", \"time_of_day\"],\n        coords={\n            \"date\": days,\n            \"time_of_day\": np.arange(\n                time_bin_minutes/2, 24 * 60, time_bin_minutes\n            )\n        },\n    )\n    # Populate 2D actogram per day\n    for date, day_activity in activity.groupby(\"time.date\"):\n        actogram.loc[dict(date=date)] = day_activity.values\n\n    # Store the actogram in the dictionary for later use\n    actogram_dict[kage] = actogram\n\nactogram_dict[\"kage14\"]  # Replace with kage17 to see the actogram for kage17\n```\n\nLet's now visualise the actograms, with the light cycle marked.\n\n```{python}\n#| code-fold: true\n#| label: fig-actograms\n#| fig-cap: \"Actograms\"\n\nmax_activity = max(\n    actogram_dict[kage].max().values for kage in [\"kage14\", \"kage17\"]\n)\n\nfig, axes = plt.subplots(\n    nrows=2, ncols=1, figsize=(8, 8), sharex=True, sharey=True,\n)\n\nfor i, kage in enumerate([\"kage14\", \"kage17\"]):\n    actogram = actogram_dict[kage]\n\n    # Create a colormap for the actogram\n    cmap = colormaps.get_cmap(\"Greys\")\n    cmap.set_bad(color=\"brown\", alpha=0.5)  # Set bad values to red\n\n    # Plot the actogram\n    ax = axes[i]\n    actogram.plot(\n        ax=ax, yincrease=False, vmin=0, vmax=max_activity, cmap=cmap\n    )\n\n    # Assign x-tick labels every 4 hours formatted as HH:MM\n    ax.set_xticks(np.arange(0, 24 * 60 + 1, 4 * 60))\n    ax.set_xticklabels([f\"{i:02d}:00\" for i in np.arange(0, 25, 4)])\n\n    # Mark light cycle\n    ax.axvline(lights_off, color=\"red\", alpha=0.7, lw=2, linestyle=\"-\", label=\"lights off\")\n    ax.axvline(dawn, color=\"blue\", alpha=0.7, lw=2, linestyle=\"--\", label=\"dawn\")\n    ax.axvline(lights_on, color=\"blue\", alpha=0.7, lw=2, linestyle=\"-\", label=\"lights on\")\n    ax.legend(loc=\"lower left\")\n\n    # Set title and axis labels\n    ax.set_title(f\"{kage} actogram\")\n    ax.set_xlabel(\"Time of day\")\n\nplt.tight_layout()\nplt.show()\n\n```\n\n::: {.callout-tip title=\"Exercise C\"}\n\n1. How would you describe the observed differences between the two mice?\n2. Compute the mean activity profile (across days) for each mouse.\n3. Plot the mean activity profile for each mouse, with the light cycle marked.\n\n**Hint**: Start with the `actogram_dict` dictionary and also make use of the `lights_off`  and `lights_on` variables defined above.\n\n:::\n\n::: {.content-visible when-profile=\"answers\"}\n\n::: {.callout-tip title=\"Click to reveal the answers\" collapse=\"true\"}\n\nThe mouse in kage17 shows higher overall activity levels, stays constantly\nactive during dark periods and also exhibits many activity bursts during the day\n(at least more than kage14 does).\n\n```{python}\n#| label: fig-mean-activity-profiles\n#| fig-cap: \"Mean daily activity profiles\"\n\nfig, ax = plt.subplots(figsize=(8, 4))\n\n# Mark light cycle\nax.axvspan(lights_off, lights_on, color=\"0.8\", label=\"Dark period\")\n\nfor kage, actogram in actogram_dict.items():    \n    actogram.mean(dim=\"date\").plot.line(ax=ax, label=kage, lw=3)\n\nax.set_title(f\"Mean daily activity profile\")\nax.set_ylabel(f\"Activity\")\nax.set_xlabel(\"Time of day\")\n\n# Assign x-tick labels every 4 hours formatted as HH:MM\nax.set_xticks(np.arange(0, 24 * 60 + 1, 4 * 60))\nax.set_xticklabels([f\"{i:02d}:00\" for i in np.arange(0, 25, 4)])\n\nax.legend()\nplt.show()\n```\n\n:::\n\n:::\n\n::: {.callout-note title=\"What underlies the differences in activity?\" collapse=\"true\"}\n\nThe mouse in `kage17` is a genetically modified model of Down Syndrome.\nThe syndrome, also known as trisomy 21, is caused in humans by the presence of all or part of a third copy of chromosome 21.\nThe mouse in `kage17` is genetically modified with a triplication of mouse chromosome 16,\nwhich carries about 150 genes homologues to the human chromosome 21.\n\nLoukia is currently investigating why this particular mouse model\nexhibits a \"restless\" behavioural phenotype.\n\n:::\n\n## Space occupancy\n\nApart from quantifying how active the mice were over time, we might also\nbe interested in which parts of the habitat they tend to frequent.\n\n`movement` provides a [`plot_occupancy()`](https://movement.neuroinformatics.dev/api/movement.plots.plot_occupancy.html) function to help us visualise the space occupancy.\n\n```{python}\n#| label: fig-occupancy1\n#| fig-cap: \"Occupancy heatmaps\"\n\nfig, axes = plt.subplots(\n    nrows=1, ncols=2, figsize=(8, 4), sharex=True, sharey=True\n)\n\nplt.suptitle(\"Body centroid occupancy (log scale)\")\n\nfor i, kage in enumerate([\"kage14\", \"kage17\"]):\n    img = images[i]\n    height, width = img.shape[:2]\n\n    axes[i].imshow(img)\n    plot_occupancy(\n        # Setting the time coordinates to \"elapsed time\" is necessary\n        # for the log scale to work properly.\n        ds_kages[kage].body_centroid.set_index(time=\"seconds_elapsed\"),\n        ax=axes[i],\n        cmap=\"turbo\",\n        norm=\"log\",  # log scale the colormap\n        vmax=10**6,\n        alpha=0.6,   # some transparency\n    )\n    # Make axes match the image dimensions\n    axes[i].set_ylim([height - 1, 0])\n    axes[i].set_xlim([0, width])\n    axes[i].set_title(kage)\n\nplt.tight_layout()\nplt.show()\n```\n\nWe see some clear hotspots, such as the nest and climbing platform.\nBut not all hotspots are created equal.\nFor example, the nest should be occupied when the mouse is stationary but\nnot when it is moving.\n\nTo investigate this, let's choose an arbitrary speed limit of 4 cm/sec, below which we consider the mouse to be stationary.\n\n```{python}\n#| code-fold: true\n#| label: fig-speed-histogram\n#| fig-cap: \"Body centroid speed histogram (log scale)\"\n\nfig, ax = plt.subplots(figsize=(8, 4))\n\nspeed_threshold = 4\n\nfor kage in [\"kage14\", \"kage17\"]:\n    ds_kages[kage].body_centroid_speed.plot.hist(\n        ax=ax, bins=50, alpha=0.5, label=kage, histtype=\"step\",\n    )\n\nax.axvline(speed_threshold, linestyle=\"--\", color=\"red\", label=\"Speed threshold\")\n\nax.set_title(\"Body centroid speed\")\nax.set_xlabel(\"Speed (cm/sec)\")\nax.set_ylabel(\"Count\")\nax.set_yscale(\"log\")\nax.legend()\nplt.show()\n```\n\n::: {.callout-note}\nWe use the log scale because speeds tend to be exponentially distributed,\ni.e. the mouse spends far more time at low speeds than at high speeds.\nTry commenting out the `ax.set_yscale(\"log\")` line and see what happens.\n:::\n\nWe will generate separate occupancy heatmaps for when the mouse is stationary vs moving.\nWe can do this by [masking with `where()`](https://docs.xarray.dev/en/latest/user-guide/indexing.html#masking-with-where).\n\n```{python}\nstationary_mask = ds_kages[\"kage14\"].body_centroid_speed < 4\nstationary_mask\n```\n\nA \"mask\" is just a boolean array of the same shape as the original data.\nIt's `True` where the condition is met, and `False` otherwise.\n\n```{python}\nstationary_position = ds_kages[\"kage14\"].body_centroid.where(stationary_mask)\nstationary_position\n```\n\nWe see that the `where()` method returns a new `xarray.DataArray` with the same dimensions as the original, but with the data values replaced by `NaN` where the condition (mask) is `False`.\n\nUsing this approach we can generate separate the body centroid position arrays\ninto states where the mouse is stationary vs moving, and then plot the occupancy\nheatmaps for each state.\n\n```{python}\n#| code-fold: true\n#| label: fig-occupancy2\n#| fig-cap: \"Occupancy heatmaps (stationary vs active)\"\n\nfig, axes = plt.subplots(\n    nrows=2, ncols=2, figsize=(8, 8), sharex=True, sharey=True\n)\n\nplt.suptitle(\"Body centroid occupancy (log scale)\")\n\nfor i, kage in enumerate([\"kage14\", \"kage17\"]):\n    img = images[i]\n    height, width = img.shape[:2]\n\n    masks = {\n        \"stationary\": ds_kages[kage].body_centroid_speed < 4,\n        \"moving\": ds_kages[kage].body_centroid_speed >= 4,\n    }\n\n    for j, (mask_name, mask) in enumerate(masks.items()):\n        ax = axes[i, j]\n        ax.imshow(img)\n        plot_occupancy(\n            ds_kages[kage].body_centroid.where(mask).set_index(time=\"seconds_elapsed\"),\n            ax=ax,\n            cmap=\"turbo\",\n            norm=\"log\",\n            vmax=10**6,\n            alpha=0.6,\n        )\n        ax.set_title(f\"{kage} {mask_name}\")\n        ax.set_ylim([height - 1, 0])\n        ax.set_xlim([0, width])\n\nplt.tight_layout()\nplt.show()\n```\n\nWe see some expected patterns like the nest being a \"hotspot\" during stationary periods,\nand going \"dark\" during active periods. But we also see some puzzling patterns:\nfor example, the running wheel is occupied during both periods, including when the mouse is \"stationary\".\n\nIs that because the mouse appears to be stationary to the camera as it's\nrunning \"in-place\" on the wheel (like on a treadmill)? Or maybe the mouse\nspends some of its downtime resting on the wheel?\n\n## Region of interest occupancy\n\nLet us precisely measure running wheel occupancy, i.e. the proportion of time\nthe mouse spends on the running wheel.\nHere we'll use `movement`'s [functionality for defining regions of interest (ROIs)](https://movement.neuroinformatics.dev/api/movement.roi.html).\n\nLet us create a circular \"running wheel\" ROI for each of the two kages.\n\n```{python}\ncentres = {\n    \"kage14\": np.array([145, 260]),  # (x, y)\n    \"kage17\": np.array([385, 210]),\n}\nradius = 70\n\n# Create a unit circle\nn_points = 32\nangles = np.linspace(0, 2 * np.pi, n_points)\nunit_circle = np.column_stack([np.cos(angles), np.sin(angles)])\n\n# Create ROIs by scaling and shifting the unit circle\nrois = {}\nfor kage in [\"kage14\", \"kage17\"]:\n    points = centres[kage] + radius * unit_circle\n    roi = PolygonOfInterest(points, name=f\"{kage} running wheel\")\n    rois[kage] = roi\n```\n\n::: {.callout-note}\n\nAdmittedly, this is not the most precise or convenient way to define the running wheel ROI.\nIt would be better to directly draw shapes on the video frames.\n\nWe are actively working on a widget in `napari` that will enable this.\nStay tuned for updates in `movement` by joining the\n[\"movement\" channel on Zulip](https://neuroinformatics.zulipchat.com/#narrow/stream/406001-Movement).\n\n:::\n\nNow that we have the ROIs defined as `PolygonOfInterest` objects, we can use some of their\n[built-in methods](https://movement.neuroinformatics.dev/api/movement.roi.PolygonOfInterest.html).\n\nFor example, we can use `.plot()` to visualise the ROIs and verify that they are roughly in the right place.\n\n```{python}\n#| code-fold: true\n#| label: fig-rois\n#| fig-cap: \"Running wheel ROIs\"\n\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(8, 8))\n\nfor i, kage in enumerate([\"kage14\", \"kage17\"]):\n    img = images[i]\n    ax[i].imshow(img)\n    rois[kage].plot(ax=ax[i], alpha=0.25, facecolor=\"red\")\n    ax[i].legend()\n\nplt.tight_layout()\nplt.show()\n```\n\nWe can also use `.contains_point()` to check if a point is inside the ROI.\nIf we pass it a whole `xarray.DataArray` of points positions, e.g.\nthe positions of the mouse's body centroid over time, the check is performed\nfor each point in the array.\n\n::: {.callout-warning}\n\nThe following code cell will take a while to run, probably a few minutes.\nThat's because  we have a large amount of data\nand the `.contains_point()` method is not fully optimised yet.\n\nIf you are an experienced Python programmer this could be a cool\nproject for the hackday.\n\n:::\n\n```{python}\nroi_occupancy = {\n    kage: rois[kage].contains_point(ds_kages[kage].body_centroid)\n    for kage in [\"kage14\", \"kage17\"]\n}\n\nroi_occupancy[\"kage14\"]\n```\n\nThe ROI occupancy data is a boolean array which is `True` when a point\n(in this case the mouse's body centroid at each time point)\nis inside the ROI, and `False` otherwise.\n\n```{python}\n#| code-fold: true\n#| label: fig-roi-occupancy-line\n#| fig-cap: \"Running wheel occupancy during April 9, 2024\"\n\nfig, axes = plt.subplots(\n    nrows=2, ncols=1, figsize=(7.5, 5), sharex=True, sharey=True\n)\n\nfor i, kage in enumerate([\"kage14\", \"kage17\"]):\n    ax = axes[i]\n    roi_occupancy[kage].sel(time=slice(None, \"2024-04-09 23:59:59\")).plot.line(\n        \"-\", ax=ax, lw=0.1\n    )\n    ax.set_title(kage)\n    ax.set_ylabel(\"Occupancy\")\n    ax.set_yticks([0, 1])\n    ax.set_yticklabels([\"False\", \"True\"])\n\nplt.tight_layout()\nplt.show()\n```\n\nWe can also compute the % of time the mouse spends on the running wheel.\n\n```{python}\nfor kage in [\"kage14\", \"kage17\"]:\n    # Count the ratio of True values in the array\n    on_wheel_ratio = roi_occupancy[kage].mean(dim=\"time\").values\n    # Convert to percentage\n    pct_on_wheel = float(100 * on_wheel_ratio)\n    print(f\"{kage} spends {pct_on_wheel:.1f}% of its time on the running wheel\")\n```\n\nBut how does the running wheel occupancy relate to the light cycle?\nWe can segment the ROI occupancy data into dark and light periods for each day\nand compute the % of time spent on the running wheel during each period.\n\n```{python}\n#| code-fold: true\n\ndays = list(\n    roi_occupancy[\"kage14\"].dropna(dim=\"time\").groupby(\"time.date\").groups.keys()\n)\nn_days = len(days)\n\n# Create a new DataArray of NaNs with shape (kage, date, period)\ndaily_occupancy = xr.DataArray(\n    np.full((2, n_days, 2), np.nan),\n    dims=[\"kage\", \"date\", \"period\"],\n    coords={\n        \"kage\": [\"kage14\", \"kage17\"],\n        \"date\": days,\n        \"period\": [\"dark\", \"light\"]\n    },\n)\n\nfor kage in [\"kage14\", \"kage17\"]:\n    for date, day_ds in roi_occupancy[kage].dropna(dim=\"time\").groupby(\"time.date\"):\n        dark_period = slice(f\"{date} 09:30:00\", f\"{date} 21:30:00\")\n        light_period1 = slice(f\"{date} 00:00:00\", f\"{date} 09:30:00\")\n        light_period2 = slice(f\"{date} 21:30:00\", f\"{date} 23:59:59\")\n\n        dark_occupancy = 100 * day_ds.sel(time=dark_period).mean()\n        light_occupancy = 100 * (\n            day_ds.sel(time=light_period1).mean() +\n            day_ds.sel(time=light_period2).mean()\n        )\n\n        daily_occupancy.loc[dict(kage=kage, date=date, period=\"dark\")] = dark_occupancy\n        daily_occupancy.loc[dict(kage=kage, date=date, period=\"light\")] = light_occupancy\n```\n\n```{python}\ndaily_occupancy\n```\n\nLet's visualise the % of time spent on the running wheel during the light and dark periods of each day.\n\n```{python}\n#| code-fold: true\n#| label: fig-roi-occupancy\n#| fig-cap: \"Running wheel occupancy during light and dark periods\"\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(8, 4), sharex=True, sharey=True)\n\nmax_occupancy = daily_occupancy.max()\n\nfor i, kage in enumerate([\"kage14\", \"kage17\"]):\n    daily_occupancy.sel(kage=kage).plot(\n        ax=axes[i], lw=3, vmax=daily_occupancy.max(), yincrease=False\n    )\n\n    axes[i].set_title(f\"{kage} running wheel occupancy\")\n    axes[i].set_xticks([0.25, 0.75])\n    axes[i].set_ylabel(\"Occupancy (%)\")\n    axes[i].set_xlabel(\"Period\")\n\nplt.tight_layout()\nplt.show()\n```\n\nAs we would expect, both mice tend to spend more time on the running wheel\nduring the dark (active) periods.\n\nEven though the mouse in `kage17` is more active overall, as we saw\nin @sec-actograms, it spends less time on the running wheel\ncompared to the mouse in `kage14`.\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":true,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"brand":{"brand":{"meta":{"name":{"short":"AiM","full":"Animals in Motion"},"link":{"home":"https://neuroinformatics.dev/course-animals-in-motion","github":"https://github.com/neuroinformatics-unit/course-animals-in-motion","zulip":"https://neuroinformatics.zulipchat.com/#narrow/channel/497885-Open-Software-Week"}},"logo":{"medium":"img/animals-in-motion-logo.png"},"color":{"palette":{"white":"#FFFFFF","dark-grey":"#1E1E1E","niu-green":"#03A062","niu-green-dark":"#04B46D","bg-orange":"#EE9040","blue":"#4178f3"},"background":"white","foreground":"dark-grey","primary":"niu-green","info":"blue","success":"niu-green","warning":"bg-orange"},"typography":{"fonts":[{"family":"Barlow","source":"google"},{"family":"JetBrains Mono","source":"google"}],"base":{"family":"Barlow"},"headings":{"family":"Barlow","weight":600},"link":{"color":"primary"},"monospace":{"family":"JetBrains Mono"}}},"data":{"meta":{"name":{"short":"AiM","full":"Animals in Motion"},"link":{"home":"https://neuroinformatics.dev/course-animals-in-motion","github":"https://github.com/neuroinformatics-unit/course-animals-in-motion","zulip":"https://neuroinformatics.zulipchat.com/#narrow/channel/497885-Open-Software-Week"}},"logo":{"medium":"img/animals-in-motion-logo.png"},"color":{"palette":{"white":"#FFFFFF","dark-grey":"#1E1E1E","niu-green":"#03A062","niu-green-dark":"#04B46D","bg-orange":"#EE9040","blue":"#4178f3"},"background":"white","foreground":"dark-grey","primary":"niu-green","info":"blue","success":"niu-green","warning":"bg-orange"},"typography":{"fonts":[{"family":"Barlow","source":"google"},{"family":"JetBrains Mono","source":"google"}],"base":{"family":"Barlow"},"headings":{"family":"Barlow","weight":600},"link":{"color":"primary"},"monospace":{"family":"JetBrains Mono"}}},"brandDir":"/Users/nsirmpilatze/Code/NIU/animals-in-motion/book","projectDir":"/Users/nsirmpilatze/Code/NIU/animals-in-motion/book","processedData":{"color":{"white":"#FFFFFF","dark-grey":"#1E1E1E","niu-green":"#03A062","niu-green-dark":"#04B46D","bg-orange":"#EE9040","blue":"#4178f3","background":"#FFFFFF","foreground":"#1E1E1E","primary":"#03A062","info":"#4178f3","success":"#03A062","warning":"#EE9040"},"typography":{"base":{"family":"Barlow"},"headings":{"family":"Barlow","weight":600},"link":{"color":"primary"},"monospace":{"family":"JetBrains Mono"},"monospace-inline":{"family":"JetBrains Mono"},"monospace-block":{"family":"JetBrains Mono"}},"logo":{"images":{},"medium":{"light":{"path":"img/animals-in-motion-logo.png"},"dark":{"path":"img/animals-in-motion-logo.png"}}}}}},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","highlight-style":"atom-one","output-file":"05-movement-mouse.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.43","bibliography":["references.bib"],"pdf":false,"jupyter":"python3","license":"CC BY","funding":{"statement":"The first edition of this workshop was made possible by a Software Sustainability Institute fellowship to Niko Sirmpilatze, as well as further funding support by the Sainsbury Wellcome Centre, the Society for Research Software Engineering and AIBIO-UK.\n"},"theme":["cosmo","brand"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}