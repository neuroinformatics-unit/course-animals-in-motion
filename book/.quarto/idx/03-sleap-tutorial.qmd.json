{"title":"Pose estimation with SLEAP","markdown":{"headingText":"Pose estimation with SLEAP","headingAttr":{"id":"sec-sleap","classes":[],"keyvalue":[]},"containsRefs":false,"markdown":"<!-- markdownlint-disable MD045 -->\n\n\nBefore we proceed, make sure you have installed [SLEAP](https://sleap.ai/) [@pereira_sleap_2022] and activated the corresponding `conda` environment (see [prerequisites @sec-install-sleap]).\nYou will also need to download the [CalMS21 dataset](https://sites.google.com/view/computational-behavior/our-datasets/calms21-dataset) [@sun_caltech_2021] video file `mouse044_task1_annotator1.mp4` from [Dropbox](https://www.dropbox.com/scl/fo/81ug5hoy9msc7v7bteqa0/AH32RLdbZqWZJstIeR4YHZY?rlkey=blgagtaizw8aac5areja6h7q1&st=w1zueyi9&dl=0) (see [prerequisites @sec-data] for details on the folder contents).\n\n## Single-animal vs multi-animal pose estimation\n\n![Single-animal pose estimation, source: @pereira_quantifying_2020.](img/pose_estimation_2D.png){width=100%}\n\n**Single-animal pose estimation** focuses on detecting keypoints for one animal per frame, which is considered a landmark-localisation task where each body part has a unique coordinate.\nThis approach is simpler and faster to train and run.\n\n![The part-grouping problem in multi-animal pose estimation.](img/SLEAP/part_grouping.png){fig-align=\"center\" width=\"80%\"}\n\n**Multi-animal pose estimation** aims to detect and track multiple animals simultaneously within the same frame.\nThis is essential for studying social behaviours, group dynamics, or any scenario where animals interact, as it addresses the unique challenges of assigning detections reliably to individuals both within an image (part-grouping problem) and across frames (identity-tracking problem).\n\n## Top-down vs bottom-up approaches\n\nFor multi-animal pose estimation, SLEAP offers both top-down and bottom-up approaches.\n\n![Source:@pereira_sleap_2022](img/SLEAP/pose_estimation_topdown.png){width=100%}\n\n**Top-down approaches** use **two models** in sequence.\nFirst, an anchor detection model (e.g. a centroid model) locates each animal in the frame.\nThen, for each detected animal, a pose estimation model processes an anchor-centred crop to predict confidence maps for the body parts of the centred animal.\n\nThis approach typically yields more accurate pose estimates and is well-suited to datasets with few animals.\nAs the second stage of the network runs once per animal, inference speed scales linearly with the number of animals.\n\n![Source: @pereira_sleap_2022](img/SLEAP/pose_estimation_bottomup.png){width=100%}\n\n**Bottom-up approaches** use a **single model** that processes the entire frame in a single pass.\nThis model outputs confidence maps for all body parts in the image, along with Part Affinity Fields (PAFs)&mdash;vector fields that represent spatial relationships between parts and are used to group them into individual animal instances.\n\nDue to their single-stage construction, bottom-up models scale efficiently with increasing numbers of animals and are particularly effective in crowded or high-occupancy scenes.\n\n## Identity tracking approaches\n\n![](img/SLEAP/id_tracking.png){fig-align=\"center\" width=\"80%\"}\n\nSLEAP addresses the challenge of maintaining consistent animal identities across frames using two primary strategies: **temporal-based** and **appearance-based** cues.\n\n**Temporal-based tracking** uses optical flow to estimate pose displacement across frames, associating past with current poses without requiring model training.\nThis makes it well-suited for animals that are visually similar, as it avoids the need to label consecutive frames.\nHowever, errors, such as identity switches, can accumulate and propagate, limiting its reliability in long videos or real-time settings where post-hoc correction is not feasible.\n\n**Appearance-based tracking (ID models)** assigns identities based on visual features while simultaneously detecting and grouping landmarks.\nThis approach mitigates error propagation but relies on animals having distinguishable visual traits that allow manual identification during labelling.\n\n- **Top-down ID models** extend the centered-instance network to predict class probabilities for each animal-centred crop.\n- **Bottom-up ID models** replace PAFs with multi-class segmentation maps, collapsing body part masks into separate channels for each unique class ID. Grouping is implicit in the ID assignment.\n\n## Resident&ndash;intruder assay\n\nThe resident&ndash;intruder assay [@jove4367] is a behavioural test used to study social interactions, especially aggression and territoriality, in rodents. A resident mouse, habituated to its home cage, is confronted with an unfamiliar intruder, and their interactions&mdash;such as chasing, attacking, or investigating&mdash;are observed and quantified. This assay is widely used in neuroscience to explore the neural and genetic basis of social behaviour.\n\n## Dataset\n\nIn this tutorial, we will use [SLEAP](https://sleap.ai/) to train a **multi-animal top-down identity model** to simultaneously perform pose estimation and identity tracking of two mice in a short video (`mouse044_task1_annotator1.mp4`) from the [CalMS21 dataset](https://sites.google.com/view/computational-behavior/our-datasets/calms21-dataset).\n\nThis video captures a brief interaction between two mice in a resident&ndash;intruder assay, where the black mouse (the resident), implanted with a\nhead-mounted microendoscope, has established territory and the white mouse (the intruder) is newly introduced into the resident's cage.\n\n## SLEAP workflow\n\n```{mermaid}\ngraph LR\n    videos(\"Videos<br>(1,2,...,n)\") --> |extract| frames[/Sampled<br>frames/]\n    frames --> |\"label<br>body parts<br>and ID\"| labels[/Training<br>dataset/]\n    labels --> |train| model[/Model/]\n\n    videos --> test[/Unseen<br>frames/]\n    test --> model\n    model --> |infer| predictions[/Predictions/]\n    \n    predictions --> |fix labels<br>and merge| labels\n```\n\nA typical SLEAP workflow for multi-animal pose estimation and identity tracking consists of the following key steps:\n\n1. **Create project**: Start a new SLEAP project and import your video(s).\n2. **Define skeleton**: Create a `Skeleton` that defines the `Nodes` (each representing a keypoint or body part of interest, e.g. nose, left ear, right ear) and `Edges` (each representing the connections between keypoints, e.g. nose&ndash;left ear, nose&ndash;right ear) for the animals to be tracked.\n3. **Sample frames**: Extract frames from your video(s) to create a set of frames for annotation.\n4. **Label frames**: Annotate the sampled frames by marking the body parts and assigning identities (`Tracks`) to each animal. These labelled frames together form the training dataset.\n5. **Train model**: Use the training dataset to train a pose estimation and identity tracking model.\n6. **Predict on new data**: Apply the trained model to new, unlabelled video frames to generate pose and identity predictions for each animal.\n7. **Proofread predictions**: Review and correct the predictions as needed.\n8. **Refine model**: Corrected predictions can be merged back into the training dataset to retrain the model as needed.\n\n### Create a new project\n\nActivate the `sleap` environment and launch the SLEAP GUI.\n\n```bash\nconda activate sleap\nsleap-label\n```\n\nAdd a video by navigating to the \"Videos\" panel and clicking on \"Add Videos\" (or go to \"File\" then \"Add Videos\").\nSince the video is in greyscale, enable the \"Grayscale\" option in the video import dialogue.\nThis ensures SLEAP processes the input as a single-channel image, which can improve performance and reduce memory usage for greyscale videos.\nFurther details can be found in [SLEAP's Creating a project guide](https://sleap.ai/tutorials/new-project.html).\n\n![](img/SLEAP/1_create_project.png){width=100%}\n\n### Define skeleton\n\n![](img/SLEAP/mouse_annotated.png){fig-align=\"center\" width=\"40%\"}\n\nIn SLEAP, `Skeletons` are defined as a set of `Nodes` (body parts or keypoints of interest) and `Edges` (connections between body parts or keypoints).\nWith the exception of bottom-up models, `Edges` serve primarily for visualisation.\n\nSwitch to the `Skeleton` panel and add `Nodes` for each body part of interest, e.g.:\n\n- `nose`\n- `right_ear`\n- `left_ear`\n- `neck`\n- `right_hip`\n- `left_hip`\n- `tail_base`\n\nThen, define the `Edges` to connect the `Nodes` using the drop-down menus, e.g.:\n\n- `nose`&ndash;`left_ear`\n- `nose`&ndash;`right_ear`\n- `left_ear`&ndash;`neck`\n- `right_ear`&ndash;`neck`\n- `neck`&ndash;`left_hip`\n- `neck`&ndash;`right_hip`\n- `left_hip`&ndash;`tail_base`\n- `right_hip`&ndash;`tail_base`\n\n![](img/SLEAP/2_define_skeleton.png){width=100%}\n\nOnce you have defined the skeleton, save the project by clicking on \"File\" then \"Save\" (or with <kbd>Ctrl</kbd>/<kbd>Cmd</kbd>+<kbd>S</kbd>).\n\n### Sample frames\n\nFor assembling a set of frames for annotation, you can either pick your own frames, or let SLEAP suggest a set of frames using the \"Labeling Suggestions\" panel, which offers several [automated sampling strategies](https://sleap.ai/guides/gui.html#suggestion-methods) to help select informative and diverse frames.\n\nIn this example, we will use the \"Labeling Suggestions\" panel to randomly sample 20 frames from the video.\n\n![](img/SLEAP/3_sample_frames.png){width=100%}\n\n### Label frames\n\nTo begin labelling, click on \"Labels\" then \"Add Instance\" in the top menu bar.\nThe initial instance will have its nodes placed randomly.\nAdjust each point to its correct location by dragging it with the mouse.\n\nIn this example, the black mouse's nose is occluded by an implant.\nTo mark the node as hidden, right-click the node or its name to toggle visibility.\nIf you can reasonably infer its location, you can also mark it as \"visible\" to help the model learn to predict occluded nodes.\n\nFor tracking the identities of the mice, we will also assign identities to each instance by adding a `Track`.\nThis step is optional if you are only tracking a single animal or if the animals are visually indistinguishable.\n\nTo assign a track, select an instance and click on \"Tracks\" then \"Set Instance Track\" in the top menu bar.\nA new `Track` (\"Track 1\") will be created.\n`Tracks` can be renamed in the \"Track\" column.\nIn this example, we will name the black mouse `resident_b` and the white mouse `intruder_w`.\n\n![](img/SLEAP/4_label_frames.png){width=100%}\n\nOnce you have labelled the initial frame, you can navigate to the previous or next suggested frame by clicking \"Previous\" or \"Next\" in the \"Labeling Suggestions\" panel.\n\n::: {.callout-important}\nRemember to save your progress frequently by clicking \"File\" then \"Save\" (or with <kbd>Ctrl</kbd>/<kbd>Cmd</kbd>+<kbd>S</kbd>).\n:::\n\nYou may also find the following keyboard shortcuts and mouse actions helpful:\n\n| Function | Keyboard shortcut/Mouse actions |\n|----------|---------------------------------|\n| Add instance | <kbd>Ctrl</kbd>/<kbd>Cmd</kbd>+<kbd>I</kbd> |\n| Add new track | <kbd>Ctrl</kbd>/<kbd>Cmd</kbd>+<kbd>0</kbd> |\n| Assign track | Select instance, then <kbd>Ctrl</kbd>/<kbd>Cmd</kbd>+<kbd>1-9</kbd> (number = track ID) |\n| Toggle node visibility (occlusion) | Right-click on node or its label |\n| Move entire instance | Hold <kbd>Alt</kbd> (Windows) / <kbd>Option</kbd> (Mac) and drag any node |\n| Rotate instance | Hold <kbd>Alt</kbd> (Windows) / <kbd>Option</kbd> (Mac), click on any node, and scroll mouse wheel |\n| Zoom in/out | Place cursor over area to zoom in or out, then scroll mouse wheel |\n| Delete instance | Select instance, then <kbd>Ctrl</kbd>/<kbd>Cmd</kbd>+<kbd>Backspace</kbd> |\n| Navigate between frames | <kbd>Left</kbd>/<kbd>Right</kbd> |\n| Go to the next suggested frame | <kbd>Space</kbd> |\n| Go to the previous suggested frame | <kbd>Shift</kbd> + <kbd>Space</kbd> |\n| Go to the next labelled frame | <kbd>Alt</kbd> (Windows) / <kbd>Option</kbd> (Mac) + <kbd>right</kbd> |\n| Go to the previous labelled frame | <kbd>Alt</kbd> (Windows) / <kbd>Option</kbd> (Mac) + <kbd>left</kbd> |\n\nTo speed up labelling in subsequent frames, right-click on the frame and choose from several options for adding a new instance. For example, selecting \"Copy prior frame\" will duplicate the instance(s) from the previous labelled frame, allowing you to quickly adjust only the necessary points. This is especially useful when animal poses change gradually between frames.\n\nSee also [SLEAP's GUI guide on \"Labels\"](https://sleap.ai/guides/gui.html#labels) for a complete reference of all labelling functions.\n\n::: {.callout-tip title=\"Discuss\"}\n\n- How did you find the labelling process? What challenges did you encounter?\n- What strategies could annotators use to ensure consistency throughout the labelling process?\n- How might the quality of annotations affect model performance?\n:::\n\n### Configure and train models\n\nOnce you have labelled a sufficient number of frames, you can configure and train your first model.\nTo do this, go to \"Predict\" then \"Run training\" in the top menu bar.\n\nHere, we will employ the [appearance-based tracking approach](#identity-tracking-approaches) for maintaining consistent identities of individual mice across frames.\nTo do so, select **\"multi-animal top-down-id\"** as the \"Training Pipeline Type\".\nWe will also configure the training pipeline to predict on 20 more frames randomly sampled from the video once training is complete.\n\n![](img/SLEAP/5_config_model.png){width=100%}\n\nSee also [SLEAP's Configuring models guide](https://sleap.ai/guides/choosing-models.html) for further details on model types and training options.\n\n#### Centroid model\n\nWe will now configure the **Centroid model**, which predicts the location of each animal in each frame.\n\nSince we have labelled only a small number of frames, increase the validation split (e.g. 0.2) to obtain more stable and representative validation metrics.\nTo reduce the risk of overfitting, set the number of training epochs to a low value (e.g. 10&ndash;20 epochs).\nIn the \"Data\" and \"Optimization\" panels, adjust \"Validation\" and \"Epochs\" accordingly.\n\nNext, adjust the receptive field size to match the scale of the features you want the model to detect.\nThis can be achieved by adjusting \"Input Scaling\" and \"Max Stride\".\nAs a rule of thumb, the receptive field (blue box in the preview) should be large enough to cover the whole animal.\n\nTo help the model generalise to animals in different orientations, enable random rotation augmentation.\nIn the \"Augmentation\" panel, enable \"Rotation\" and set the rotation range (e.g. -180&deg; to 180&deg;).\n\n![](img/SLEAP/5_config_model_centroid.png){width=100%}\n\n#### Top-down ID model\n\nThe **Top-down ID model** predicts the full pose (locations of all defined nodes) and assigns identities to each animal in each frame.\n\nAs with the Centroid model, configure the validation split, number of training epochs, random rotation augmentation, and receptive field size accordingly.\n\nYou may also want to reduce the batch size (e.g. to 4) to ensure the model trains reliably and fits within memory constraints, especially when using limited hardware resources (e.g. CPU or lower-end GPUs).\n\n![](img/SLEAP/5_config_model_id.png){width=100%}\n\nOnce you have configured the models, click \"Run\" to begin training.\n\n::: {.callout-tip}\nFor top-down models, set a larger receptive field for the centroid model (by decreasing the \"Input Scaling\" value) than for the top-down ID model, since you only need to locate the animal, not fine details.\n:::\n\n### Monitor training progress\n\nYou should now see a training progress window showing the loss curves and metrics for each model as it trains.\n\n![Centroid model training progress](img/SLEAP/5_train_model_centroid.png){width=100%}\n\n![Top-down ID model training progress](img/SLEAP/5_train_model_id.png){width=100%}\n\n::: {.callout-tip}\nSLEAP supports remote training and inference workflows, allowing users to leverage external computational resources, particularly those with GPU support, for batch training or inference.\nFor detailed instructions and guidance, see the [Running SLEAP remotely guide](https://sleap.ai/guides/remote.html).\n:::\n\n### Run inference and proofread predictions\n\nAfter the models are trained, if you had configured the training pipeline to run inference upon completion, SLEAP will automatically apply the trained models to the selected frames.\n\nAlternatively, to manually run inference on random frames with your trained models, go to the \"Predict\" and select \"Run Inference\" in the top menu bar.\n\n![](img/SLEAP/6_inference.png){width=100%}\n\n::: {.callout-note}\nSince we have trained an [appearance-based ID model](#identity-tracking-approaches), which outputs identity assignments directly for each detected animal in each frame, there is no need to configure a \"Tracker\" (typically used for temporal-based tracking).\nFurther details on the Tracker module is available in [SLEAP's Tracking and proofreading guide](https://sleap.ai/guides/proofreading.html).\n:::\n\nLabelled frames (frames with user or predicted labels) will be marked in the seekbar.\n\n![](img/SLEAP/6_predictions.png){width=100%}\n\nTo step through labelled frames, click on \"Go\" and \"Next Labeled Frame\" (or use <kbd>Alt</kbd> (Windows) / <kbd>Option</kbd> (Mac) + <kbd>left</kbd>/<kbd>right</kbd>).\n\nThe models are not expected to perform well at this stage, but you can review and correct these predictions, and merge them back into your training dataset.\nYou can also generate more labelling suggestions and label them to ensure your training dataset captures a wider range of poses and variations.\nOnce you have added and/or corrected more instances, you can repeat the process: train a new model, predict on more frames, correct those predictions, and so on until you are ready to apply the model on the entire video.\n\nSee [SLEAP's Prediction-assisted labeling guide](https://sleap.ai/tutorials/assisted-labeling.html) for more details on this iterative process.\n\n::: {.callout-note}\nPredicted instances will not be used for model training unless you correct the predictions in the GUI.\n:::\n\n### Evaluate trained models\n\nThe metrics of trained models can be accessed by clicking on \"Predict\" then \"Evaluation Metrics for Trained Models\" in the top menu bar.\n\n![](img/SLEAP/7_evaluate_model.png){width=100%}\n\nSee [SLEAP's Model evaluation guide](https://sleap.ai/notebooks/Model_evaluation.html) for examples on generating accuracy metrics for your trained model.\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":true,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"brand":{"brand":{"meta":{"name":{"short":"AiM","full":"Animals in Motion"},"link":{"home":"https://neuroinformatics.dev/course-animals-in-motion","github":"https://github.com/neuroinformatics-unit/course-animals-in-motion","zulip":"https://neuroinformatics.zulipchat.com/#narrow/channel/497885-Open-Software-Week"}},"logo":{"medium":"img/animals-in-motion-logo.png"},"color":{"palette":{"white":"#FFFFFF","dark-grey":"#1E1E1E","niu-green":"#03A062","niu-green-dark":"#04B46D","bg-orange":"#EE9040","blue":"#4178f3"},"background":"white","foreground":"dark-grey","primary":"niu-green","info":"blue","success":"niu-green","warning":"bg-orange"},"typography":{"fonts":[{"family":"Barlow","source":"google"},{"family":"JetBrains Mono","source":"google"}],"base":{"family":"Barlow"},"headings":{"family":"Barlow","weight":600},"link":{"color":"primary"},"monospace":{"family":"JetBrains Mono"}}},"data":{"meta":{"name":{"short":"AiM","full":"Animals in Motion"},"link":{"home":"https://neuroinformatics.dev/course-animals-in-motion","github":"https://github.com/neuroinformatics-unit/course-animals-in-motion","zulip":"https://neuroinformatics.zulipchat.com/#narrow/channel/497885-Open-Software-Week"}},"logo":{"medium":"img/animals-in-motion-logo.png"},"color":{"palette":{"white":"#FFFFFF","dark-grey":"#1E1E1E","niu-green":"#03A062","niu-green-dark":"#04B46D","bg-orange":"#EE9040","blue":"#4178f3"},"background":"white","foreground":"dark-grey","primary":"niu-green","info":"blue","success":"niu-green","warning":"bg-orange"},"typography":{"fonts":[{"family":"Barlow","source":"google"},{"family":"JetBrains Mono","source":"google"}],"base":{"family":"Barlow"},"headings":{"family":"Barlow","weight":600},"link":{"color":"primary"},"monospace":{"family":"JetBrains Mono"}}},"brandDir":"/Users/nsirmpilatze/Code/NIU/animals-in-motion/book","projectDir":"/Users/nsirmpilatze/Code/NIU/animals-in-motion/book","processedData":{"color":{"white":"#FFFFFF","dark-grey":"#1E1E1E","niu-green":"#03A062","niu-green-dark":"#04B46D","bg-orange":"#EE9040","blue":"#4178f3","background":"#FFFFFF","foreground":"#1E1E1E","primary":"#03A062","info":"#4178f3","success":"#03A062","warning":"#EE9040"},"typography":{"base":{"family":"Barlow"},"headings":{"family":"Barlow","weight":600},"link":{"color":"primary"},"monospace":{"family":"JetBrains Mono"},"monospace-inline":{"family":"JetBrains Mono"},"monospace-block":{"family":"JetBrains Mono"}},"logo":{"images":{},"medium":{"light":{"path":"img/animals-in-motion-logo.png"},"dark":{"path":"img/animals-in-motion-logo.png"}}}}}},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","highlight-style":"atom-one","output-file":"03-sleap-tutorial.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.43","bibliography":["references.bib"],"pdf":false,"jupyter":"python3","license":"CC BY","funding":{"statement":"The first edition of this workshop was made possible by a Software Sustainability Institute fellowship to Niko Sirmpilatze, as well as further funding support by the Sainsbury Wellcome Centre, the Society for Research Software Engineering and AIBIO-UK.\n"},"theme":["cosmo","brand"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}