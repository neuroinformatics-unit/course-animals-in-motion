{"title":"Analysing tracks with movement","markdown":{"headingText":"Analysing tracks with movement","headingAttr":{"id":"sec-movement-intro","classes":[],"keyvalue":[]},"containsRefs":false,"markdown":"\nIn this tutorial, we will introduce the `movement` package\nand walk through parts of its [documentation](https://movement.neuroinformatics.dev/).\n\nYou will be given a set of exercises to complete—using `movement` to analyse the pose tracks you've generated in @sec-sleap or any other tracking data you may have access to.\n\n::: {.callout-note title=\"Using the right environment\"}\n\nIf you are following along this chapter on your own computer,\nmake sure to run all code snippets with the `animals-in-motion-env` environment activated\n(see [prerequisites @sec-install-movement]). This also applies to launching the\n`movement` graphical user interface (@sec-movement-gui) and running\n`movement` examples as Jupyter notebooks (@sec-movement-filtering and @sec-movement-kinematics).\n\n:::\n\n```{python}\n#| include: false\nimport xarray as xr\n\nxr.set_options(\n    display_expand_attrs=False,\n    display_expand_coords=False,\n    keep_attrs=True,\n)\n```\n\n## What is movement?\n\nIn [chapters @sec-intro] and [-@sec-dl-cv] we saw how the rise of deep learning-based markerless motion tracking tools is transforming the study of animal behaviour.\n\nIn [@sec-sleap] we dove deeper into [SLEAP](https://sleap.ai/), a popular package for pose estimation and tracking.\nWe saw how SLEAP and similar tools—like\n[DeepLabCut](https://www.mackenziemathislab.org/deeplabcut) and [LightningPose](https://lightning-pose.readthedocs.io/en/latest/)—detect the positions of user-defined keypoints in video frames,\ngroup the keypoints into poses, and connect their identities across time into sequential collections we call pose tracks.\n\nThe extraction of pose tracks is often just the beginning of the analysis.\nResearchers use these tracks to investigate various aspects of animal behaviour, such as kinematics, spatial navigation, social interactions, etc.\nTypically, these analyses involve custom, project-specific scripts that are hard to reuse across different projects and are rarely maintained after the project's conclusion.\n\nIn response to these challenges, we saw the need for a versatile and easy-to-use toolbox that is compatible with a range of ML-based motion tracking frameworks and supports interactive data exploration and analysis.\nThat's where `movement` comes in.\nWe started building in early 2023 to answer the question: __what can I do now with these tracks?__\n\n::: {.callout-note title=\"movement's mission\"}\n\n`movement`aims to facilitate the study of animal behaviour\nby providing a consistent, modular interface for analysing motion tracks,\nenabling steps such as data cleaning, visualisation, and motion quantification.\n\nSee [movement's mission and scope statement](https://movement.neuroinformatics.dev/community/mission-scope.html) for more details.\n\n:::\n\n![Overview of the `movement` package](img/movement_overview.png){#fig-movement-overview}\n\n## A unified interface for motion tracks\n\n`movement` aims to support all popular animal tracking frameworks and file formats,\nin 2D and 3D, tracking single or multiple animals of any species.\n\nTo achieve this level of versatility, we had to identify what's common across the outputs of motion tracking tools\nand how we can represent them in a standardised way.\n\nWhat we came up with takes the form of collections of multi-dimensional arrays—an `xarray.Dataset` object.\nEach array within a dataset is an `xarray.DataArray` object holding different aspects of the collected data (position, time, confidence scores…).\nYou can think of an `xarray.DataArray` as a multi-dimensional `numpy.ndarray` with pandas-style indexing and labelling.\n\nThis may sound complicated but fear not, we'll build some understanding by exploring some example datasets\nthat are included in the `movement` package.\n\n```{python}\nfrom movement import sample_data\n```\n\n### Poses datasets {#sec-movement-ds-poses}\n\nFirst, let's see how `movement` represents pose tracks, like the ones we get with SLEAP.\n\n![The structure of a `movement` poses dataset](img/movement_poses_dataset_card.png){#fig-movement-ds-poses}\n\nLet's load an example dataset and explore its contents.\n\n```{python}\nposes_ds = sample_data.fetch_dataset(\"SLEAP_two-mice_octagon.analysis.h5\")\nposes_ds\n```\n\n### Bounding boxes datasets\n\n`movement` also supports datasets that consist of bounding boxes, like the ones\nwe would get if we performed object detection on a video, followed by\ntracking identities across time.\n\n![The structure of a `movement` bounding boxes dataset](img/movement_bboxes_dataset_card.png){#fig-movement-ds-bboxes}\n\n```{python}\n#| output: false\nbboxes_ds = sample_data.fetch_dataset(\n    \"VIA_single-crab_MOCA-crab-1_linear-interp.csv\"\n)\n```\n\n```{python}\nbboxes_ds\n```\n\n::: {.callout-tip title=\"Discuss\"}\n\n- What are some limitations of `movement`'s approach? What kinds of data cannot be accommodated?\n- Can you think of alternative ways of representing these data?\n\nSee the [documentation on movement datasets](https://movement.neuroinformatics.dev/user_guide/movement_dataset.html) for more details\non `movement` data structures.\n:::\n\n### Working with xarray objects\n\nSince `movement` represents motion tracking data as `xarray` objects, we can use all of `xarray`'s intuitive interface and rich\n[built-in functionalities](https://movement.neuroinformatics.dev/user_guide/movement_dataset.html#using-xarrays-built-in-functionality)\nfor data manipulation and analysis.\n\nAccessing data variables and attributes (metadata) is straightforward:\n\n```{python}\nprint(f\"Source software: {poses_ds.source_software}\")\nprint(f\"Frames per second: {poses_ds.fps}\")\n\nposes_ds.position\n```\n\nWe can select a subset of data along any dimension in a variety of ways:\nby integer index (order) or coordinate label.\n\n```{python}\n# First individual, first time point\nposes_ds.position.isel(individuals=0, time=0)\n\n# 0-10 seconds, two specific keypoints\nposes_ds.position.sel(time=slice(0, 10), keypoints=[\"EarLeft\", \"EarRight\"])\n```\n\nWe can also do all sorts of [computations](https://docs.xarray.dev/en/stable/user-guide/computation.html)\non the data, along any dimension.\n\n```{python}\n# Each point's median confidence score across time\nposes_ds.confidence.median(dim=\"time\")\n\n# Take the block mean for every 10 frames.\nposes_ds.position.coarsen(time=10, boundary=\"trim\").mean()\n```\n\n`xarray` also provides a rich set of built-in [plotting methods](https://docs.xarray.dev/en/stable/user-guide/plotting.html)\nfor visualising the data.\n\n```{python}\n#| label: fig-plot-tail-base-pos\n#| fig-cap: \"The x,y spatial coordinates of the TailBase keypoint across time\"\n\nfrom matplotlib import pyplot as plt\n\ntail_base_pos = poses_ds.sel(keypoints=\"TailBase\").position\ntail_base_pos.plot.line(\n    x=\"time\", row=\"individuals\", hue=\"space\", aspect=2, size=2.5\n)\nplt.show()\n```\n\nYou can also combine those with `matplotlib` figures.\n\n```{python}\n#| label: fig-plot-confidence-histograms\n#| fig-cap: \"Confidence histograms per keypoint\"\n\ncolors = plt.cm.tab10.colors\n\nfig, ax = plt.subplots()\nfor kp, color in zip(poses_ds.keypoints, colors):\n    data = poses_ds.confidence.sel(keypoints=kp)\n    data.plot.hist(\n        bins=50, histtype=\"step\", density=True, ax=ax, color=color, label=kp\n    )\nax.set_ylabel(\"Density\")\nax.set_title(\"Confidence histograms per keypoint\")\nplt.legend()\nplt.show()\n```\n\nYou may also want to export date to structures you may be more familiar with,\nsuch as\n[Pandas DataFrames](https://docs.xarray.dev/en/stable/user-guide/pandas.html)\nor [NumPy arrays](https://docs.xarray.dev/en/stable/user-guide/duckarrays.html).\n\nExport the position data array as a pandas DataFrame:\n\n```{python}\nposition_df = poses_ds.position.to_dataframe(\n    dim_order=[\"time\", \"individuals\", \"keypoints\", \"space\"]\n)\nposition_df.head()\n```\n\nExport data variables or coordinates as numpy arrays:\n\n```{python}\nposition_array = poses_ds.position.values\nprint(f\"Position array shape: {position_array.shape}\")\n\ntime_array = poses_ds.time.values\nprint(f\"Time array shape: {time_array.shape}\")\n```\n\nFor saving datasets to disk, we recommend leveraging `xarray`'s built-in\n[support for the netCDF file format](https://docs.xarray.dev/en/stable/user-guide/io.html#netcdf).\n\n```{.python}\nimport xarray as xr\n\n# To save a dataset to disk\nposes_ds.to_netcdf(\"poses_ds.nc\")\n\n# To load the dataset back from memory\nposes_ds = xr.open_dataset(\"poses_ds.nc\")\n```\n\n## Load and explore data {#sec-movement-load}\n\nAs stated above, our goal with `movement` is to enable pipelines that are input-agnostic,\nmeaning they are not tied to a specific motion tracking tool or data format.\nTherefore, `movement` offers input/output functions that facilitate data flows\nbetween various motion tracking frameworks and `movement`'s own `xarray` data structure.\n\nPlease refer to the [Input/Output section](https://movement.neuroinformatics.dev/user_guide/input_output.html)\nof the `movement` documentation for more details, including a full list of\nsupported formats.\n\n::: {.callout-tip title=\"Exercise A\"}\n\n1. Load the predictions you generated in @sec-sleap into a `movement` dataset. Alternatively, feel free to work with the `CalMS21/mouse044_task1_annotator1.slp` file from Dropbox (refer to [prerequisites @sec-data]) or any of `movement`'s [sample datasets](https://movement.neuroinformatics.dev/user_guide/input_output.html#sample-data).\n2. Compute the overall minimum and maximum x,y positions.\n3. Select a narrow time window (e.g. 10 seconds) and plot the x, y positions of a certain keypoint across time.\n4. Plot the centroid trajectory of a given individual across time.\n\n__Bonus:__ Overlay the centroid trajectory (task 4) on top of a frame extracted from the video.\nYou may find inspiration in the [\"Pupil tracking\" example](https://movement.neuroinformatics.dev/examples/mouse_eye_movements.html).\n\nUseful resources:\n\n- [Input/Output guide](https://movement.neuroinformatics.dev/user_guide/input_output.html)\n- [The \"Load and explore pose tracks\" example](https://movement.neuroinformatics.dev/examples/load_and_explore_poses.html)\n- [`plot_centroid_trajectory()` function](https://movement.neuroinformatics.dev/api/movement.plots.plot_centroid_trajectory.html)\n\n:::\n\n::: {.content-visible when-profile=\"answers\"}\n\n::: {.callout-tip title=\"Click to reveal the answers\" collapse=\"true\"}\n\nLoading pose tracks from a file:\n\n```{python}\nfrom pathlib import Path\nfrom movement.io import load_poses\n\nfile_name = \"mouse044_task1_annotator1.slp\"\nfile_path = Path.home() / \".movement\" / \"CalMS21\" / file_name\n\nds = load_poses.from_file(file_path, source_software=\"SLEAP\", fps=30)\nds\n```\n\nComputing the minimum and maximum x,y positions:\n\n```{python}\nfor space_coord in [\"x\", \"y\"]:\n    min_pos = ds.position.sel(space=space_coord).min().values\n    max_pos = ds.position.sel(space=space_coord).max().values\n    print(f\"Min-Max {space_coord} positions: {min_pos:.2f}-{max_pos:.2f}\")\n```\n\nPlotting the x,y positions of a certain keypoint across time, within a narrow time window:\n\n```{python}\nds.position.sel(keypoints=\"tail_base\", time=slice(0, 10)).plot.line(\n    x=\"time\", row=\"individuals\", hue=\"space\", aspect=2, size=2.5\n)\n```\n\nPlotting the centroid trajectory:\n\n```{python}\nfrom movement.plots import plot_centroid_trajectory\n\nfig, ax = plt.subplots(figsize=(8, 4))\nplot_centroid_trajectory(ds.position, individual=\"resident_b\", ax=ax)\nplt.show()\n```\n\nAs a __bonus__, we can also overlay that trajectory on top of a video frame.\n\n```{python}\nimport sleap_io as sio\n\n\nvideo_path = Path.home() / \".movement\" / \"CalMS21\" / \"mouse044_task1_annotator1.mp4\"\nvideo = sio.load_video(video_path)\n\nn_frames, height, width, channels = video.shape\nprint(f\"Number of frames: {n_frames}\")\nprint(f\"Frame size: {width}x{height}\")\nprint(f\"Number of channels: {channels}\\n\")\n\n# Extract the first frame to use as background\nbackground = video[0]\n\nfig, ax = plt.subplots(figsize=(8, 4))\n\n# Plot the first video frame\nax.imshow(background, cmap=\"gray\")\n\n# Plot the centroid trajectory\nplot_centroid_trajectory(\n    ds.position, individual=\"resident_b\", ax=ax, alpha=0.75, s=5,\n)\n\nplt.show()\n```\n\n:::\n\n:::\n\n## Visualise motion tracks interactively {#sec-movement-gui}\n\nThe `movement` [graphical user interface (GUI)](https://movement.neuroinformatics.dev/user_guide/gui.html),\npowered by our custom plugin for [napari](https://napari.org/dev/),\nmakes it easy to view and explore motion tracks.\nCurrently, you can use it to visualise 2D `movement` datasets as points, tracks,\nand rectangular bounding boxes (if defined) overlaid on video frames.\n\nWe'll first demonstrate how the GUI works by using it to explore the\n`CalMS21` dataset and some of the\n[sample data](https://movement.neuroinformatics.dev/user_guide/input_output.html#sample-data)\nwe can fetch with `movement`.\n\n![Data from the CalMS21 dataset viewed via the `movement` plugin for `napari`](img/napari_calms21_screenshot.png){#fig-napari-calms21}\n\nAfter that you are free to play with the GUI on your own.\nYou may try to explore the predictions you generated in @sec-sleap,\nor any other tracking data you have at hand and is supported by `movement`.\n\nConsult the [GUI user guide](https://movement.neuroinformatics.dev/user_guide/gui.html)\nin the `movement` documentation for more details.\n\n::: {.callout-tip title=\"Discuss\"}\n\n- Did you spot any errors in the data you've explored?\n- What kinds of errors do you expect in pose tracks?\n- What are some common sources of errors?\n- How can such errors be avoided or corrected?\n\n:::\n\n## Clean motion tracks {#sec-movement-filtering}\n\nIn this section, we will walk through the tools `movement` offers for dealing with errors in motion tracking.\nThese tools are implemented in the\n[`movement.filtering`](https://movement.neuroinformatics.dev/api/movement.filtering.html)\nmodule and include functions for:\n\n- identifying and removing outliers\n- interpolating missing data\n- smoothing trajectories over time\n\nWe will go through two notebooks from `movement`'s\n[example gallery](https://movement.neuroinformatics.dev/examples/index.html):\n\n- [Drop outliers and interpolate](https://movement.neuroinformatics.dev/examples/filter_and_interpolate.html)\n- [Smooth pose tracks](https://movement.neuroinformatics.dev/examples/smooth.html)\n\nTo run these examples as interactive Jupyter notebooks, you can go to end\nof each example and either:\n\n- Click the __Download Jupyter Notebook__ button and open the `.ipynb` file in your code editor of choice (recommended), or\n- Click the __launch binder__ button and run the notebook in your browser.\n\n::: {.callout-tip title=\"Exercise B\"}\n\nFor this exercise you may continue working with the dataset you used\nto solve Exercise A in @sec-movement-load, or load another one.\nYou should store each intermediate output as a data variable\nwithin the same dataset.\n\n1. Drop position values that are below a certain confidence threshold.\n2. Smooth the data across time using rolling median filter.\n3. Interpolate missing values across time.\n4. Save the dataset, including the processed position data, to a netCDF file.\n\nRefer to the [filtering module documentation](https://movement.neuroinformatics.dev/api/movement.filtering.html)\nand to the examples we went through above.\n\nEach filtering step involves selecting one or more parameters.\nWe encourage you to experiment with different parameters values and\ninspect their effects by plotting the data.\n\n:::\n\n::: {.content-visible when-profile=\"answers\"}\n\n::: {.callout-tip title=\"Click to reveal the answers\" collapse=\"true\"}\n\nWe will work with the same \"SLEAP_two-mice_octagon.analysis.h5\"\ndataset we loaded in @sec-movement-ds-poses.\n\n```{python}\nds_oct = sample_data.fetch_dataset(\"SLEAP_two-mice_octagon.analysis.h5\")\nds_oct\n```\n\nConsulting @fig-plot-confidence-histograms, we decide on a confidence threshold of 0.8.\nThis decision is always somewhat arbitrary, and confidence scores are not usually\ncomparable across different tracking tools and datasets.\n\nWe can now drop the position values that are below the threshold:\n\n```{python}\nfrom movement.filtering import (\n    filter_by_confidence,\n    rolling_filter,\n    interpolate_over_time,\n)\n\nconfidence_threshold = 0.8\n\nds_oct[\"position_filtered\"] = filter_by_confidence(\n    ds_oct.position,\n    ds_oct.confidence,\n    threshold=confidence_threshold,\n    print_report=True\n)\n```\n\nSmoothing the data with a rolling median filter:\n\n```{python}\nds_oct[\"position_smoothed\"] = rolling_filter(\n    ds_oct.position_filtered,\n    window=5,\n    statistic=\"median\",\n    min_periods=2,\n    print_report=True\n)\n```\n\nInterpolating missing values across time:\n\n```{python}\nds_oct[\"position_interpolated\"] = interpolate_over_time(\n    ds_oct.position_smoothed,\n    method=\"linear\",\n    max_gap=10,\n    print_report=True\n)\n```\n\nThe dataset now contains all intermediate processing steps.\n\n```{python}\nds_oct\n```\n\nWe can even inspect the log of the final position data array:\n\n```{python}\nprint(ds_oct.position_interpolated.log)\n```\n\nLet's pick a keypoint and plot its position across time for every\nprocessing step. To make the plotting a bit easier, we'll\nstack the four position data arrays across a new dimension called `step`.\n\n```{python}\nposition_all_steps = xr.concat(\n    [\n        ds_oct.position,\n        ds_oct.position_filtered,\n        ds_oct.position_smoothed,\n        ds_oct.position_interpolated\n    ],\n    dim=\"step\"\n).assign_coords(step=[\"original\", \"filtered\", \"smoothed\", \"interpolated\"])\n\nposition_all_steps\n```\n\nLet's plot the position of the EarLeft keypoint across time for every step.\n\n```{python}\nposition_all_steps.sel(individuals=\"1\", keypoints=\"EarLeft\").plot.line(\n    x=\"time\", row=\"step\", hue=\"space\", aspect=2, size=2.5\n)\nplt.show()\n```\n\n:::\n\n:::\n\n## Quantify motion {#sec-movement-kinematics}\n\nIn this section, we will familiarise ourselves with the\n[`movement.kinematics`](https://movement.neuroinformatics.dev/api/movement.kinematics.html)\nmodule, which provides functions for deriving various useful quantities\nfrom motion tracks, such as velocity, acceleration, distances, orientations, and angles.\n\nAs in the previous section, we will go through a specific example from `movement`'s\n[example gallery](https://movement.neuroinformatics.dev/examples/index.html):\n\n- [Compute and visualise kinematics](https://movement.neuroinformatics.dev/examples/compute_kinematics.html)\n\nYou can run this notebook interactively in the same way as in @sec-movement-filtering.\n\n::: {.callout-tip title=\"Exercise C\"}\n\nFor this exercise you may continue working with the dataset you used\nto solve Exercise B in @sec-movement-filtering. In fact, you\ncan use the fully processed position data as your starting point.\n\n1. Take the mean position across keypoints (each individual's centroid).\n2. Compute and plot the centroid speed across time.\n3. Compute the distance travelled by each individual within a certain time window.\n4. Compute the distance between two individuals (or between two keypoints)\nand plot it across time.\n\nRefer to the [kinematics module documentation](https://movement.neuroinformatics.dev/api/movement.kinematics.html)\nand to the example we went through above.\n\n:::\n\n::: {.content-visible when-profile=\"answers\"}\n\n::: {.callout-tip title=\"Click to reveal the answers\" collapse=\"true\"}\n\nWe will continue working with the `ds_oct` from the previous exercise,\nusing the `position_interpolated` data variable as our starting point.\n\nLet's first import the functions we will use:\n\n```{python}\nfrom movement.kinematics import (\n    compute_speed,\n    compute_path_length,\n    compute_pairwise_distances,\n)\n```\n\nTo compute the centroid position and speed:\n\n```{python}\nds_oct[\"centroid_position\"] = ds_oct.position_interpolated.mean(dim=\"keypoints\")\nds_oct[\"centroid_speed\"] = compute_speed(ds_oct.centroid_position)\n\nds_oct.centroid_speed\n```\n\nTo plot it across time:\n\n```{python}\nds_oct.centroid_speed.plot.line(x=\"time\", row=\"individuals\", aspect=2, size=2.5)\nplt.show()\n```\n\nTo compute the distance travelled by each individual within a certain time window:\n\n```{python}\nds_oct[\"path_length\"] = compute_path_length(\n    ds_oct.centroid_position.sel(time=slice(50, 100))\n)\n\nds_oct.path_length\n```\n\nWe will compute the distance between the centroids of the two individuals and plot it across time:\n\n```{python}\nds_oct[\"inter_individual_distance\"] = compute_pairwise_distances(\n    ds_oct.centroid_position,\n    dim=\"individuals\",\n    pairs={\"1\": \"2\"}\n)\n\nds_oct.inter_individual_distance\n```\n\nTo plot the inter-individual distance across time:\n\n```{python}\nds_oct.inter_individual_distance.plot.line(x=\"time\", aspect=2, size=2.5)\nplt.show()\n```\n\n:::\n\n:::\n\nIf you wish to learn more about quantifying motion with `movement`,\nincluding things we didn't cover here, the following example notebooks\nare good follow-ups:\n\n- [Compute head direction](https://movement.neuroinformatics.dev/examples/compute_head_direction.html): a good place to learn about orientations and angles\n- [Pupil tracking](https://movement.neuroinformatics.dev/examples/mouse_eye_movements.html): an interesting application of kinematics to analyse mouse eye movements\n\nThe following two chapters—[@sec-movement-mouse] and [@sec-movement-zebras]—constitute\ncase studies in which we apply `movement` to some real-world datasets.\nThe two case studies represent quite different applications:\n\n- [@sec-movement-mouse]: continuous home cage monitoring data tracking mice for months\n- [@sec-movement-zebras]: collective escape events in a herd of zebras, recorded via a drone\n\n## Join the movement {#sec-movement-next}\n\nThis chapter does not represent a full list of `movement`'s current and future capabilities.\nThe package will continue to evolve as it's being actively developed by a core team of engineers\n(aka the authors of this book) supported by a growing, global\n[community of contributors](https://movement.neuroinformatics.dev/community/people.html).\n\nWe are committed to openness and transparency and always welcome feedback and contributions from the community,\nespecially from practicing animal behaviour researchers, to shape the project's direction.\n\nVisit the [movement community page](https://movement.neuroinformatics.dev/community)\nto find about ways to get help and get involved.\n\n::: {.callout-tip title=\"Discuss\"}\n\n- What do you think of the `movement` package? What aspects of it could be improved?\n- What is currently missing? What types of analyses would you like for `movement` to support?\n\nIf you have ideas, tell us about them on Zulip, open an issue on GitHub, or suggest a project for the hackday!\n\n:::\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":true,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"brand":{"brand":{"meta":{"name":{"short":"AiM","full":"Animals in Motion"},"link":{"home":"https://neuroinformatics.dev/course-animals-in-motion","github":"https://github.com/neuroinformatics-unit/course-animals-in-motion","zulip":"https://neuroinformatics.zulipchat.com/#narrow/channel/497885-Open-Software-Week"}},"logo":{"medium":"img/animals-in-motion-logo.png"},"color":{"palette":{"white":"#FFFFFF","dark-grey":"#1E1E1E","niu-green":"#03A062","niu-green-dark":"#04B46D","bg-orange":"#EE9040","blue":"#4178f3"},"background":"white","foreground":"dark-grey","primary":"niu-green","info":"blue","success":"niu-green","warning":"bg-orange"},"typography":{"fonts":[{"family":"Barlow","source":"google"},{"family":"JetBrains Mono","source":"google"}],"base":{"family":"Barlow"},"headings":{"family":"Barlow","weight":600},"link":{"color":"primary"},"monospace":{"family":"JetBrains Mono"}}},"data":{"meta":{"name":{"short":"AiM","full":"Animals in Motion"},"link":{"home":"https://neuroinformatics.dev/course-animals-in-motion","github":"https://github.com/neuroinformatics-unit/course-animals-in-motion","zulip":"https://neuroinformatics.zulipchat.com/#narrow/channel/497885-Open-Software-Week"}},"logo":{"medium":"img/animals-in-motion-logo.png"},"color":{"palette":{"white":"#FFFFFF","dark-grey":"#1E1E1E","niu-green":"#03A062","niu-green-dark":"#04B46D","bg-orange":"#EE9040","blue":"#4178f3"},"background":"white","foreground":"dark-grey","primary":"niu-green","info":"blue","success":"niu-green","warning":"bg-orange"},"typography":{"fonts":[{"family":"Barlow","source":"google"},{"family":"JetBrains Mono","source":"google"}],"base":{"family":"Barlow"},"headings":{"family":"Barlow","weight":600},"link":{"color":"primary"},"monospace":{"family":"JetBrains Mono"}}},"brandDir":"/Users/nsirmpilatze/Code/NIU/animals-in-motion/book","projectDir":"/Users/nsirmpilatze/Code/NIU/animals-in-motion/book","processedData":{"color":{"white":"#FFFFFF","dark-grey":"#1E1E1E","niu-green":"#03A062","niu-green-dark":"#04B46D","bg-orange":"#EE9040","blue":"#4178f3","background":"#FFFFFF","foreground":"#1E1E1E","primary":"#03A062","info":"#4178f3","success":"#03A062","warning":"#EE9040"},"typography":{"base":{"family":"Barlow"},"headings":{"family":"Barlow","weight":600},"link":{"color":"primary"},"monospace":{"family":"JetBrains Mono"},"monospace-inline":{"family":"JetBrains Mono"},"monospace-block":{"family":"JetBrains Mono"}},"logo":{"images":{},"medium":{"light":{"path":"img/animals-in-motion-logo.png"},"dark":{"path":"img/animals-in-motion-logo.png"}}}}}},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","highlight-style":"atom-one","output-file":"04-movement-intro.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.43","bibliography":["references.bib"],"pdf":false,"jupyter":"python3","license":"CC BY","funding":{"statement":"The first edition of this workshop was made possible by a Software Sustainability Institute fellowship to Niko Sirmpilatze, as well as further funding support by the Sainsbury Wellcome Centre, the Society for Research Software Engineering and AIBIO-UK.\n"},"theme":["cosmo","brand"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}