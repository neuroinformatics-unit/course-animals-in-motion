{"title":"Introduction","markdown":{"headingText":"Introduction","headingAttr":{"id":"sec-intro","classes":[],"keyvalue":[]},"containsRefs":false,"markdown":"\n## Summary\n\nMeasuring animal behaviour often comes down to measuring movement, whether of the whole body or specific parts, using tools such as video cameras and GPS loggers.\n\nAdvances in computer vision, particularly deep learning-based markerless motion tracking, have transformed how we study animal behaviour, bridging traditions from multiple scientific disciplines.\nThese methods now underpin the emerging field of computational (neuro)ethology, enabling researchers to extract precise motion data from videos, quantify behavioural features, and (in neuroscience) relate them directly to neural activity.\n\nIn this course, we focus on two key steps in this workflow: pose estimation and motion quantification, while recognising the broader ecosystem of open-source tools available.\n\n## Measuring behaviour as movement\n\nDefining behaviour is tricky, and behavioural biologists cannot agree on a single definition [@levitis_behavioural_2009].\nThe following one by Tinbergen has some historical sway:\n\n> The total movements made by the intact animal [@tinbergen_study_1951].\n\nFraming behaviour as \"movement\" is useful.\nIn most studies, what we actually measure is the motion of one or more animalsâ€”and/or their body partsâ€”over time.\nWe can see that reflected in the devices we most often use to record animal behaviour:\n\n- ðŸŽ¥ **Video cameras**\n- ðŸ“± Inertial measurement units (IMUs)\n- ðŸ›°ï¸ GPS-based biologgers\n- ðŸŽ¤ Microphones\n\nWith the exception of microphones, these devices measure movement, at different spatial and temporal scales.\n\n::: {.callout-note}\nYou could say that microphones capture body movements indirectly: sound waves are generated via the motion of specialised organs such as vocal cords.\n:::\n\nIn this course, **we focus exclusively on video recordings**.\nThere are two main ways to extract motion from videos:\n\n- **Marker-based** methods: physical markers are placed on individuals or body parts.\n- **Markerless** methods: computer vision is used to extract user-defined features directly from videos.\n\n::: {#fig-motion-capture layout=\"[[40,60], [100]]\" layout-valign=\"bottom\"}\n![By danceinthesky, openverse.org](img/motion_capture.jpg)\n\n![Source: @ants_crispr_2017](img/ants_colored.jpeg)\n\n![Source: @mathis_primer_2020](img/from_pixels_to_keypoints.png)\n\nExamples of marker-based (top row) and markerless (bottom row) methods.\n:::\n\n::: {.callout-tip title=\"Discuss\"}\n\n- What are some advantages and disadvantages of marker-based vs. markerless methods?\n- In which cases would you use one over the other?\n:::\n\n## The rise of markerless methods\n\nThere have broadly been two traditions in the study of animal behaviour [@datta_computational_2019]:\n\n- **Neuroscientists** have long focused on how animals generate behaviours in response to rewards and punishments, often training them to perform simple, easily measured actions.\n- **Ethologists**, in contrast, tend to study naturalistic behaviours expressed freely in ecologically relevant contexts, aiming to understand how behaviour is structured and organised over timeâ€”for example through the use of ethograms.\n\n::: {#fig-two-traditions layout=\"[[25,25,50]]\"}\n![By Ethan Tyler & Lex Kravitz, scidraw.io](img/mouse_lever.svg)\n\n![By Andrea Colins Rodriguez, scidraw.io](img/Monkey_schematic.svg)\n\n![Excerpt from a fiddler crab ethogram by Sanna Titus](img/crab_ethogram.png)\n\nSchematic representation of the neuroscientific tradition, juxtaposed with an ethogram.\n:::\n\nIn the last 10 years, computer vision methods for motion tracking have had an utterly transformative impact.\nTools such as [DeepLabCut](https://www.mackenziemathislab.org/deeplabcut/) [@mathis_deeplabcut_2018] and [SLEAP](https://sleap.ai/) [@pereira_sleap_2022] enable researchers to track animal movements in video recordings with impressive accuracy, without the need for physical markers.\n\n::: {#fig-tracking-gifs layout=\"[[60,40], [100]]\" layout-valign=\"bottom\"}\n![Source: mackenziemathislab.org/deeplabcut](img/MousereachGIF.gif)\n\n![Source: idtracker.ai](img/idtracker_zebrafish.gif)\n\n![Source: sleap.ai](img/sleap_movie.gif)\n\nExamples of markerless motion tracking in action.\n:::\n\nThis rise is mostly driven by advances in deep learning and has dramatically accelerated the scale at which naturalistic behaviour can be measured and analysed. The old distinctions between research traditions are being erased and a new field is emerging:\n\n> **In the past decade, a field we now call \"computational ethology\" has begun to take shape**. It involves the use of machine vision and machine learning to measure and analyze the patterns of action generated by animals in contexts designed to evoke ecologically relevant behaviors [@anderson_toward_2014]. Technical progress in statistical inference and deep learning, the democratization of high-performance computing (due to falling hardware costs and the ability to rent GPUs and CPUs in the cloud), and new and creative ideas about how to apply technology to measuring naturalistic behavior have dramatically accelerated progress in this research area. [@datta_computational_2019]\n\n![Scientific disciplines converging on markerless motion tracking methods.](img/fields_converge_on_tracking.png){#fig-fields-converge}\n\n::: {.callout-tip title=\"Discuss\"}\n\n- Which scientific disciplines do you represent?\n- What species are you working with?\n- Have you witnessed an increasing use of markerless tracking methods in your field?\n:::\n\n## The computational (neuro)ethology workflow\n\nThe rise of markerless methods has reconfigured the data acquisition and analysis workflows for scientists interested in measuring animal behaviour:\n\n- Video recordings are the primary data source.\n- Computer vision tools (most often based on deep learning) are used to extract user-defined features from video frames and track them over time.\n- The resulting tracks can then be used to quantify various aspects of motion, such as speed, orientation, distance travelled, etc.\n- The motion tracks, video features and derived kinematic features may serve as input for behaviour segmentation algorithms.\n\n![An overview of modern computational ethology workflows](img/ethology_workflow.png){#fig-ethology-workflow}\n\nAn ecosystem of open-source tools has emerged to support this workflow. See @luxem_open-source_2023, @pereira_quantifying_2020, @blau_study_2024 for comprehensive reviews. We also provide a non-exhaustive list of open-source tools in [@sec-useful-tools].\n\n::: {.callout-note title=\"Where does the 'neuro' come in?\" collapse=true}\n\nNeuroscientists are increasingly interested in precisely quantifying naturalistic behaviour, for several reasons:\n\n> if we are to understand how the brain works, we need to think about the actual problems it evolved to solve. Addressing this challenge means studying natural behavior â€” the kinds of behaviors generated by animals when they are free to act on their own internally-generated goals without physical or psychological restraint ([source: Datta Lab website](http://datta.hms.harvard.edu/research/overview/)).\n>\n> â€¦detailed examination of brain parts or their selective perturbation is not sufficient to understand how the brain generates behavior [@krakauer_neuroscience_2017].\n>\n> The behavioral work needs to be as fine-grained as work at the neural level. Otherwise one is imperiled by a granularity mismatch between levelsâ€¦ [@krakauer_neuroscience_2017].\n\nThis shift in focus within neuroscience has been a major driver for the rapid development of the computational approaches described above.\nSome refer to this field as **computational neuroethology**â€”the science of quantifying naturalistic behaviours to understand the brain [@datta_computational_2019].\n\nThe data acquisition and analysis workflows used in computational neuroethology are similar to those shown in @fig-ethology-workflow.\nThe key difference is that derived measures of behaviourâ€”whether continuous variables like speed or discrete actions such as \"grooming\"â€”are ultimately analysed alongside neural data, such as spike trains or calcium imaging traces (@fig-neuroethology-workflow).\n\n![An overview of modern computational neuroethology workflows.](img/neuroethology_workflow.png){#fig-neuroethology-workflow}\n\nHighly **recommended readings**:\n\n- @krakauer_neuroscience_2017\n- @datta_computational_2019\n- @pereira_quantifying_2020\n\n:::\n\n## The scope of this course\n\nWe will start with a primer on deep learning for computer vision (@sec-dl-cv), going over the key concepts and technologies that underpin most markerless tracking approaches.\n\nAfter that we could take a whirlwind tour through all the stages of a computaional ethology workflow [@fig-ethology-workflow] and the various tools available for each step. However, since we want this to work as a two-day hands-on workshop with plenty of time for exercises and active learning, we have instead chosen to focus on two key steps:\n\n- [@sec-sleap]: Pose estimation and tracking with [SLEAP](https://sleap.ai/) [@pereira_sleap_2022]. We chose SLEAP because we know it best, but the knowledge gained should be applicable to most other pose estimation tools.\n- [@sec-movement-intro]: Analysing motion tracks with [movement](https://movement.neuroinformatics.dev/)â€”a Python package we develop [@sirmpilatze_movement_2025]â€”followed by two case studies on real-world datasets [@sec-movement-mouse; @sec-movement-zebras].\n\n![What we'll cover in this course](img/SLEAP_and_movement.png){#fig-sleap-movement}\n\n::: {.callout-tip}\nWe want to emphasise that there are many excellent open-source tools beyond those we focus on here.\nThe next section provides a non-exhaustive list of tools that may be useful for your own projects.\n:::\n\n## Useful open-source tools {#sec-useful-tools}\n\n::: {.callout-note}\nWe've mainly highlighted tools commonly applied to animal behaviour data.\nThis is not a comprehensive list, and the tools appear in no particular order.\nSome don't fit neatly into the categories below, and their classification is somewhat subjective.\nIf you'd like to add a tool you've built or enjoy using, please [open an issue](https://github.com/neuroinformatics-unit/course-animals-in-motion/issues) or submit a pull request.\n:::\n\n### Data acquisition\n\n- [Bonsai](https://bonsai-rx.org/) [@lopes_bonsai_2015]\n\n### Video processing\n\n- [OpenCV](https://opencv.org/) @opencv_library\n- [ffmpeg](https://ffmpeg.org/) @tomar2006converting\n\n### Motion tracking\n\n- [DeepLabCut](http://www.mackenziemathislab.org/deeplabcut) @mathis_deeplabcut_2018; @lauer_multi-animal_2022\n- [SLEAP](https://sleap.ai/) @pereira_sleap_2022\n- [LightningPose](https://lightning-pose.readthedocs.io/en/latest/) @biderman_lightning_2024\n- [TRex](https://trex.run/) @walter_trex_2021\n- [idtracker.ai](https://idtrackerai.readthedocs.io/en/latest/) @romero-ferrero_idtrackerai_2019\n- [Anipose](https://anipose.readthedocs.io/en/latest/) @karashchuk_anipose_2021\n- [DANNCE](https://github.com/spoonsso/dannce/) @dunn_geometric_2021\n- [DeepPoseKit](https://github.com/jgraving/deepposekit) @graving_deepposekit_2019\n- [FastTrack](https://www.fasttrack.sh/) @gallois_fasttrack_2021\n- [DeepFly3D](https://github.com/NeLy-EPFL/DeepFly3D) @gunel_deepfly3d_2019\n\n### Motion quantification\n\n- [movement](https://github.com/neuroinformatics-unit/movement) @sirmpilatze_movement_2025\n- [animovement](https://roald-arboel.com/animovement/) @roaldarbol_animovement_2024\n- [PyRat](https://github.com/pyratlib/pyrat) @de_almeida_pyrat_2022\n- [DLC2Kinematics](https://github.com/AdaptiveMotorControlLab/DLC2Kinematics)\n- [pyomeca](https://pyomeca.github.io/pyomeca/) @Martinez2020\n- [movingpandas](https://github.com/movingpandas/movingpandas) @graser_movingpandas_2019\n- [scikit-mobility](https://github.com/scikit-mobility/scikit-mobility) @scikit-mobility\n- [Rtrack](https://rupertoverall.net/Rtrack/index.html) @Rtrack_2020\n- [ColonyTrack](https://rupertoverall.net/ColonyTrack) @colony_track_2024\n- [swaRm](https://swarm-lab.github.io/swaRm)\n\n### Behaviour segmentation\n\n- [(Keypoint) MoSeq](https://dattalab.github.io/moseq2-website/index.html) @wiltschko_mapping_2015; @weinreb_keypoint-moseq_2024\n- [VAME](https://ethoml.github.io/VAME/) @luxem_identifying_2022\n- [B-SOiD](https://github.com/YttriLab/B-SOID) @hsu_b-soid_2021\n- [A-SOiD](https://github.com/YttriLab/A-SOID) @schweihoff_-soid_2022\n- [DeepEthogram](https://github.com/jbohnslav/deepethogram) @bohnslav_deepethogram_2021\n- [SimBA](https://simba-uw-tf-dev.readthedocs.io/en/latest/) @goodwin_simple_2024\n- [DeepOF](https://deepof.readthedocs.io/en/latest/) @Miranda2023\n- [LISBET](https://github.com/BelloneLab/lisbet) @chindemi2023lisbet\n- [DLC2action](https://github.com/amathislab/DLC2action)\n- [LabGym](https://github.com/umyelab/LabGym) @hu_labgym_2023\n- [JABS](https://github.com/KumarLabJax/JABS-behavior-classifier) @beane_jax_2023\n- [JAABA](https://jaaba.sourceforge.net/) @kabra_jaaba_2013\n- [MotionMapper](https://github.com/gordonberman/MotionMapper) @berman_mapping_2014\n- [BORIS](https://www.boris.unito.it/) @friard_boris_2016\n- [MARS](https://neuroethology.github.io/MARS/) @mars2021\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":true,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"brand":{"brand":{"meta":{"name":{"short":"AiM","full":"Animals in Motion"},"link":{"home":"https://neuroinformatics.dev/course-animals-in-motion","github":"https://github.com/neuroinformatics-unit/course-animals-in-motion","zulip":"https://neuroinformatics.zulipchat.com/#narrow/channel/497885-Open-Software-Week"}},"logo":{"medium":"img/animals-in-motion-logo.png"},"color":{"palette":{"white":"#FFFFFF","dark-grey":"#1E1E1E","niu-green":"#03A062","niu-green-dark":"#04B46D","bg-orange":"#EE9040","blue":"#4178f3"},"background":"white","foreground":"dark-grey","primary":"niu-green","info":"blue","success":"niu-green","warning":"bg-orange"},"typography":{"fonts":[{"family":"Barlow","source":"google"},{"family":"JetBrains Mono","source":"google"}],"base":{"family":"Barlow"},"headings":{"family":"Barlow","weight":600},"link":{"color":"primary"},"monospace":{"family":"JetBrains Mono"}}},"data":{"meta":{"name":{"short":"AiM","full":"Animals in Motion"},"link":{"home":"https://neuroinformatics.dev/course-animals-in-motion","github":"https://github.com/neuroinformatics-unit/course-animals-in-motion","zulip":"https://neuroinformatics.zulipchat.com/#narrow/channel/497885-Open-Software-Week"}},"logo":{"medium":"img/animals-in-motion-logo.png"},"color":{"palette":{"white":"#FFFFFF","dark-grey":"#1E1E1E","niu-green":"#03A062","niu-green-dark":"#04B46D","bg-orange":"#EE9040","blue":"#4178f3"},"background":"white","foreground":"dark-grey","primary":"niu-green","info":"blue","success":"niu-green","warning":"bg-orange"},"typography":{"fonts":[{"family":"Barlow","source":"google"},{"family":"JetBrains Mono","source":"google"}],"base":{"family":"Barlow"},"headings":{"family":"Barlow","weight":600},"link":{"color":"primary"},"monospace":{"family":"JetBrains Mono"}}},"brandDir":"/Users/nsirmpilatze/Code/NIU/animals-in-motion/book","projectDir":"/Users/nsirmpilatze/Code/NIU/animals-in-motion/book","processedData":{"color":{"white":"#FFFFFF","dark-grey":"#1E1E1E","niu-green":"#03A062","niu-green-dark":"#04B46D","bg-orange":"#EE9040","blue":"#4178f3","background":"#FFFFFF","foreground":"#1E1E1E","primary":"#03A062","info":"#4178f3","success":"#03A062","warning":"#EE9040"},"typography":{"base":{"family":"Barlow"},"headings":{"family":"Barlow","weight":600},"link":{"color":"primary"},"monospace":{"family":"JetBrains Mono"},"monospace-inline":{"family":"JetBrains Mono"},"monospace-block":{"family":"JetBrains Mono"}},"logo":{"images":{},"medium":{"light":{"path":"img/animals-in-motion-logo.png"},"dark":{"path":"img/animals-in-motion-logo.png"}}}}}},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","highlight-style":"atom-one","output-file":"01-intro.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.43","bibliography":["references.bib"],"pdf":false,"jupyter":"python3","license":"CC BY","funding":{"statement":"The first edition of this workshop was made possible by a Software Sustainability Institute fellowship to Niko Sirmpilatze, as well as further funding support by the Sainsbury Wellcome Centre, the Society for Research Software Engineering and AIBIO-UK.\n"},"theme":["cosmo","brand"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}