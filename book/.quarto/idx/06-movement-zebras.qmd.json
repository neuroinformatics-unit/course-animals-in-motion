{"title":"Zebra escape trajectories","markdown":{"headingText":"Zebra escape trajectories","headingAttr":{"id":"sec-movement-zebras","classes":[],"keyvalue":[]},"containsRefs":false,"markdown":"\nIn this case study we will be using [`movement`](https://movement.neuroinformatics.dev/) to quantify the collective behaviour of a herd of zebras in escape response. The data we will use is part of a larger dataset collected in Mpala (Kenya), in which researchers simulated a predation event to study the group response of the animals. We will demonstrate how we can compute useful metrics for this analysis using `movement`.\n\n::: {.callout-note title=\"Using the right environment\"}\n\nIf you are following along this chapter on your own computer,\nmake sure to run all code snippets with the `animals-in-motion-env` environment activated\n(see [prerequisites @sec-install-movement]).\n\n:::\n\n## Dataset description\n\n<!-- Some information about the dataset, with credits to Isla, and citing the preprint.\nAnalysis will roughly follow <https://github.com/neuroinformatics-unit/zebras-stitching/blob/main/notebooks/03_notebook_compute_behaviour_metrics.py>. -->\n\nThe 3.5-min dataset presented here consists of 44 trajectories of zebras (_Equus quagga_) expressed in a coordinate system fixed to the ground. Each individual has two keypoints (`head` and `tail`). The data was obtained as follows: first, a trained [`SLEAP`](https://sleap.ai/) model was run on a video clip recorded from a camera drone, to obtain the trajectories of the zebras in a coordinate system linked to the drone. Then, the trajectories were expressed in a coordinate system fixed to the ground using [Structure-from-motion](https://en.wikipedia.org/wiki/Structure_from_motion) (with [`OpenSFM`](https://github.com/mapillary/OpenSfM) and [`OpenDroneMap`](https://github.com/OpenDroneMap/OpenDroneMap)). This transformation between camera and world coordinate systems is necessary to be able to disentangle the motion of the zebras from the movement of the camera drone. After this coordinate transformation, the data was cleaned by removing low-confidence keypoints and implausible data points.\n\nYou can find a detailed description of the approach as a collection of notebooks in [this repository](https://github.com/neuroinformatics-unit/zebras-stitching). Further details about the dataset and the prototype pipeline can be found in Duporge, Miñano _et al_ [-@duporge_tracking_2025].\n\n::: {.callout-note title=\"Acknowledgement\"}\nThe sample video and original SLEAP trajectories were kindly shared by [Dr. Isla Duporge](https://eeb.princeton.edu/people/isla-duporge) from the [Rubenstein Lab at Princeton University](https://dir.princeton.edu/), with permission to use for this workshop. The trajectories in world coordinate system were computed by Sofía Miñano, Niko Sirmpilatze and Igor Tatarnikov.\n:::\n\n## Load and explore the dataset\n\nFirst, let's load and explore the dataset\n\n```{python}\nimport matplotlib.pyplot as plt\n\nimport numpy as np\nimport xarray as xr\n\nfrom movement import sample_data\nfrom movement.kinematics import compute_pairwise_distances, compute_speed\nfrom movement.transforms import scale\nfrom movement.utils.vector import compute_norm, convert_to_unit\nfrom movement.utils.reports import report_nan_values\n```\n\n```{python}\nds = sample_data.fetch_dataset(\"SLEAP_OSFM_zebras_drone.h5\")\nds\n```\n\nWe can see the [poses dataset](https://movement.neuroinformatics.dev/user_guide/movement_dataset.html#dataset-structure) `ds` is made up of two data arrays, `position` and `confidence`. In this example, we will use the `position` data array only, which spans four dimensions: `time`, `space`, `keypoints` and `individuals`. We can verify there are 44 individuals in this dataset (`track_0` to `track_43`) and two keypoints per individual, labelled `H` (head) and `T` (tailbase). The data was collected at 29.97 frames per second, and the dataloader used this information to automatically express the `time` dimension in seconds.\n\n::: {.callout-note title=\"Position units\"}\nThe `position` data in `ds` is expressed in arbitrary units. This is because no GPS data was available for georeferencing or defining ground control points in the structure-from-motion (SfM) analysis. As a result, the scale factor remains a free parameter in the reconstruction of the world coordinates.\n\nNote however that this will not be a problem for our analysis, since the relative positions between the individuals are still correct. Moreover, we will use the median zebra body length to scale the data to more informative units. For more details on the coordinate systems involved in SfM analysis see the [OpenSfM documentation](https://opensfm.org/docs/geometry.html#world-coordinates).\n:::\n\n## Compute the body length per individual\n\nWe define the body vector for each individual as the vector going from the `T` keypoint (tail) to the `H` keypoint (head).\n\n```{python}\nbody_vector = ds.position.sel(keypoints=\"H\") - ds.position.sel(keypoints=\"T\")\n\nbody_vector\n```\n\nWe can compute the body length of each individual by computing the norm of the body vector.\n\n```{python}\n# Compute body length per individual\nbody_length = compute_norm(body_vector)\n```\n\nIt would be useful to check if there are missing values in the body length array. We can quickly inspect this using `movement`'s `report_nan_values` function.\n\n```{python}\nprint(report_nan_values(body_length))\n```\n\nThe output shows that the number of missing values per individual varies between 0.27% and 19.92%. This is not necessarily a problem for our analysis, but it is something to keep in mind when interpreting the results. These missing points are likely due to imperfect tracking of one or both of the keypoints required to compute the body vector.\n\nLet's compute some basic statistics to get a sense of the distribution of the body length values.\n\n```{python}\n# Compute basic statistics\nbody_length_std = body_length.std()\nbody_length_mean = body_length.mean()\nbody_length_median = body_length.median()\n\nprint(f\"Body length mean: {body_length_mean:.2f} a.u.\")  # a.u.: arbitrary units\nprint(f\"Body length median: {body_length_median:.2f} a.u.\")\nprint(f\"Body length std: {body_length_std:.2f} a.u.\")\n\n```\n\nWe can also plot the distribution of body lengths.\n\n```{python}\n#| code-fold: true\n#| label: fig-body-length-distribution\n#| fig-cap: \"Distribution of zebra body lengths.\"\n\nfig, ax = plt.subplots()\n\n# plot histogram of body length values\ncounts, bins, _ = body_length.plot.hist(bins=100)\n\n# add reference lines for mean and mean +- 2 stds\nax.vlines(\n    body_length_mean,\n    ymin=0,\n    ymax=np.max(counts),\n    color=\"red\",\n    linestyle=\"-\",\n    label=\"mean body length\",\n)\nlower_bound = body_length_mean - 2 * body_length_std\nupper_bound = body_length_mean + 2 * body_length_std\nfor bound in [lower_bound, upper_bound]:\n    ax.vlines(\n        bound,\n        ymin=0,\n        ymax=np.max(counts),\n        color=\"red\",\n        linestyle=\"--\",\n        label=\"mean +- 2 std\",\n    )\nax.set_ylim(0, np.max(counts))\nax.set_xlabel(\"body length (a.u.)\")\nax.set_ylabel(\"counts\")\nax.legend()\n\n```\n\nWe can see there is some variability in the body lengths per individual. Part of it may reflect the diversity across individuals, but from visual inspection of the video we expect the majority of it to be due to imperfect tracking of the keypoints. To remove some of these outliers, we continue the analysis considering only body vectors that are within 2 standard deviations of the mean.\n\n```{python}\nbody_vector_filtered = body_vector.where(\n    np.logical_and(\n        body_length <= body_length_mean + 2 * body_length_std,\n        body_length >= body_length_mean - 2 * body_length_std,\n    )\n)\n\n```\n\n## Compute polarization\n\nWe would now like to inspect the orientation of each individual in relation to the group while the simulated escape events take place.\n\nFor this, we first compute each animal's **unit body vector**. These are a scaled version of the body vectors we just computed, normalised to have unit length. `movement` provides a convenience function to do this:\n\n```{python}\nbody_vector_filtered_unit = convert_to_unit(body_vector_filtered)\n\n```\n\nWe can quickly check if their norms are now equal to 1.\n\n```{python}\nprint(compute_norm(body_vector_filtered_unit)) \n```\n\nWe now define the **herd vector** as the mean of the unit body vectors across all individuals. The mean vector of a set of $n$ vectors is the sum of all the vectors (i.e., the resultant vector) scaled by $1/n$.\n\n```{python}\nherd_vector = body_vector_filtered_unit.mean(\"individuals\")\nprint(herd_vector) \n```\n\nThe resulting array has `(time, space)` dimensions, which means that we have a single herd vector defined at each timestep.\n\nThe norm of the herd vector already gives us an intuition of how aligned the whole herd is. When its norm is close to 1, it means that the majority of the unit body vectors are aligned. When its norm is close to 0, it means that the unit body vectors are dispersed. The norm of the herd vector is sometimes called **polarization**.\n\n```{python}\npolarization = compute_norm(herd_vector)\n```\n\nWe can plot the evolution of the polarization over time to get a sense of how the herd's alignment changes.\n\n```{python}\n#| label: fig-polarization\n#| fig-cap: \"Evolution of the herd's polarization over time.\"\n\nfig, ax = plt.subplots()\nax.plot(herd_vector.time, polarization)\nax.set_ylabel(\"polarization\")\nax.set_xlabel(\"time (s)\")\nax.grid()\n```\n\nThe plot suggests that the herd alternates between periods of higher and lower polarization.\n\n## Compute average speed of the herd\n\nWe would also like to inspect how the speed of the herd changes over the course of the simulated escape events.\n\nFirst, let's scale the position data to express it in units of body lengths (BL). This will make the results more interpretable. We can use `movement`'s `scale` function to do this.\n\n```{python}\nposition_scaled = scale(\n    ds.position, \n    factor=1 / body_length_median.item(), \n    space_unit=\"body_length\",\n)\n```\n\nFor simplicity, we would also like to reduce the position of each individual to a single point. A good candidate for this is the centroid, which is the mean of all the keypoints per individual. In our case, the centroid will be the midpoint between the head and tail keypoints.\n\n```{python}\ncentroid = position_scaled.mean(\"keypoints\")\n```\n\n::: {.callout-tip title=\"Exercise A\"}\n\nUse the `centroid` data array to:\n\n1. Compute `centroid_speed`, the speed of each individual's centroid.\n2. Compute `herd_speed`, the average speed across all individuals.\n3. Plot the evolution of `herd_speed` over time.\n:::\n\n::: {.content-visible when-profile=\"answers\"}\n::: {.callout-tip title=\"Click to reveal the answers\" collapse=\"true\"}\n\n```{python}\n# Compute speed of each zebra's centroid\ncentroid_speed = compute_speed(centroid)\n\n# Compute the average speed across all individuals\nherd_speed = centroid_speed.mean(\"individuals\")\n\n# Plot the evolution of the herd speed over time\nfig, ax = plt.subplots()\nax.plot(herd_speed.time, herd_speed)\nax.set_ylabel(\"herd speed (BL/s)\")\nax.set_xlabel(\"time (s)\")\nax.grid()\n```\n\nWe can see that there are four periods in the dataset in which the speed of the herd surpasses 2 BL/s for about 10 to 20 seconds.\n\n:::\n:::\n\nWe can also inspect how the speed of each individual changes over time.\n\n```{python}\n#| code-fold: true\n#| label: fig-speed-individuals-time\n#| fig-cap: \"Evolution of the speed of each individual over time.\"\n\nfig, ax = plt.subplots()\nim = ax.matshow(\n    centroid_speed,\n    aspect=\"auto\",\n    cmap=\"viridis\",\n)\n\n# convert frames to seconds in y-axis\ntime_ticks_step = 1498\ntime_ticks = np.arange(0, len(centroid_speed.time), time_ticks_step) \ntime_labels = [f\"{t:.0f}\" for t in centroid_speed.time.values[0:-1:time_ticks_step]]\nax.set_yticks(time_ticks)\nax.set_yticklabels(time_labels)\nax.tick_params(axis='x', bottom=True, top=False, labelbottom=True, labeltop=False)\n\nax.set_xlabel(\"individual\")\nax.set_ylabel(\"time (s)\") \n\n# add colorbar\ncbar = plt.colorbar(im)\ncbar.set_label(\"speed (BL/s)\")\nax.get_images()[0].set_clim(0, 6) # cap values at 6 BL/s\n```\n\nThe plot suggests that the individuals are quite coordinated also in their change of speed. We can clearly see the four periods of higher speed, which correspond to the four simulated escape events.\n\n## Polarization vs speed\n\nLet's put it all together and inspect how polarization changes with the speed of the herd.\n\nWe will use the logarithm of the speed to focus on the higher range of speeds.\n\n```{python}\nlog10_herd_speed = np.log10(herd_speed)\n```\n\nWe can now plot the polarization in time, colouring the points by the logarithm of the speed.\n\n```{python}\n#| label: fig-polarization-vs-speed\n#| fig-cap: \"Periods of highest polarization are associated with higher speeds\"\n\nfig, ax = plt.subplots()\nsc = ax.scatter(\n    x=polarization.time,\n    y=polarization,\n    c=log10_herd_speed,\n    s=5,\n    cmap=\"turbo\",\n    # rescale color map to 1st and 99th percentiles\n    vmin=log10_herd_speed.quantile(0.01).item(),\n    vmax=log10_herd_speed.quantile(0.99).item(),\n)\n\n\nax.set_xlabel(\"time (s)\")\nax.set_ylabel(\"polarization\")\n\ncbar = plt.colorbar(sc)\ncbar.set_label(\"log10 herd speed (BL/s)\")\n```\n\nThe plot shows that for this dataset, the periods of highest polarization are associated with higher speeds. This is consistent with the interpretation that the zebras become more aligned when escaping at speed, and more dispersed when they are at rest.\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":true,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"brand":{"brand":{"meta":{"name":{"short":"AiM","full":"Animals in Motion"},"link":{"home":"https://neuroinformatics.dev/course-animals-in-motion","github":"https://github.com/neuroinformatics-unit/course-animals-in-motion","zulip":"https://neuroinformatics.zulipchat.com/#narrow/channel/497885-Open-Software-Week"}},"logo":{"medium":"img/animals-in-motion-logo.png"},"color":{"palette":{"white":"#FFFFFF","dark-grey":"#1E1E1E","niu-green":"#03A062","niu-green-dark":"#04B46D","bg-orange":"#EE9040","blue":"#4178f3"},"background":"white","foreground":"dark-grey","primary":"niu-green","info":"blue","success":"niu-green","warning":"bg-orange"},"typography":{"fonts":[{"family":"Barlow","source":"google"},{"family":"JetBrains Mono","source":"google"}],"base":{"family":"Barlow"},"headings":{"family":"Barlow","weight":600},"link":{"color":"primary"},"monospace":{"family":"JetBrains Mono"}}},"data":{"meta":{"name":{"short":"AiM","full":"Animals in Motion"},"link":{"home":"https://neuroinformatics.dev/course-animals-in-motion","github":"https://github.com/neuroinformatics-unit/course-animals-in-motion","zulip":"https://neuroinformatics.zulipchat.com/#narrow/channel/497885-Open-Software-Week"}},"logo":{"medium":"img/animals-in-motion-logo.png"},"color":{"palette":{"white":"#FFFFFF","dark-grey":"#1E1E1E","niu-green":"#03A062","niu-green-dark":"#04B46D","bg-orange":"#EE9040","blue":"#4178f3"},"background":"white","foreground":"dark-grey","primary":"niu-green","info":"blue","success":"niu-green","warning":"bg-orange"},"typography":{"fonts":[{"family":"Barlow","source":"google"},{"family":"JetBrains Mono","source":"google"}],"base":{"family":"Barlow"},"headings":{"family":"Barlow","weight":600},"link":{"color":"primary"},"monospace":{"family":"JetBrains Mono"}}},"brandDir":"/Users/nsirmpilatze/Code/NIU/animals-in-motion/book","projectDir":"/Users/nsirmpilatze/Code/NIU/animals-in-motion/book","processedData":{"color":{"white":"#FFFFFF","dark-grey":"#1E1E1E","niu-green":"#03A062","niu-green-dark":"#04B46D","bg-orange":"#EE9040","blue":"#4178f3","background":"#FFFFFF","foreground":"#1E1E1E","primary":"#03A062","info":"#4178f3","success":"#03A062","warning":"#EE9040"},"typography":{"base":{"family":"Barlow"},"headings":{"family":"Barlow","weight":600},"link":{"color":"primary"},"monospace":{"family":"JetBrains Mono"},"monospace-inline":{"family":"JetBrains Mono"},"monospace-block":{"family":"JetBrains Mono"}},"logo":{"images":{},"medium":{"light":{"path":"img/animals-in-motion-logo.png"},"dark":{"path":"img/animals-in-motion-logo.png"}}}}}},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","highlight-style":"atom-one","output-file":"06-movement-zebras.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.43","bibliography":["references.bib"],"pdf":false,"jupyter":"python3","license":"CC BY","funding":{"statement":"The first edition of this workshop was made possible by a Software Sustainability Institute fellowship to Niko Sirmpilatze, as well as further funding support by the Sainsbury Wellcome Centre, the Society for Research Software Engineering and AIBIO-UK.\n"},"theme":["cosmo","brand"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}