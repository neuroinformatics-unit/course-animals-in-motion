
@misc{schweihoff_-soid_2022,
	title = {A-{SOiD}, an active learning platform for expert-guided, data efficient discovery of behavior},
	copyright = {© 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2022.11.04.515138v1},
	doi = {10.1101/2022.11.04.515138},
	abstract = {Behavior identification and quantification techniques have undergone rapid development. To this end, supervised or unsupervised methods are chosen based upon their intrinsic strengths and weaknesses (e.g. user bias, training cost, complexity, action discovery). Here, a new active learning platform, A-SOiD, blends these strengths and in doing so, overcomes several of their inherent drawbacks. A-SOiD iteratively learns user-defined groups with a fraction of the usual training data while attaining expansive classification through directed unsupervised classification. In socially-interacting mice, A-SOiD outperformed standard methods despite requiring 85\% less training data. Additionally, it isolated two additional ethologically-distinct mouse interactions via unsupervised classification. Similar performance and efficiency was observed using non-human primate 3D pose data. In both cases, the transparency in A-SOiD’s cluster definitions revealed the defining features of the supervised classification through a game-theoretic approach. To facilitate use, A-SOiD comes as an intuitive, open-source interface for efficient segmentation of user-defined behaviors and discovered subactions.
Behavior identification and quantification techniques have undergone rapid development. To this end, supervised or unsupervised methods are chosen based upon their intrinsic strengths and weaknesses (e.g. user bias, training cost, complexity, action discovery). Here, a new active learning platform, A-SOiD, blends these strengths and in doing so, overcomes several of their inherent drawbacks. A-SOiD iteratively learns user-defined groups with a fraction of the usual training data while attaining expansive classification through directed unsupervised classification. In socially-interacting mice, A-SOiD outperformed standard methods despite requiring 85\% less training data. Additionally, it isolated two additional ethologically-distinct mouse interactions via unsupervised classification. Similar performance and efficiency was observed using non-human primate 3D pose data. In both cases, the transparency in A-SOiD’s cluster definitions revealed the defining features of the supervised classification through a game-theoretic approach. To facilitate use, A-SOiD comes as an intuitive, open-source interface for efficient segmentation of user-defined behaviors and discovered subactions.},
	language = {en},
	urldate = {2022-11-08},
	publisher = {bioRxiv},
	author = {Schweihoff, Jens F. and Hsu, Alexander I. and Schwarz, Martin K. and Yttri, Eric A.},
	month = nov,
	year = {2022},
	note = {Pages: 2022.11.04.515138
Section: New Results},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/LHWWBJM6/Schweihoff et al. - 2022 - A-SOiD, an active learning platform for expert-gui.pdf:application/pdf;Snapshot:/Users/nsirmpilatze/Zotero/storage/ICT7MB5N/2022.11.04.html:text/html},
}

@article{lopes_bonsai_2015,
	title = {Bonsai: an event-based framework for processing and controlling data streams},
	volume = {9},
	issn = {1662-5196},
	shorttitle = {Bonsai},
	url = {https://www.frontiersin.org/articles/10.3389/fninf.2015.00007},
	abstract = {The design of modern scientific experiments requires the control and monitoring of many different data streams. However, the serial execution of programming instructions in a computer makes it a challenge to develop software that can deal with the asynchronous, parallel nature of scientific data. Here we present Bonsai, a modular, high-performance, open-source visual programming framework for the acquisition and online processing of data streams. We describe Bonsai's core principles and architecture and demonstrate how it allows for the rapid and flexible prototyping of integrated experimental designs in neuroscience. We specifically highlight some applications that require the combination of many different hardware and software components, including video tracking of behavior, electrophysiology and closed-loop control of stimulation.},
	urldate = {2022-11-08},
	journal = {Frontiers in Neuroinformatics},
	author = {Lopes, Gonçalo and Bonacchi, Niccolò and Frazão, João and Neto, Joana P. and Atallah, Bassam V. and Soares, Sofia and Moreira, Luís and Matias, Sara and Itskov, Pavel M. and Correia, Patrícia A. and Medina, Roberto E. and Calcaterra, Lorenza and Dreosti, Elena and Paton, Joseph J. and Kampff, Adam R.},
	year = {2015},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/IPX8I3SP/Lopes et al. - 2015 - Bonsai an event-based framework for processing an.pdf:application/pdf},
}

@article{mathis_deeplabcut_2018,
	title = {{DeepLabCut}: markerless pose estimation of user-defined body parts with deep learning},
	volume = {21},
	copyright = {2018 The Author(s)},
	issn = {1546-1726},
	shorttitle = {{DeepLabCut}},
	url = {https://www.nature.com/articles/s41593-018-0209-y},
	doi = {10.1038/s41593-018-0209-y},
	abstract = {Quantifying behavior is crucial for many applications in neuroscience. Videography provides easy methods for the observation and recording of animal behavior in diverse settings, yet extracting particular aspects of a behavior for further analysis can be highly time consuming. In motor control studies, humans or other animals are often marked with reflective markers to assist with computer-based tracking, but markers are intrusive, and the number and location of the markers must be determined a priori. Here we present an efficient method for markerless pose estimation based on transfer learning with deep neural networks that achieves excellent results with minimal training data. We demonstrate the versatility of this framework by tracking various body parts in multiple species across a broad collection of behaviors. Remarkably, even when only a small number of frames are labeled ({\textasciitilde}200), the algorithm achieves excellent tracking performance on test frames that is comparable to human accuracy.},
	language = {en},
	number = {9},
	urldate = {2022-11-08},
	journal = {Nature Neuroscience},
	author = {Mathis, Alexander and Mamidanna, Pranav and Cury, Kevin M. and Abe, Taiga and Murthy, Venkatesh N. and Mathis, Mackenzie Weygandt and Bethge, Matthias},
	month = sep,
	year = {2018},
	note = {Number: 9
Publisher: Nature Publishing Group},
	keywords = {Computational neuroscience, Bs41592-022-01426-1ehavioural methods, Machine learning},
	pages = {1281--1289},
	file = {Mathis et al. - 2018 - DeepLabCut markerless pose estimation of user-def.pdf:/Users/nsirmpilatze/Zotero/storage/I2K9ZHDW/Mathis et al. - 2018 - DeepLabCut markerless pose estimation of user-def.pdf:application/pdf;Snapshot:/Users/nsirmpilatze/Zotero/storage/WDSLJFIR/s41593-018-0209-y.html:text/html},
}

@article{lauer_multi-animal_2022,
	title = {Multi-animal pose estimation, identification and tracking with {DeepLabCut}},
	volume = {19},
	copyright = {2022 The Author(s)},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/s41592-022-01443-0},
	doi = {10.1038/s41592-022-01443-0},
	abstract = {Estimating the pose of multiple animals is a challenging computer vision problem: frequent interactions cause occlusions and complicate the association of detected keypoints to the correct individuals, as well as having highly similar looking animals that interact more closely than in typical multi-human scenarios. To take up this challenge, we build on DeepLabCut, an open-source pose estimation toolbox, and provide high-performance animal assembly and tracking—features required for multi-animal scenarios. Furthermore, we integrate the ability to predict an animal’s identity to assist tracking (in case of occlusions). We illustrate the power of this framework with four datasets varying in complexity, which we release to serve as a benchmark for future algorithm development.},
	language = {en},
	number = {4},
	urldate = {2022-11-08},
	journal = {Nature Methods},
	author = {Lauer, Jessy and Zhou, Mu and Ye, Shaokai and Menegas, William and Schneider, Steffen and Nath, Tanmay and Rahman, Mohammed Mostafizur and Di Santo, Valentina and Soberanes, Daniel and Feng, Guoping and Murthy, Venkatesh N. and Lauder, George and Dulac, Catherine and Mathis, Mackenzie Weygandt and Mathis, Alexander},
	month = apr,
	year = {2022},
	note = {Number: 4
Publisher: Nature Publishing Group},
	keywords = {Zoology, Computational neuroscience, Behavioural methods, Machine learning},
	pages = {496--504},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/RLYCET7V/Lauer et al. - 2022 - Multi-animal pose estimation, identification and t.pdf:application/pdf;Snapshot:/Users/nsirmpilatze/Zotero/storage/VRJA8R8I/s41592-022-01443-0.html:text/html},
}

@article{pereira_sleap_2022,
	title = {{SLEAP}: {A} deep learning system for multi-animal pose tracking},
	volume = {19},
	copyright = {2022 The Author(s)},
	issn = {1548-7105},
	shorttitle = {{SLEAP}},
	url = {https://www.nature.com/articles/s41592-022-01426-1},
	doi = {10.1038/s41592-022-01426-1},
	abstract = {The desire to understand how the brain generates and patterns behavior has driven rapid methodological innovation in tools to quantify natural animal behavior. While advances in deep learning and computer vision have enabled markerless pose estimation in individual animals, extending these to multiple animals presents unique challenges for studies of social behaviors or animals in their natural environments. Here we present Social LEAP Estimates Animal Poses (SLEAP), a machine learning system for multi-animal pose tracking. This system enables versatile workflows for data labeling, model training and inference on previously unseen data. SLEAP features an accessible graphical user interface, a standardized data model, a reproducible configuration system, over 30 model architectures, two approaches to part grouping and two approaches to identity tracking. We applied SLEAP to seven datasets across flies, bees, mice and gerbils to systematically evaluate each approach and architecture, and we compare it with other existing approaches. SLEAP achieves greater accuracy and speeds of more than 800 frames per second, with latencies of less than 3.5 ms at full 1,024 × 1,024 image resolution. This makes SLEAP usable for real-time applications, which we demonstrate by controlling the behavior of one animal on the basis of the tracking and detection of social interactions with another animal.},
	language = {en},
	number = {4},
	urldate = {2022-11-08},
	journal = {Nature Methods},
	author = {Pereira, Talmo D. and Tabris, Nathaniel and Matsliah, Arie and Turner, David M. and Li, Junyu and Ravindranath, Shruthi and Papadoyannis, Eleni S. and Normand, Edna and Deutsch, David S. and Wang, Z. Yan and McKenzie-Smith, Grace C. and Mitelut, Catalin C. and Castro, Marielisa Diez and D’Uva, John and Kislin, Mikhail and Sanes, Dan H. and Kocher, Sarah D. and Wang, Samuel S.-H. and Falkner, Annegret L. and Shaevitz, Joshua W. and Murthy, Mala},
	month = apr,
	year = {2022},
	note = {Number: 4
Publisher: Nature Publishing Group},
	keywords = {Computational neuroscience, Software, Machine learning},
	pages = {486--495},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/KZWJJE4T/Pereira et al. - 2022 - SLEAP A deep learning system for multi-animal pos.pdf:application/pdf;Snapshot:/Users/nsirmpilatze/Zotero/storage/Y662EYII/s41592-022-01426-1.html:text/html},
}

@article{walter_trex_2021,
	title = {{TRex}, a fast multi-animal tracking system with markerless identification, and {2D} estimation of posture and visual fields},
	volume = {10},
	issn = {2050-084X},
	url = {https://doi.org/10.7554/eLife.64000},
	doi = {10.7554/eLife.64000},
	abstract = {Automated visual tracking of animals is rapidly becoming an indispensable tool for the study of behavior. It offers a quantitative methodology by which organisms’ sensing and decision-making can be studied in a wide range of ecological contexts. Despite this, existing solutions tend to be challenging to deploy in practice, especially when considering long and/or high-resolution video-streams. Here, we present TRex, a fast and easy-to-use solution for tracking a large number of individuals simultaneously using background-subtraction with real-time (60 Hz) tracking performance for up to approximately 256 individuals and estimates 2D visual-fields, outlines, and head/rear of bilateral animals, both in open and closed-loop contexts. Additionally, TRex offers highly accurate, deep-learning-based visual identification of up to approximately 100 unmarked individuals, where it is between 2.5 and 46.7 times faster, and requires 2–10 times less memory, than comparable software (with relative performance increasing for more organisms/longer videos) and provides interactive data-exploration within an intuitive, platform-independent graphical user-interface.},
	urldate = {2022-11-08},
	journal = {eLife},
	author = {Walter, Tristan and Couzin, Iain D},
	editor = {Lentink, David and Rutz, Christian and Pujades, Sergi},
	month = feb,
	year = {2021},
	note = {Publisher: eLife Sciences Publications, Ltd},
	keywords = {c. cyphergaster, p. reticulata, posture estimation, s. gregaria, tracking, visual field},
	pages = {e64000},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/834RWMNS/Walter and Couzin - 2021 - TRex, a fast multi-animal tracking system with mar.pdf:application/pdf},
}

@misc{nilsson_simple_2020,
	title = {Simple {Behavioral} {Analysis} ({SimBA}) – an open source toolkit for computer classification of complex social behaviors in experimental animals},
	copyright = {© 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NoDerivs 4.0 International), CC BY-ND 4.0, as described at http://creativecommons.org/licenses/by-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2020.04.19.049452v2},
	doi = {10.1101/2020.04.19.049452},
	abstract = {Aberrant social behavior is a core feature of many neuropsychiatric disorders, yet the study of complex social behavior in freely moving rodents is relatively infrequently incorporated into preclinical models. This likely contributes to limited translational impact. A major bottleneck for the adoption of socially complex, ethology-rich, preclinical procedures are the technical limitations for consistently annotating detailed behavioral repertoires of rodent social behavior. Manual annotation is subjective, prone to observer drift, and extremely time-intensive. Commercial approaches are expensive and inferior to manual annotation. Open-source alternatives often require significant investments in specialized hardware and significant computational and programming knowledge. By combining recent computational advances in convolutional neural networks and pose-estimation with further machine learning analysis, complex rodent social behavior is primed for inclusion under the umbrella of computational neuroethology.
Here we present an open-source package with graphical interface and workflow (Simple Behavioral Analysis, SimBA) that uses pose-estimation to create supervised machine learning predictive classifiers of rodent social behavior, with millisecond resolution and accuracies that can out-perform human observers. SimBA does not require specialized video acquisition hardware nor extensive computational background. Standard descriptive statistical analysis, along with graphical region of interest annotation, are provided in addition to predictive classifier generation. To increase ease-of-use for behavioural neuroscientists, we designed SimBA with accessible menus for pre-processing videos, annotating behavioural training datasets, selecting advanced machine learning options, robust classifier validation functions and flexible visualizations tools. This allows for predictive classifier transparency, explainability and tunability prior to, and during, experimental use. We demonstrate that this approach is flexible and robust in both mice and rats by classifying social behaviors that are commonly central to the study of brain function and social motivation. Finally, we provide a library of poseestimation weights and behavioral predictive classifiers for resident-intruder behaviors in mice and rats. All code and data, together with detailed tutorials and documentation, are available on the SimBA GitHub repository.
Graphical abstract SimBA graphical interface (GUI) for creating supervised machine learning classifiers of rodent social behavior.(a) Pre-process videos. SimBA supports common video pre-processing functions (e.g., cropping, clipping, sampling, format conversion, etc.) that can be performed either on single videos, or as a batch.(b) Managing poseestimation data and creating classification projects. Pose-estimation tracking projects in DeepLabCut and DeepPoseKit can be either imported or created and managed within the SimBA graphical user interface, and the tracking results are imported into SimBA classification projects.SimBA also supports userdrawn region-of-interests (ROIs) for descriptive statistics of animal movements, or as features in machine learning classification projects.(c) Create classifiers, perform classifications, and analyze classification data. SimBA has graphical tools for correcting poseestimation tracking inaccuracies when multiple subjects are within a single frame, annotating behavioral events from videos, and optimizing machine learning hyperparameters and discrimination thresholds. A number of validation checkpoints and logs are included for increased classifier explainability and tunability prior to, and during, experimental use. Both detailed and summary data are provided at the end of classifier analysis. SimBA accepts behavioral annotations generated elsewhere (such as through JWatcher) that can be imported into SimBA classification projects.(d) Visualize classification results. SimBA has several options for visualizing machine learning classifications, animal movements and ROI data, and analyzing the durations and frequencies of classified behaviors.See the SimBA GitHub repository for a comprehensive documentation and user tutorials.{\textless}img class="highwire-fragment fragment-image" alt="Figure" src="https://www.biorxiv.org/content/biorxiv/early/2020/04/21/2020.04.19.049452/F1.medium.gif" width="305" height="440"/{\textgreater}},
	language = {en},
	urldate = {2022-11-08},
	publisher = {bioRxiv},
	author = {Nilsson, Simon RO and Goodwin, Nastacia L. and Choong, Jia Jie and Hwang, Sophia and Wright, Hayden R. and Norville, Zane C. and Tong, Xiaoyu and Lin, Dayu and Bentzley, Brandon S. and Eshel, Neir and McLaughlin, Ryan J. and Golden, Sam A.},
	month = apr,
	year = {2020},
	note = {Pages: 2020.04.19.049452
Section: New Results},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/VJ8FWA86/Nilsson et al. - 2020 - Simple Behavioral Analysis (SimBA) – an open sourc.pdf:application/pdf;Snapshot:/Users/nsirmpilatze/Zotero/storage/LZUMWIHB/2020.04.19.html:text/html},
}

@misc{schneider_learnable_2022,
	title = {Learnable latent embeddings for joint behavioral and neural analysis},
	url = {http://arxiv.org/abs/2204.00673},
	doi = {10.48550/arXiv.2204.00673},
	abstract = {Mapping behavioral actions to neural activity is a fundamental goal of neuroscience. As our ability to record large neural and behavioral data increases, there is growing interest in modeling neural dynamics during adaptive behaviors to probe neural representations. In particular, neural latent embeddings can reveal underlying correlates of behavior, yet, we lack non-linear techniques that can explicitly and flexibly leverage joint behavior and neural data. Here, we fill this gap with a novel method, CEBRA, that jointly uses behavioral and neural data in a hypothesis- or discovery-driven manner to produce consistent, high-performance latent spaces. We validate its accuracy and demonstrate our tool's utility for both calcium and electrophysiology datasets, across sensory and motor tasks, and in simple or complex behaviors across species. It allows for single and multi-session datasets to be leveraged for hypothesis testing or can be used label-free. Lastly, we show that CEBRA can be used for the mapping of space, uncovering complex kinematic features, and rapid, high-accuracy decoding of natural movies from visual cortex.},
	urldate = {2022-11-08},
	publisher = {arXiv},
	author = {Schneider, Steffen and Lee, Jin Hwa and Mathis, Mackenzie Weygandt},
	month = oct,
	year = {2022},
	note = {arXiv:2204.00673 [cs, q-bio]},
	keywords = {Computer Science - Machine Learning, Quantitative Biology - Neurons and Cognition, Quantitative Biology - Quantitative Methods},
	file = {arXiv Fulltext PDF:/Users/nsirmpilatze/Zotero/storage/MI2YA4SY/Schneider et al. - 2022 - Learnable latent embeddings for joint behavioral a.pdf:application/pdf;arXiv.org Snapshot:/Users/nsirmpilatze/Zotero/storage/3NKTNX6Q/2204.html:text/html},
}

@article{kabra_jaaba_2013,
	title = {{JAABA}: interactive machine learning for automatic annotation of animal behavior},
	volume = {10},
	copyright = {2012 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1548-7105},
	shorttitle = {{JAABA}},
	url = {https://www.nature.com/articles/nmeth.2281},
	doi = {10.1038/nmeth.2281},
	abstract = {Open-source software that allows biologists to create a variety of behavior classifiers for automatically annotating video of behaving animals is presented. The program, called JAABA, uses state-of-the-art machine-learning methods and is applicable to tracking data from different organisms, including mice and adult and larval Drosophila.},
	language = {en},
	number = {1},
	urldate = {2022-11-08},
	journal = {Nature Methods},
	author = {Kabra, Mayank and Robie, Alice A. and Rivera-Alba, Marta and Branson, Steven and Branson, Kristin},
	month = jan,
	year = {2013},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Behavioural methods, Machine learning, Experimental organisms},
	pages = {64--67},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/U72RA2LU/Kabra et al. - 2013 - JAABA interactive machine learning for automatic .pdf:application/pdf;Snapshot:/Users/nsirmpilatze/Zotero/storage/9PD5CXJT/nmeth.html:text/html},
}

@article{de_almeida_pyrat_2022,
	title = {{PyRAT}: {An} {Open}-{Source} {Python} {Library} for {Animal} {Behavior} {Analysis}},
	volume = {16},
	issn = {1662-453X},
	shorttitle = {{PyRAT}},
	url = {https://www.frontiersin.org/articles/10.3389/fnins.2022.779106},
	abstract = {Here we developed an open-source Python-based library called Python rodent Analysis and Tracking (PyRAT). Our library analyzes tracking data to classify distinct behaviors, estimate traveled distance, speed and area occupancy. To classify and cluster behaviors, we used two unsupervised algorithms: hierarchical agglomerative clustering and t-distributed stochastic neighbor embedding (t-SNE). Finally, we built algorithms that associate the detected behaviors with synchronized neural data and facilitate the visualization of this association in the pixel space. PyRAT is fully available on GitHub: https://github.com/pyratlib/pyrat.},
	urldate = {2022-11-08},
	journal = {Frontiers in Neuroscience},
	author = {De Almeida, Tulio Fernandes and Spinelli, Bruno Guedes and Hypolito Lima, Ramón and Gonzalez, Maria Carolina and Rodrigues, Abner Cardoso},
	year = {2022},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/CH29HVUC/De Almeida et al. - 2022 - PyRAT An Open-Source Python Library for Animal Be.pdf:application/pdf},
}

@misc{calhoun_what_2021,
	title = {What is behavior? {No} seriously, what is it?},
	copyright = {© 2021, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
	shorttitle = {What is behavior?},
	url = {https://www.biorxiv.org/content/10.1101/2021.07.04.451053v1},
	doi = {10.1101/2021.07.04.451053},
	abstract = {Studying ‘behavior’ lies at the heart of many disciplines. Nevertheless, academics rarely provide an explicit definition of what ‘behavior’ actually is. What range of definitions do people use, and how does that vary across disciplines? To answer these questions we have developed a survey to probe what constitutes ‘behavior’. We find that academics adopt different definitions of behavior according to their academic discipline, animal model that they work with, and level of academic seniority. Using hierarchical clustering, we identify at least six distinct types of ‘behavior’ which are used in seven distinct operational archetypes of ‘behavior’. Individual respondents have clear consistent definitions of behavior, but these definitions are not consistent across the population. Our study is a call for academics to clarify what they mean by ‘behavior’ wherever they study it, with the hope that this will foster interdisciplinary studies that will improve our understanding of behavioral phenomena.},
	language = {en},
	urldate = {2022-11-21},
	publisher = {bioRxiv},
	author = {Calhoun, Adam and Hady, Ahmed El},
	month = jul,
	year = {2021},
	note = {Pages: 2021.07.04.451053
Section: New Results},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/NER3HPF4/Calhoun and Hady - 2021 - What is behavior No seriously, what is it.pdf:application/pdf;Snapshot:/Users/nsirmpilatze/Zotero/storage/CJEL3YHV/2021.07.04.html:text/html},
}

@article{levitis_behavioural_2009,
	title = {Behavioural biologists don't agree on what constitutes behaviour},
	volume = {78},
	issn = {0003-3472},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2760923/},
	doi = {10.1016/j.anbehav.2009.03.018},
	abstract = {Behavioural biology is a major discipline within biology, centred on the key concept of `behaviour.' But how is `behaviour' defined, and how should it be defined? We outline what characteristics we believe a scientific definition should have, and why we think it important that a definition have these traits. We then examine the range of available published definitions for the word. Finding no consensus, we present survey responses from 174 members of three behaviour-focused scientific societies as to their understanding of the term. Here again, we find surprisingly widespread disagreement as to what qualifies as behaviour. Respondents contradict themselves, each other, and published definitions, indicating that they are using individually variable intuitive, rather than codified, meanings of `behaviour.' We offer a new definition, based largely on survey responses: “Behaviour is the internally coordinated responses (actions or inactions) of whole living organisms (individuals or groups) to internal and/or external stimuli, excluding responses more easily understood as developmental changes.” Finally, we discuss the usage, meanings and limitations of this definition.},
	number = {1},
	urldate = {2022-11-21},
	journal = {Animal behaviour},
	author = {Levitis, Daniel A. and Lidicker, William Z. and Freund, Glenn},
	month = jul,
	year = {2009},
	pmid = {20160973},
	pmcid = {PMC2760923},
	pages = {103--110},
	file = {PubMed Central Full Text PDF:/Users/nsirmpilatze/Zotero/storage/HCJ8HI4V/Levitis et al. - 2009 - Behavioural biologists don't agree on what constit.pdf:application/pdf},
}

@article{nath_using_2019,
	title = {Using {DeepLabCut} for {3D} markerless pose estimation across species and behaviors},
	volume = {14},
	copyright = {2019 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {1750-2799},
	url = {https://www.nature.com/articles/s41596-019-0176-0},
	doi = {10.1038/s41596-019-0176-0},
	abstract = {Noninvasive behavioral tracking of animals during experiments is critical to many scientific pursuits. Extracting the poses of animals without using markers is often essential to measuring behavioral effects in biomechanics, genetics, ethology, and neuroscience. However, extracting detailed poses without markers in dynamically changing backgrounds has been challenging. We recently introduced an open-source toolbox called DeepLabCut that builds on a state-of-the-art human pose-estimation algorithm to allow a user to train a deep neural network with limited training data to precisely track user-defined features that match human labeling accuracy. Here, we provide an updated toolbox, developed as a Python package, that includes new features such as graphical user interfaces (GUIs), performance improvements, and active-learning-based network refinement. We provide a step-by-step procedure for using DeepLabCut that guides the user in creating a tailored, reusable analysis pipeline with a graphical processing unit (GPU) in 1–12 h (depending on frame size). Additionally, we provide Docker environments and Jupyter Notebooks that can be run on cloud resources such as Google Colaboratory.},
	language = {en},
	number = {7},
	urldate = {2022-11-21},
	journal = {Nature Protocols},
	author = {Nath, Tanmay and Mathis, Alexander and Chen, An Chi and Patel, Amir and Bethge, Matthias and Mathis, Mackenzie Weygandt},
	month = jul,
	year = {2019},
	note = {Number: 7
Publisher: Nature Publishing Group},
	keywords = {Software, Behavioural methods, Computational platforms and environments, Learning algorithms},
	pages = {2152--2176},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/LDG84AMV/Nath et al. - 2019 - Using DeepLabCut for 3D markerless pose estimation.pdf:application/pdf},
}

@article{marken_you_2009,
	title = {You {Say} you {Had} a {Revolution}: {Methodological} {Foundations} of {Closed}-{Loop} {Psychology}},
	volume = {13},
	issn = {1089-2680},
	shorttitle = {You {Say} you {Had} a {Revolution}},
	url = {https://doi.org/10.1037/a0015106},
	doi = {10.1037/a0015106},
	abstract = {To the extent that a scientific revolution represents a fundamental change in a discipline, the cognitive revolution in psychology was not particularly revolutionary. What changed least in this revolution was methodology. The experimental methods used in cognitive psychology are the same as those used in the behaviorism it overthrew. This methodological continuity results from the fact that both behaviorism and cognitive psychology are based on the same paradigm, which is also the basis of experimental psychology: the open-loop causal model of behavioral organization. A truly revolutionary approach to understanding the mind has been largely ignored because it is built on a paradigm that is inconsistent with conventional research methods. This new approach to psychology, called Perceptual Control Theory (PCT), is based on a closed-loop control model of behavioral organization that is tested using control engineering methods that are unfamiliar to most psychologists. This paper introduces the methodological foundations of closed-loop psychology, explains why the closed-loop revolution has not happened yet, and suggests what psychology might look like after the revolution has occurred.},
	language = {en},
	number = {2},
	urldate = {2022-11-21},
	journal = {Review of General Psychology},
	author = {Marken, Richard S.},
	month = jun,
	year = {2009},
	note = {Publisher: SAGE Publications Inc},
	pages = {137--145},
	file = {SAGE PDF Full Text:/Users/nsirmpilatze/Zotero/storage/E6E5UZJY/Marken - 2009 - You Say you Had a Revolution Methodological Found.pdf:application/pdf},
}

@article{niv_primacy_nodate,
	title = {The primacy of behavioral research for understanding the brain.},
	volume = {135},
	issn = {1939-0084},
	url = {https://psycnet.apa.org/fulltext/2021-53272-001.pdf},
	doi = {10.1037/bne0000471},
	number = {5},
	urldate = {2022-11-21},
	journal = {Behavioral Neuroscience},
	author = {Niv, Yael},
	note = {Publisher: US: American Psychological Association},
	pages = {601},
	file = {Snapshot:/Users/nsirmpilatze/Zotero/storage/JLCYXRGW/2021-53272-001.html:text/html;Submitted Version:/Users/nsirmpilatze/Zotero/storage/DD2P9355/Niv - The primacy of behavioral research for understandi.pdf:application/pdf},
}

@article{luxem_identifying_2022,
	title = {Identifying behavioral structure from deep variational embeddings of animal motion},
	volume = {5},
	copyright = {2022 The Author(s)},
	issn = {2399-3642},
	url = {https://www.nature.com/articles/s42003-022-04080-7},
	doi = {10.1038/s42003-022-04080-7},
	abstract = {Quantification and detection of the hierarchical organization of behavior is a major challenge in neuroscience. Recent advances in markerless pose estimation enable the visualization of high-dimensional spatiotemporal behavioral dynamics of animal motion. However, robust and reliable technical approaches are needed to uncover underlying structure in these data and to segment behavior into discrete hierarchically organized motifs. Here, we present an unsupervised probabilistic deep learning framework that identifies behavioral structure from deep variational embeddings of animal motion (VAME). By using a mouse model of beta amyloidosis as a use case, we show that VAME not only identifies discrete behavioral motifs, but also captures a hierarchical representation of the motif’s usage. The approach allows for the grouping of motifs into communities and the detection of differences in community-specific motif usage of individual mouse cohorts that were undetectable by human visual observation. Thus, we present a robust approach for the segmentation of animal motion that is applicable to a wide range of experimental setups, models and conditions without requiring supervised or a-priori human interference.},
	language = {en},
	number = {1},
	urldate = {2022-11-22},
	journal = {Communications Biology},
	author = {Luxem, Kevin and Mocellin, Petra and Fuhrmann, Falko and Kürsch, Johannes and Miller, Stephanie R. and Palop, Jorge J. and Remy, Stefan and Bauer, Pavol},
	month = nov,
	year = {2022},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Computational neuroscience, Behavioural methods},
	pages = {1--15},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/GQMXGQJ6/Luxem et al. - 2022 - Identifying behavioral structure from deep variati.pdf:application/pdf;Snapshot:/Users/nsirmpilatze/Zotero/storage/CJ2HCESG/s42003-022-04080-7.html:text/html},
}

@article{sturman_deep_2020,
	title = {Deep learning-based behavioral analysis reaches human accuracy and is capable of outperforming commercial solutions},
	volume = {45},
	copyright = {2020 The Author(s)},
	issn = {1740-634X},
	url = {https://www.nature.com/articles/s41386-020-0776-y},
	doi = {10.1038/s41386-020-0776-y},
	abstract = {To study brain function, preclinical research heavily relies on animal monitoring and the subsequent analyses of behavior. Commercial platforms have enabled semi high-throughput behavioral analyses by automating animal tracking, yet they poorly recognize ethologically relevant behaviors and lack the flexibility to be employed in variable testing environments. Critical advances based on deep-learning and machine vision over the last couple of years now enable markerless tracking of individual body parts of freely moving rodents with high precision. Here, we compare the performance of commercially available platforms (EthoVision XT14, Noldus; TSE Multi-Conditioning System, TSE Systems) to cross-verified human annotation. We provide a set of videos—carefully annotated by several human raters—of three widely used behavioral tests (open field test, elevated plus maze, forced swim test). Using these data, we then deployed the pose estimation software DeepLabCut to extract skeletal mouse representations. Using simple post-analyses, we were able to track animals based on their skeletal representation in a range of classic behavioral tests at similar or greater accuracy than commercial behavioral tracking systems. We then developed supervised machine learning classifiers that integrate the skeletal representation with the manual annotations. This new combined approach allows us to score ethologically relevant behaviors with similar accuracy to humans, the current gold standard, while outperforming commercial solutions. Finally, we show that the resulting machine learning approach eliminates variation both within and between human annotators. In summary, our approach helps to improve the quality and accuracy of behavioral data, while outperforming commercial systems at a fraction of the cost.},
	language = {en},
	number = {11},
	urldate = {2022-11-24},
	journal = {Neuropsychopharmacology},
	author = {Sturman, Oliver and von Ziegler, Lukas and Schläppi, Christa and Akyol, Furkan and Privitera, Mattia and Slominski, Daria and Grimm, Christina and Thieren, Laetitia and Zerbi, Valerio and Grewe, Benjamin and Bohacek, Johannes},
	month = oct,
	year = {2020},
	note = {Number: 11
Publisher: Nature Publishing Group},
	keywords = {Behavioural methods, Anxiety},
	pages = {1942--1952},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/DL9FDINA/Sturman et al. - 2020 - Deep learning-based behavioral analysis reaches hu.pdf:application/pdf;Snapshot:/Users/nsirmpilatze/Zotero/storage/BG7DSXKW/s41386-020-0776-y.html:text/html},
}

@article{wiltschko_revealing_2020,
	title = {Revealing the structure of pharmacobehavioral space through motion sequencing},
	volume = {23},
	issn = {1546-1726},
	doi = {10.1038/s41593-020-00706-3},
	abstract = {Understanding how genes, drugs and neural circuits influence behavior requires the ability to effectively organize information about similarities and differences within complex behavioral datasets. Motion Sequencing (MoSeq) is an ethologically inspired behavioral analysis method that identifies modular components of three-dimensional mouse body language called 'syllables'. Here, we show that MoSeq effectively parses behavioral differences and captures similarities elicited by a panel of neuroactive and psychoactive drugs administered to a cohort of nearly 700 mice. MoSeq identifies syllables that are characteristic of individual drugs, a finding we leverage to reveal specific on- and off-target effects of both established and candidate therapeutics in a mouse model of autism spectrum disorder. These results demonstrate that MoSeq can meaningfully organize large-scale behavioral data, illustrate the power of a fundamentally modular description of behavior and suggest that behavioral syllables represent a new class of druggable target.},
	language = {eng},
	number = {11},
	journal = {Nature Neuroscience},
	author = {Wiltschko, Alexander B. and Tsukahara, Tatsuya and Zeine, Ayman and Anyoha, Rockwell and Gillis, Winthrop F. and Markowitz, Jeffrey E. and Peterson, Ralph E. and Katon, Jesse and Johnson, Matthew J. and Datta, Sandeep Robert},
	month = nov,
	year = {2020},
	pmid = {32958923},
	pmcid = {PMC7606807},
	keywords = {Animals, Male, Behavior, Animal, Pattern Recognition, Automated, Behavior Observation Techniques, Mice, Inbred C57BL, Video Recording},
	pages = {1433--1443},
	file = {Accepted Version:/Users/nsirmpilatze/Zotero/storage/YKM8RN2P/Wiltschko et al. - 2020 - Revealing the structure of pharmacobehavioral spac.pdf:application/pdf},
}

@article{wiltschko_mapping_2015,
	title = {Mapping {Sub}-{Second} {Structure} in {Mouse} {Behavior}},
	volume = {88},
	issn = {0896-6273},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4708087/},
	doi = {10.1016/j.neuron.2015.11.031},
	abstract = {Complex animal behaviors are likely built from simpler modules, but their systematic identification in mammals remains a significant challenge. Here we use depth imaging to show that three-dimensional (3D) mouse pose dynamics are structured at the sub-second timescale. Computational modeling of these fast dynamics effectively describes mouse behavior as a series of reused and stereotyped modules with defined transition probabilities. We demonstrate this combined 3D imaging and machine learning method can be used to unmask potential strategies employed by the brain to adapt to the environment, to capture both predicted and previously-hidden phenotypes caused by genetic or neural manipulations, and to systematically expose the global structure of behavior within an experiment. This work reveals that mouse body language is built from identifiable components and is organized in a predictable fashion; deciphering this language establishes an objective framework for characterizing the influence of environmental cues, genes and neural activity on behavior.},
	number = {6},
	urldate = {2022-11-24},
	journal = {Neuron},
	author = {Wiltschko, Alexander B. and Johnson, Matthew J. and Iurilli, Giuliano and Peterson, Ralph E. and Katon, Jesse M. and Pashkovski, Stan L. and Abraira, Victoria E. and Adams, Ryan P. and Datta, Sandeep Robert},
	month = dec,
	year = {2015},
	pmid = {26687221},
	pmcid = {PMC4708087},
	pages = {1121--1135},
	file = {PubMed Central Full Text PDF:/Users/nsirmpilatze/Zotero/storage/4GI9BDKC/Wiltschko et al. - 2015 - Mapping Sub-Second Structure in Mouse Behavior.pdf:application/pdf},
}

@article{datta_computational_2019,
	title = {Computational {Neuroethology}: {A} {Call} to {Action}},
	volume = {104},
	issn = {1097-4199},
	shorttitle = {Computational {Neuroethology}},
	doi = {10.1016/j.neuron.2019.09.038},
	abstract = {The brain is worthy of study because it is in charge of behavior. A flurry of recent technical advances in measuring and quantifying naturalistic behaviors provide an important opportunity for advancing brain science. However, the problem of understanding unrestrained behavior in the context of neural recordings and manipulations remains unsolved, and developing approaches to addressing this challenge is critical. Here we discuss considerations in computational neuroethology-the science of quantifying naturalistic behaviors for understanding the brain-and propose strategies to evaluate progress. We point to open questions that require resolution and call upon the broader systems neuroscience community to further develop and leverage measures of naturalistic, unrestrained behavior, which will enable us to more effectively probe the richness and complexity of the brain.},
	language = {eng},
	number = {1},
	journal = {Neuron},
	author = {Datta, Sandeep Robert and Anderson, David J. and Branson, Kristin and Perona, Pietro and Leifer, Andrew},
	month = oct,
	year = {2019},
	pmid = {31600508},
	pmcid = {PMC6981239},
	keywords = {Animals, Brain, Behavior, Animal, Electrophysiological Phenomena, Ethology, Machine Learning, Neurosciences},
	pages = {11--24},
	file = {Full Text:/Users/nsirmpilatze/Zotero/storage/PI62MM24/Datta et al. - 2019 - Computational Neuroethology A Call to Action.pdf:application/pdf},
}

@article{hsu_b-soid_2021,
	title = {B-{SOiD}, an open-source unsupervised algorithm for identification and fast prediction of behaviors},
	volume = {12},
	copyright = {2021 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-021-25420-x},
	doi = {10.1038/s41467-021-25420-x},
	abstract = {Studying naturalistic animal behavior remains a difficult objective. Recent machine learning advances have enabled limb localization; however, extracting behaviors requires ascertaining the spatiotemporal patterns of these positions. To provide a link from poses to actions and their kinematics, we developed B-SOiD - an open-source, unsupervised algorithm that identifies behavior without user bias. By training a machine classifier on pose pattern statistics clustered using new methods, our approach achieves greatly improved processing speed and the ability to generalize across subjects or labs. Using a frameshift alignment paradigm, B-SOiD overcomes previous temporal resolution barriers. Using only a single, off-the-shelf camera, B-SOiD provides categories of sub-action for trained behaviors and kinematic measures of individual limb trajectories in any animal model. These behavioral and kinematic measures are difficult but critical to obtain, particularly in the study of rodent and other models of pain, OCD, and movement disorders.},
	language = {en},
	number = {1},
	urldate = {2022-11-24},
	journal = {Nature Communications},
	author = {Hsu, Alexander I. and Yttri, Eric A.},
	month = aug,
	year = {2021},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Behavioural methods, Diseases of the nervous system, Motor control},
	pages = {5188},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/XDUPWNZF/Hsu and Yttri - 2021 - B-SOiD, an open-source unsupervised algorithm for .pdf:application/pdf;Snapshot:/Users/nsirmpilatze/Zotero/storage/LP3WRSXC/s41467-021-25420-x.html:text/html},
}

@article{luxem_identifying_2022-1,
	title = {Identifying behavioral structure from deep variational embeddings of animal motion},
	volume = {5},
	copyright = {2022 The Author(s)},
	issn = {2399-3642},
	url = {https://www.nature.com/articles/s42003-022-04080-7},
	doi = {10.1038/s42003-022-04080-7},
	abstract = {Quantification and detection of the hierarchical organization of behavior is a major challenge in neuroscience. Recent advances in markerless pose estimation enable the visualization of high-dimensional spatiotemporal behavioral dynamics of animal motion. However, robust and reliable technical approaches are needed to uncover underlying structure in these data and to segment behavior into discrete hierarchically organized motifs. Here, we present an unsupervised probabilistic deep learning framework that identifies behavioral structure from deep variational embeddings of animal motion (VAME). By using a mouse model of beta amyloidosis as a use case, we show that VAME not only identifies discrete behavioral motifs, but also captures a hierarchical representation of the motif’s usage. The approach allows for the grouping of motifs into communities and the detection of differences in community-specific motif usage of individual mouse cohorts that were undetectable by human visual observation. Thus, we present a robust approach for the segmentation of animal motion that is applicable to a wide range of experimental setups, models and conditions without requiring supervised or a-priori human interference.},
	language = {en},
	number = {1},
	urldate = {2022-11-24},
	journal = {Communications Biology},
	author = {Luxem, Kevin and Mocellin, Petra and Fuhrmann, Falko and Kürsch, Johannes and Miller, Stephanie R. and Palop, Jorge J. and Remy, Stefan and Bauer, Pavol},
	month = nov,
	year = {2022},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Computational neuroscience, Behavioural methods},
	pages = {1--15},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/QG3NXHAU/Luxem et al. - 2022 - Identifying behavioral structure from deep variati.pdf:application/pdf},
}

@article{klibaite_deep_2022,
	title = {Deep phenotyping reveals movement phenotypes in mouse neurodevelopmental models},
	volume = {13},
	issn = {2040-2392},
	url = {https://doi.org/10.1186/s13229-022-00492-8},
	doi = {10.1186/s13229-022-00492-8},
	abstract = {Repetitive action, resistance to environmental change and fine motor disruptions are hallmarks of autism spectrum disorder (ASD) and other neurodevelopmental disorders, and vary considerably from individual to individual. In animal models, conventional behavioral phenotyping captures such fine-scale variations incompletely. Here we observed male and female C57BL/6J mice to methodically catalog adaptive movement over multiple days and examined two rodent models of developmental disorders against this dynamic baseline. We then investigated the behavioral consequences of a cerebellum-specific deletion in Tsc1 protein and a whole-brain knockout in Cntnap2 protein in mice. Both of these mutations are found in clinical conditions and have been associated with ASD.},
	language = {en},
	number = {1},
	urldate = {2022-11-24},
	journal = {Molecular Autism},
	author = {Klibaite, Ugne and Kislin, Mikhail and Verpeut, Jessica L. and Bergeler, Silke and Sun, Xiaoting and Shaevitz, Joshua W. and Wang, Samuel S.-H.},
	month = mar,
	year = {2022},
	keywords = {Cerebellum, Mouse, Behavior, Autism, Clustering, Pose estimation},
	pages = {12},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/ZVDNU3BL/Klibaite et al. - 2022 - Deep phenotyping reveals movement phenotypes in mo.pdf:application/pdf},
}

@article{graving_deepposekit_2019,
	title = {{DeepPoseKit}, a software toolkit for fast and robust animal pose estimation using deep learning},
	volume = {8},
	issn = {2050-084X},
	url = {https://doi.org/10.7554/eLife.47994},
	doi = {10.7554/eLife.47994},
	abstract = {Quantitative behavioral measurements are important for answering questions across scientific disciplines—from neuroscience to ecology. State-of-the-art deep-learning methods offer major advances in data quality and detail by allowing researchers to automatically estimate locations of an animal’s body parts directly from images or videos. However, currently available animal pose estimation methods have limitations in speed and robustness. Here, we introduce a new easy-to-use software toolkit, DeepPoseKit, that addresses these problems using an efficient multi-scale deep-learning model, called Stacked DenseNet, and a fast GPU-based peak-detection algorithm for estimating keypoint locations with subpixel precision. These advances improve processing speed {\textgreater}2x with no loss in accuracy compared to currently available methods. We demonstrate the versatility of our methods with multiple challenging animal pose estimation tasks in laboratory and field settings—including groups of interacting individuals. Our work reduces barriers to using advanced tools for measuring behavior and has broad applicability across the behavioral sciences.},
	urldate = {2022-11-24},
	journal = {eLife},
	author = {Graving, Jacob M and Chae, Daniel and Naik, Hemal and Li, Liang and Koger, Benjamin and Costelloe, Blair R and Couzin, Iain D},
	editor = {Baldwin, Ian T and Shaevitz, Josh W and Shaevitz, Josh W and Stephens, Greg},
	month = oct,
	year = {2019},
	note = {Publisher: eLife Sciences Publications, Ltd},
	keywords = {D. melanogaster, desert locust, Equus grevyi, Grévy's zebra, Schistocerca gregaria},
	pages = {e47994},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/8NNL8E33/Graving et al. - 2019 - DeepPoseKit, a software toolkit for fast and robus.pdf:application/pdf},
}

@article{mathis_primer_2020,
	title = {A {Primer} on {Motion} {Capture} with {Deep} {Learning}: {Principles}, {Pitfalls}, and {Perspectives}},
	volume = {108},
	issn = {0896-6273},
	shorttitle = {A {Primer} on {Motion} {Capture} with {Deep} {Learning}},
	url = {https://www.sciencedirect.com/science/article/pii/S0896627320307170},
	doi = {10.1016/j.neuron.2020.09.017},
	abstract = {Extracting behavioral measurements non-invasively from video is stymied by the fact that it is a hard computational problem. Recent advances in deep learning have tremendously advanced our ability to predict posture directly from videos, which has quickly impacted neuroscience and biology more broadly. In this primer, we review the budding field of motion capture with deep learning. In particular, we will discuss the principles of those novel algorithms, highlight their potential as well as pitfalls for experimentalists, and provide a glimpse into the future.},
	language = {en},
	number = {1},
	urldate = {2022-11-24},
	journal = {Neuron},
	author = {Mathis, Alexander and Schneider, Steffen and Lauer, Jessy and Mathis, Mackenzie Weygandt},
	month = oct,
	year = {2020},
	pages = {44--65},
	file = {ScienceDirect Full Text PDF:/Users/nsirmpilatze/Zotero/storage/FCQKPT37/Mathis et al. - 2020 - A Primer on Motion Capture with Deep Learning Pri.pdf:application/pdf;ScienceDirect Snapshot:/Users/nsirmpilatze/Zotero/storage/K8WDCKX4/S0896627320307170.html:text/html},
}

@article{karashchuk_anipose_2021,
	title = {Anipose: {A} toolkit for robust markerless {3D} pose estimation},
	volume = {36},
	issn = {2211-1247},
	shorttitle = {Anipose},
	url = {https://www.sciencedirect.com/science/article/pii/S2211124721011797},
	doi = {10.1016/j.celrep.2021.109730},
	abstract = {Quantifying movement is critical for understanding animal behavior. Advances in computer vision now enable markerless tracking from 2D video, but most animals move in 3D. Here, we introduce Anipose, an open-source toolkit for robust markerless 3D pose estimation. Anipose is built on the 2D tracking method DeepLabCut, so users can expand their existing experimental setups to obtain accurate 3D tracking. It consists of four components: (1) a 3D calibration module, (2) filters to resolve 2D tracking errors, (3) a triangulation module that integrates temporal and spatial regularization, and (4) a pipeline to structure processing of large numbers of videos. We evaluate Anipose on a calibration board as well as mice, flies, and humans. By analyzing 3D leg kinematics tracked with Anipose, we identify a key role for joint rotation in motor control of fly walking. To help users get started with 3D tracking, we provide tutorials and documentation at http://anipose.org/.},
	language = {en},
	number = {13},
	urldate = {2022-11-24},
	journal = {Cell Reports},
	author = {Karashchuk, Pierre and Rupp, Katie L. and Dickinson, Evyn S. and Walling-Bell, Sarah and Sanders, Elischa and Azim, Eiman and Brunton, Bingni W. and Tuthill, John C.},
	month = sep,
	year = {2021},
	keywords = {neuroscience, 3D, behavior, camera calibration, deep learning, Drosophila joint rotation, markerless tracking, pose estimation, robust tracking, visualization},
	pages = {109730},
}

@article{arac_deepbehavior_2019,
	title = {{DeepBehavior}: {A} {Deep} {Learning} {Toolbox} for {Automated} {Analysis} of {Animal} and {Human} {Behavior} {Imaging} {Data}},
	volume = {13},
	issn = {1662-5137},
	shorttitle = {{DeepBehavior}},
	url = {https://www.frontiersin.org/articles/10.3389/fnsys.2019.00020},
	abstract = {Detailed behavioral analysis is key to understanding the brain-behavior relationship. Here, we present deep learning-based methods for analysis of behavior imaging data in mice and humans. Specifically, we use three different convolutional neural network architectures and five different behavior tasks in mice and humans and provide detailed instructions for rapid implementation of these methods for the neuroscience community. We provide examples of three dimensional (3D) kinematic analysis in the food pellet reaching task in mice, three-chamber test in mice, social interaction test in freely moving mice with simultaneous miniscope calcium imaging, and 3D kinematic analysis of two upper extremity movements in humans (reaching and alternating pronation/supination). We demonstrate that the transfer learning approach accelerates the training of the network when using images from these types of behavior video recordings. We also provide code for post-processing of the data after initial analysis with deep learning. Our methods expand the repertoire of available tools using deep learning for behavior analysis by providing detailed instructions on implementation, applications in several behavior tests, and post-processing methods and annotated code for detailed behavior analysis. Moreover, our methods in human motor behavior can be used in the clinic to assess motor function during recovery after an injury such as stroke.},
	urldate = {2022-11-24},
	journal = {Frontiers in Systems Neuroscience},
	author = {Arac, Ahmet and Zhao, Pingping and Dobkin, Bruce H. and Carmichael, S. Thomas and Golshani, Peyman},
	year = {2019},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/77U92TAS/Arac et al. - 2019 - DeepBehavior A Deep Learning Toolbox for Automate.pdf:application/pdf},
}

@article{liu_optiflex_2021,
	title = {{OptiFlex}: {Multi}-{Frame} {Animal} {Pose} {Estimation} {Combining} {Deep} {Learning} {With} {Optical} {Flow}},
	volume = {15},
	issn = {1662-5102},
	shorttitle = {{OptiFlex}},
	url = {https://www.frontiersin.org/articles/10.3389/fncel.2021.621252},
	abstract = {Animal pose estimation tools based on deep learning have greatly improved animal behaviour quantification. These tools perform pose estimation on individual video frames, but do not account for variability of animal body shape in their prediction and evaluation. Here, we introduce a novel multi-frame animal pose estimation framework, referred to as OptiFlex. This framework integrates a flexible base model (i.e., FlexibleBaseline), which accounts for variability in animal body shape, with an OpticalFlow model that incorporates temporal context from nearby video frames. Pose estimation can be optimised using multi-view information to leverage all four dimensions (3D space and time). We evaluate FlexibleBaseline using datasets of four different lab animal species (mouse, fruit fly, zebrafish, and monkey) and introduce an intuitive evaluation metric—adjusted percentage of correct key points (aPCK). Our analyses show that OptiFlex provides prediction accuracy that outperforms current deep learning based tools, highlighting its potential for studying a wide range of behaviours across different animal species.},
	urldate = {2022-11-24},
	journal = {Frontiers in Cellular Neuroscience},
	author = {Liu, XiaoLe and Yu, Si-yang and Flierman, Nico A. and Loyola, Sebastián and Kamermans, Maarten and Hoogland, Tycho M. and De Zeeuw, Chris I.},
	year = {2021},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/H2LGZMAM/Liu et al. - 2021 - OptiFlex Multi-Frame Animal Pose Estimation Combi.pdf:application/pdf},
}

@article{luxem_identifying_2022-2,
	title = {Identifying behavioral structure from deep variational embeddings of animal motion},
	volume = {5},
	copyright = {2022 The Author(s)},
	issn = {2399-3642},
	url = {https://www.nature.com/articles/s42003-022-04080-7},
	doi = {10.1038/s42003-022-04080-7},
	abstract = {Quantification and detection of the hierarchical organization of behavior is a major challenge in neuroscience. Recent advances in markerless pose estimation enable the visualization of high-dimensional spatiotemporal behavioral dynamics of animal motion. However, robust and reliable technical approaches are needed to uncover underlying structure in these data and to segment behavior into discrete hierarchically organized motifs. Here, we present an unsupervised probabilistic deep learning framework that identifies behavioral structure from deep variational embeddings of animal motion (VAME). By using a mouse model of beta amyloidosis as a use case, we show that VAME not only identifies discrete behavioral motifs, but also captures a hierarchical representation of the motif’s usage. The approach allows for the grouping of motifs into communities and the detection of differences in community-specific motif usage of individual mouse cohorts that were undetectable by human visual observation. Thus, we present a robust approach for the segmentation of animal motion that is applicable to a wide range of experimental setups, models and conditions without requiring supervised or a-priori human interference.},
	language = {en},
	number = {1},
	urldate = {2023-01-18},
	journal = {Communications Biology},
	author = {Luxem, Kevin and Mocellin, Petra and Fuhrmann, Falko and Kürsch, Johannes and Miller, Stephanie R. and Palop, Jorge J. and Remy, Stefan and Bauer, Pavol},
	month = nov,
	year = {2022},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Computational neuroscience, Behavioural methods},
	pages = {1--15},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/659KBEJK/Luxem et al. - 2022 - Identifying behavioral structure from deep variati.pdf:application/pdf},
}

@article{roy_extracting_2021,
	title = {Extracting the dynamics of behavior in sensory decision-making experiments},
	volume = {109},
	issn = {0896-6273},
	url = {https://www.sciencedirect.com/science/article/pii/S0896627320309636},
	doi = {10.1016/j.neuron.2020.12.004},
	abstract = {Decision-making strategies evolve during training and can continue to vary even in well-trained animals. However, studies of sensory decision-making tend to characterize behavior in terms of a fixed psychometric function that is fit only after training is complete. Here, we present PsyTrack, a flexible method for inferring the trajectory of sensory decision-making strategies from choice data. We apply PsyTrack to training data from mice, rats, and human subjects learning to perform auditory and visual decision-making tasks. We show that it successfully captures trial-to-trial fluctuations in the weighting of sensory stimuli, bias, and task-irrelevant covariates such as choice and stimulus history. This analysis reveals dramatic differences in learning across mice and rapid adaptation to changes in task statistics. PsyTrack scales easily to large datasets and offers a powerful tool for quantifying time-varying behavior in a wide variety of animals and tasks.},
	language = {en},
	number = {4},
	urldate = {2022-12-15},
	journal = {Neuron},
	author = {Roy, Nicholas A. and Bak, Ji Hyun and Akrami, Athena and Brody, Carlos D. and Pillow, Jonathan W.},
	month = feb,
	year = {2021},
	keywords = {learning, behavioral dynamics, psychophysics, sensory decision making},
	pages = {597--610.e6},
	file = {ScienceDirect Full Text PDF:/Users/nsirmpilatze/Zotero/storage/4FEPXIWY/Roy et al. - 2021 - Extracting the dynamics of behavior in sensory dec.pdf:application/pdf;ScienceDirect Snapshot:/Users/nsirmpilatze/Zotero/storage/H3HF5G79/S0896627320309636.html:text/html},
}

@article{ashwood_mice_2022,
	title = {Mice alternate between discrete strategies during perceptual decision-making},
	volume = {25},
	copyright = {2022 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {1546-1726},
	url = {https://www.nature.com/articles/s41593-021-01007-z},
	doi = {10.1038/s41593-021-01007-z},
	abstract = {Classical models of perceptual decision-making assume that subjects use a single, consistent strategy to form decisions, or that decision-making strategies evolve slowly over time. Here we present new analyses suggesting that this common view is incorrect. We analyzed data from mouse and human decision-making experiments and found that choice behavior relies on an interplay among multiple interleaved strategies. These strategies, characterized by states in a hidden Markov model, persist for tens to hundreds of trials before switching, and often switch multiple times within a session. The identified decision-making strategies were highly consistent across mice and comprised a single ‘engaged’ state, in which decisions relied heavily on the sensory stimulus, and several biased states in which errors frequently occurred. These results provide a powerful alternate explanation for ‘lapses’ often observed in rodent behavioral experiments, and suggest that standard measures of performance mask the presence of major changes in strategy across trials.},
	language = {en},
	number = {2},
	urldate = {2022-12-15},
	journal = {Nature Neuroscience},
	author = {Ashwood, Zoe C. and Roy, Nicholas A. and Stone, Iris R. and Urai, Anne E. and Churchland, Anne K. and Pouget, Alexandre and Pillow, Jonathan W.},
	month = feb,
	year = {2022},
	note = {Number: 2
Publisher: Nature Publishing Group},
	keywords = {Decision, Computational neuroscience},
	pages = {201--212},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/L6HWS5ND/Ashwood et al. - 2022 - Mice alternate between discrete strategies during .pdf:application/pdf},
}

@article{hu_labgym_2023,
	title = {{LabGym}: {Quantification} of user-defined animal behaviors using learning-based holistic assessment},
	volume = {0},
	issn = {2667-2375},
	shorttitle = {{LabGym}},
	url = {https://www.cell.com/cell-reports-methods/abstract/S2667-2375(23)00026-7},
	doi = {10.1016/j.crmeth.2023.100415},
	language = {English},
	number = {0},
	urldate = {2023-02-27},
	journal = {Cell Reports Methods},
	author = {Hu, Yujia and Ferrario, Carrie R. and Maitland, Alexander D. and Ionides, Rita B. and Ghimire, Anjesh and Watson, Brendon and Iwasaki, Kenichi and White, Hope and Xi, Yitao and Zhou, Jie and Ye, Bing},
	month = feb,
	year = {2023},
	note = {Publisher: Elsevier},
	keywords = {mouse, rat, behavior, deep learning, automatic categorization, automatic quantification, CP: Neuroscience, deep neural network, Drosophila, machine learning},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/A6EEPS6D/Hu et al. - 2023 - LabGym Quantification of user-defined animal beha.pdf:application/pdf},
}

@article{pereira_quantifying_2020,
	title = {Quantifying behavior to understand the brain},
	volume = {23},
	copyright = {2020 Springer Nature America, Inc.},
	issn = {1546-1726},
	url = {https://www.nature.com/articles/s41593-020-00734-z},
	doi = {10.1038/s41593-020-00734-z},
	abstract = {Over the past years, numerous methods have emerged to automate the quantification of animal behavior at a resolution not previously imaginable. This has opened up a new field of computational ethology and will, in the near future, make it possible to quantify in near completeness what an animal is doing as it navigates its environment. The importance of improving the techniques with which we characterize behavior is reflected in the emerging recognition that understanding behavior is an essential (or even prerequisite) step to pursuing neuroscience questions. The use of these methods, however, is not limited to studying behavior in the wild or in strictly ethological settings. Modern tools for behavioral quantification can be applied to the full gamut of approaches that have historically been used to link brain to behavior, from psychophysics to cognitive tasks, augmenting those measurements with rich descriptions of how animals navigate those tasks. Here we review recent technical advances in quantifying behavior, particularly in methods for tracking animal motion and characterizing the structure of those dynamics. We discuss open challenges that remain for behavioral quantification and highlight promising future directions, with a strong emphasis on emerging approaches in deep learning, the core technology that has enabled the markedly rapid pace of progress of this field. We then discuss how quantitative descriptions of behavior can be leveraged to connect brain activity with animal movements, with the ultimate goal of resolving the relationship between neural circuits, cognitive processes and behavior.},
	language = {en},
	number = {12},
	urldate = {2023-03-14},
	journal = {Nature Neuroscience},
	author = {Pereira, Talmo D. and Shaevitz, Joshua W. and Murthy, Mala},
	month = dec,
	year = {2020},
	note = {Number: 12
Publisher: Nature Publishing Group},
	keywords = {Neuroscience, Sensorimotor processing},
	pages = {1537--1549},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/IM2VLNEM/Pereira et al. - 2020 - Quantifying behavior to understand the brain.pdf:application/pdf},
}

@misc{han_social_2023,
	title = {Social {Behavior} {Atlas}: {A} computational framework for tracking and mapping {3D} close interactions of free-moving animals},
	copyright = {© 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	shorttitle = {Social {Behavior} {Atlas}},
	url = {https://www.biorxiv.org/content/10.1101/2023.03.05.531235v1},
	doi = {10.1101/2023.03.05.531235},
	abstract = {The study of social behaviors in animals is essential for understanding their survival and reproductive strategies. However, accurately tracking and analyzing the social interactions of free-moving animals has remained a challenge. Existing multi-animal pose estimation techniques suffer from drawbacks such as the need for extensive manual annotation and difficulty in discriminating between similar-looking animals in close social interactions. In this paper, we present the Social Behavior Atlas (SBeA), a novel computational framework that solves these challenges by employing a deep learning-based video instance segmentation model, 3D pose reconstruction, and unsupervised dynamic behavioral clustering. SBeA framework also involves a multi-camera setup to prevent occlusion, and a novel approach to identify individual animals in close social interactions. We demonstrate the effectiveness of SBeA in tracking and mapping the 3D close interactions of free-moving animals using the example of genetic mutant mice, birds, and dogs. Our results show that SBeA is capable of identifying subtle social interaction abnormalities, and the models and frameworks developed can be applied to a wide range of animal species. SBeA is a powerful tool for researchers in the fields of neuroscience and ecology to study animal social behaviors with a high degree of accuracy and reliability.},
	language = {en},
	urldate = {2023-03-07},
	publisher = {bioRxiv},
	author = {Han, Yaning and Chen, Ke and Wang, Yunke and Liu, Wenhao and Wang, Xiaojing and Liao, Jiahui and Huang, Yiting and Han, Chuanliang and Huang, Kang and Zhang, Jiajia and Cai, Shengyuan and Wang, Zhouwei and Wu, Yongji and Gao, Gao and Wang, Nan and Li, Jinxiu and Song, Yangwangzi and Li, Jing and Wang, Guodong and Wang, Liping and Zhang, Yaping and Wei, Pengfei},
	month = mar,
	year = {2023},
	note = {Pages: 2023.03.05.531235
Section: New Results},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/4S9HN5VL/Han et al. - 2023 - Social Behavior Atlas A computational framework f.pdf:application/pdf},
}

@article{luxem_open-source_2023,
	title = {Open-source tools for behavioral video analysis: {Setup}, methods, and best practices},
	volume = {12},
	issn = {2050-084X},
	shorttitle = {Open-source tools for behavioral video analysis},
	url = {https://doi.org/10.7554/eLife.79305},
	doi = {10.7554/eLife.79305},
	abstract = {Recently developed methods for video analysis, especially models for pose estimation and behavior classification, are transforming behavioral quantification to be more precise, scalable, and reproducible in fields such as neuroscience and ethology. These tools overcome long-standing limitations of manual scoring of video frames and traditional ‘center of mass’ tracking algorithms to enable video analysis at scale. The expansion of open-source tools for video acquisition and analysis has led to new experimental approaches to understand behavior. Here, we review currently available open-source tools for video analysis and discuss how to set up these methods for labs new to video recording. We also discuss best practices for developing and using video analysis methods, including community-wide standards and critical needs for the open sharing of datasets and code, more widespread comparisons of video analysis methods, and better documentation for these methods especially for new users. We encourage broader adoption and continued development of these tools, which have tremendous potential for accelerating scientific progress in understanding the brain and behavior.},
	urldate = {2023-03-29},
	journal = {eLife},
	author = {Luxem, Kevin and Sun, Jennifer J and Bradley, Sean P and Krishnan, Keerthi and Yttri, Eric and Zimmermann, Jan and Pereira, Talmo D and Laubach, Mark},
	editor = {Cai, Denise J and Colgin, Laura L},
	month = mar,
	year = {2023},
	note = {Publisher: eLife Sciences Publications, Ltd},
	keywords = {reproducibility, open source, behavior, pose estimation, methods, video},
	pages = {e79305},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/E9TQH8BP/Luxem et al. - 2023 - Open-source tools for behavioral video analysis S.pdf:application/pdf},
}

@misc{dutta_robust_2023,
	title = {A robust and flexible deep-learning workflow for animal tracking},
	copyright = {© 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial 4.0 International), CC BY-NC 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2023.04.20.537633v1},
	doi = {10.1101/2023.04.20.537633},
	abstract = {Developments in automated animal tracking software are increasing the efficiency of data collection and improving the standardization of behavioural measurements. There are now several open-source tools for tracking laboratory animals, but often these are only accurate under limited conditions (e.g. uniform lighting and background, uncluttered scenes, unobstructed focal animal). Tracking fish presents a particular challenge for these tools because movement at the water’s surface introduces significant noise. Partial occlusion of the focal animal can also be troublesome, particularly when tracking the whole organism. We conducted a behavioural experiment that required us to track the trajectory of a fish as it swam through a field of obstacles. In addition to measuring the body’s trajectory, we also needed to record the position of the obstacles, and to identify when the fish passed through the ‘virtual gates’ between adjacent obstacles and/or the aquarium wall. We automated data collection by employing a range of computer vision and computational geometry algorithms (e.g. object detection and tracking, optical flow, parallel plane homology mapping, Voronoi tessellation). Our workflow is divided into several discrete steps, and provides a set of modular software building blocks that can be adapted to analyse other experimental designs. A detailed tutorial is provided, together with all the data and code required to reproduce our results.},
	language = {en},
	urldate = {2023-04-27},
	publisher = {bioRxiv},
	author = {Dutta, Abhishek and Pérez-Campanero, Natalia and Taylor, Graham K. and Zisserman, Andrew and Newport, Cait},
	month = apr,
	year = {2023},
	note = {Pages: 2023.04.20.537633
Section: New Results},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/GP8LMCSJ/Dutta et al. - 2023 - A robust and flexible deep-learning workflow for a.pdf:application/pdf},
}

@misc{beane_jax_2023,
	title = {{JAX} {Animal} {Behavior} {System} ({JABS}): {A} video-based phenotyping platform for the laboratory mouse},
	copyright = {© 2023, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
	shorttitle = {{JAX} {Animal} {Behavior} {System} ({JABS})},
	url = {https://www.biorxiv.org/content/10.1101/2022.01.13.476229v2},
	doi = {10.1101/2022.01.13.476229},
	abstract = {Automated detection of complex animal behavior remains a challenge in neuroscience. Developments in computer-vision have greatly advanced automated behavior detection and allow high-throughput pre-clinical studies. An integrated hardware and software solution is necessary to facilitate the adoption of these advances in the field of behavioral neurogenetics, particularly for non-computational labs. We have published a series of papers using an open field arena to annotate complex behaviors such as grooming, posture, and gait as well as higher level constructs such as frailty. Here, we present an integrated rodent phenotyping platform, JAX Animal Behavior System (JABS) to the community for data acquisition, machine learning based behavior annotation and classification, classifier sharing, and genetic analysis. JABS Data acquisition module enables uniform data collection with its combination of 3D hardware designs and software for real-time monitoring and video data collection. JABS-Active Learning Module allows behavior annotation, classifier training, and validation. We also present a novel graph-based framework (ethograph) that enables efficient boutwise comparison of classifiers. JABS-Database Module allows users to share behavior classifiers and finally the JABS-Analysis Module infers a deposited classifier on a library of 600 open field videos consisting of 60 mouse strains, returns frame level and bout level classifier statistics.In summary, this open-source tool is an ecosystem that allows the neuroscience community to build shared resources for behavior analysis.},
	language = {en},
	urldate = {2023-07-03},
	publisher = {bioRxiv},
	author = {Beane, Glen and Geuther, Brian Q. and Sproule, Thomas J. and Choudhary, Anshul and Trapszo, Jarek and Hession, Leinani and Kohar, Vivek and Kumar, Vivek},
	month = feb,
	year = {2023},
	note = {Pages: 2022.01.13.476229
Section: New Results},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/A5XX3H7G/Beane et al. - 2023 - JAX Animal Behavior System (JABS) A video-based p.pdf:application/pdf},
}

@article{bordes_automatically_2023,
	title = {Automatically annotated motion tracking identifies a distinct social behavioral profile following chronic social defeat stress},
	volume = {14},
	copyright = {2023 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-023-40040-3},
	doi = {10.1038/s41467-023-40040-3},
	abstract = {Severe stress exposure increases the risk of stress-related disorders such as major depressive disorder (MDD). An essential characteristic of MDD is the impairment of social functioning and lack of social motivation. Chronic social defeat stress is an established animal model for MDD research, which induces a cascade of physiological and behavioral changes. Current markerless pose estimation tools allow for more complex and naturalistic behavioral tests. Here, we introduce the open-source tool DeepOF to investigate the individual and social behavioral profile in mice by providing supervised and unsupervised pipelines using DeepLabCut-annotated pose estimation data. Applying this tool to chronic social defeat in male mice, the DeepOF supervised and unsupervised pipelines detect a distinct stress-induced social behavioral pattern, which was particularly observed at the beginning of a novel social encounter and fades with time due to habituation. In addition, while the classical social avoidance task does identify the stress-induced social behavioral differences, both DeepOF behavioral pipelines provide a clearer and more detailed profile. Moreover, DeepOF aims to facilitate reproducibility and unification of behavioral classification by providing an open-source tool, which can advance the study of rodent individual and social behavior, thereby enabling biological insights and, for example, subsequent drug development for psychiatric disorders.},
	language = {en},
	number = {1},
	urldate = {2023-07-21},
	journal = {Nature Communications},
	author = {Bordes, Joeri and Miranda, Lucas and Reinhardt, Maya and Narayan, Sowmya and Hartmann, Jakob and Newman, Emily L. and Brix, Lea Maria and van Doeselaar, Lotte and Engelhardt, Clara and Dillmann, Larissa and Mitra, Shiladitya and Ressler, Kerry J. and Pütz, Benno and Agakov, Felix and Müller-Myhsok, Bertram and Schmidt, Mathias V.},
	month = jul,
	year = {2023},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Social behaviour, Preclinical research, Stress and resilience},
	pages = {4319},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/7P4DQIPB/Bordes et al. - 2023 - Automatically annotated motion tracking identifies.pdf:application/pdf},
}

@misc{segalin_mouse_2021,
	title = {The {Mouse} {Action} {Recognition} {System} ({MARS}) software pipeline for automated analysis of social behaviors in mice},
	copyright = {© 2021 Segalin et al.. This article is distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use and redistribution provided that the original author and source are credited.},
	url = {https://elifesciences.org/articles/63720/figures},
	abstract = {The Mouse Action Recognition System is a computational pipeline for automated classification of social behaviors in freely interacting mice, accompanied by a graphical interface for analysis of multimodal neuroscience datasets.},
	language = {en},
	urldate = {2023-07-25},
	journal = {eLife},
	author = {Segalin, Cristina and Williams, Jalani and Karigo, Tomomi and Hui, May and Zelikowsky, Moriel and Sun, Jennifer J. and Perona, Pietro and Anderson, David J. and Kennedy, Ann},
	month = nov,
	year = {2021},
	doi = {10.7554/eLife.63720},
	note = {Publisher: eLife Sciences Publications Limited},
	file = {Full Text:/Users/nsirmpilatze/Zotero/storage/PN3EE5IN/Segalin et al. - 2021 - The Mouse Action Recognition System (MARS) softwar.pdf:application/pdf;Snapshot:/Users/nsirmpilatze/Zotero/storage/HV2NVRBV/figures.html:text/html},
}

@misc{biderman_lightning_2023,
	title = {Lightning {Pose}: improved animal pose estimation via semi-supervised learning, {Bayesian} ensembling, and cloud-native open-source tools},
	copyright = {© 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NoDerivs 4.0 International), CC BY-ND 4.0, as described at http://creativecommons.org/licenses/by-nd/4.0/},
	shorttitle = {Lightning {Pose}},
	url = {https://www.biorxiv.org/content/10.1101/2023.04.28.538703v1},
	doi = {10.1101/2023.04.28.538703},
	abstract = {Pose estimation algorithms are shedding new light on animal behavior and intelligence. Most existing models are only trained with labeled frames (supervised learning). Although effective in many cases, the fully supervised approach requires extensive image labeling, struggles to generalize to new videos, and produces noisy outputs that hinder downstream analyses. We address each of these limitations with a semi-supervised approach that leverages the spatiotemporal statistics of unlabeled videos in two different ways. First, we introduce unsupervised training objectives that penalize the network whenever its predictions violate smoothness of physical motion, multiple-view geometry, or depart from a low-dimensional subspace of plausible body configurations. Second, we design a new network architecture that predicts pose for a given frame using temporal context from surrounding unlabeled frames. These context frames help resolve brief occlusions or ambiguities between nearby and similar-looking body parts. The resulting pose estimation networks achieve better performance with fewer labels, generalize better to unseen videos, and provide smoother and more reliable pose trajectories for downstream analysis; for example, these improved pose trajectories exhibit stronger correlations with neural activity. We also propose a Bayesian post-processing approach based on deep ensembling and Kalman smoothing that further improves tracking accuracy and robustness. We release a deep learning package that adheres to industry best practices, supporting easy model development and accelerated training and prediction. Our package is accompanied by a cloud application that allows users to annotate data, train networks, and predict new videos at scale, directly from the browser.},
	language = {en},
	urldate = {2023-09-13},
	publisher = {bioRxiv},
	author = {Biderman, Dan and Whiteway, Matthew R. and Hurwitz, Cole and Greenspan, Nicholas and Lee, Robert S. and Vishnubhotla, Ankit and Warren, Richard and Pedraja, Federico and Noone, Dillon and Schartner, Michael and Huntenburg, Julia M. and Khanal, Anup and Meijer, Guido T. and Noel, Jean-Paul and Pan-Vazquez, Alejandro and Socha, Karolina Z. and Urai, Anne E. and Laboratory, The International Brain and Cunningham, John P. and Sawtell, Nathaniel and Paninski, Liam},
	month = apr,
	year = {2023},
	note = {Pages: 2023.04.28.538703
Section: New Results},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/9T3JYJ82/Biderman et al. - 2023 - Lightning Pose improved animal pose estimation vi.pdf:application/pdf},
}

@inproceedings{wei_convolutional_2016,
	title = {Convolutional {Pose} {Machines}},
	url = {https://openaccess.thecvf.com/content_cvpr_2016/html/Wei_Convolutional_Pose_Machines_CVPR_2016_paper.html},
	urldate = {2023-10-16},
	author = {Wei, Shih-En and Ramakrishna, Varun and Kanade, Takeo and Sheikh, Yaser},
	year = {2016},
	pages = {4724--4732},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/HJ3MF4PJ/Wei et al. - 2016 - Convolutional Pose Machines.pdf:application/pdf},
}

@misc{newell_stacked_2016,
	title = {Stacked {Hourglass} {Networks} for {Human} {Pose} {Estimation}},
	url = {http://arxiv.org/abs/1603.06937},
	doi = {10.48550/arXiv.1603.06937},
	abstract = {This work introduces a novel convolutional network architecture for the task of human pose estimation. Features are processed across all scales and consolidated to best capture the various spatial relationships associated with the body. We show how repeated bottom-up, top-down processing used in conjunction with intermediate supervision is critical to improving the performance of the network. We refer to the architecture as a "stacked hourglass" network based on the successive steps of pooling and upsampling that are done to produce a final set of predictions. State-of-the-art results are achieved on the FLIC and MPII benchmarks outcompeting all recent methods.},
	urldate = {2023-10-16},
	publisher = {arXiv},
	author = {Newell, Alejandro and Yang, Kaiyu and Deng, Jia},
	month = jul,
	year = {2016},
	note = {arXiv:1603.06937 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/nsirmpilatze/Zotero/storage/FLWAFYDK/Newell et al. - 2016 - Stacked Hourglass Networks for Human Pose Estimati.pdf:application/pdf;arXiv.org Snapshot:/Users/nsirmpilatze/Zotero/storage/W6USY3WQ/1603.html:text/html},
}

@article{de_chaumont_real-time_2019,
	title = {Real-time analysis of the behaviour of groups of mice via a depth-sensing camera and machine learning},
	volume = {3},
	issn = {2157-846X},
	doi = {10.1038/s41551-019-0396-1},
	abstract = {Preclinical studies of psychiatric disorders use animal models to investigate the impact of environmental factors or genetic mutations on complex traits such as decision-making and social interactions. Here, we introduce a method for the real-time analysis of the behaviour of mice housed in groups of up to four over several days and in enriched environments. The method combines computer vision through a depth-sensing infrared camera, machine learning for animal and posture identification, and radio-frequency identification to monitor the quality of mouse tracking. It tracks multiple mice accurately, extracts a list of behavioural traits of both individuals and the groups of mice, and provides a phenotypic profile for each animal. We used the method to study the impact of Shank2 and Shank3 gene mutations-mutations that are associated with autism-on mouse behaviour. Characterization and integration of data from the behavioural profiles of Shank2 and Shank3 mutant female mice revealed their distinctive activity levels and involvement in complex social interactions.},
	language = {eng},
	number = {11},
	journal = {Nature Biomedical Engineering},
	author = {de Chaumont, Fabrice and Ey, Elodie and Torquet, Nicolas and Lagache, Thibault and Dallongeville, Stéphane and Imbert, Albane and Legou, Thierry and Le Sourd, Anne-Marie and Faure, Philippe and Bourgeron, Thomas and Olivo-Marin, Jean-Christophe},
	month = nov,
	year = {2019},
	pmid = {31110290},
	keywords = {Animals, Female, Male, Mice, Behavior, Animal, Video Recording, Machine Learning, Autistic Disorder, Behavioral Research, Disease Models, Animal, Mice, Knockout, Microfilament Proteins, Mutation, Nerve Tissue Proteins, Phenotype, Social Behavior},
	pages = {930--942},
	file = {Submitted Version:/Users/nsirmpilatze/Zotero/storage/3ABEDZCI/de Chaumont et al. - 2019 - Real-time analysis of the behaviour of groups of m.pdf:application/pdf},
}

@article{krakauer_neuroscience_2017,
	title = {Neuroscience {Needs} {Behavior}: {Correcting} a {Reductionist} {Bias}},
	volume = {93},
	issn = {0896-6273},
	shorttitle = {Neuroscience {Needs} {Behavior}},
	url = {https://www.sciencedirect.com/science/article/pii/S0896627316310406},
	doi = {10.1016/j.neuron.2016.12.041},
	abstract = {There are ever more compelling tools available for neuroscience research, ranging from selective genetic targeting to optogenetic circuit control to mapping whole connectomes. These approaches are coupled with a deep-seated, often tacit, belief in the reductionist program for understanding the link between the brain and behavior. The aim of this program is causal explanation through neural manipulations that allow testing of necessity and sufficiency claims. We argue, however, that another equally important approach seeks an alternative form of understanding through careful theoretical and experimental decomposition of behavior. Specifically, the detailed analysis of tasks and of the behavior they elicit is best suited for discovering component processes and their underlying algorithms. In most cases, we argue that study of the neural implementation of behavior is best investigated after such behavioral work. Thus, we advocate a more pluralistic notion of neuroscience when it comes to the brain-behavior relationship: behavioral work provides understanding, whereas neural interventions test causality.},
	number = {3},
	urldate = {2023-11-07},
	journal = {Neuron},
	author = {Krakauer, John W. and Ghazanfar, Asif A. and Gomez-Marin, Alex and MacIver, Malcolm A. and Poeppel, David},
	month = feb,
	year = {2017},
	pages = {480--490},
	file = {ScienceDirect Full Text PDF:/Users/nsirmpilatze/Zotero/storage/GBA5NJJ9/Krakauer et al. - 2017 - Neuroscience Needs Behavior Correcting a Reductio.pdf:application/pdf},
}

@misc{cao_observation-centric_2023,
	title = {Observation-{Centric} {SORT}: {Rethinking} {SORT} for {Robust} {Multi}-{Object} {Tracking}},
	shorttitle = {Observation-{Centric} {SORT}},
	url = {http://arxiv.org/abs/2203.14360},
	abstract = {Kalman filter (KF) based methods for multi-object tracking (MOT) make an assumption that objects move linearly. While this assumption is acceptable for very short periods of occlusion, linear estimates of motion for prolonged time can be highly inaccurate. Moreover, when there is no measurement available to update Kalman filter parameters, the standard convention is to trust the priori state estimations for posteriori update. This leads to the accumulation of errors during a period of occlusion. The error causes significant motion direction variance in practice. In this work, we show that a basic Kalman filter can still obtain state-of-the-art tracking performance if proper care is taken to fix the noise accumulated during occlusion. Instead of relying only on the linear state estimate (i.e., estimation-centric approach), we use object observations (i.e., the measurements by object detector) to compute a virtual trajectory over the occlusion period to fix the error accumulation of filter parameters during the occlusion period. This allows more time steps to correct errors accumulated during occlusion. We name our method Observation-Centric SORT (OC-SORT). It remains Simple, Online, and Real-Time but improves robustness during occlusion and non-linear motion. Given off-the-shelf detections as input, OC-SORT runs at 700+ FPS on a single CPU. It achieves state-of-the-art on multiple datasets, including MOT17, MOT20, KITTI, head tracking, and especially DanceTrack where the object motion is highly non-linear. The code and models are available at {\textbackslash}url\{https://github.com/noahcao/OC\_SORT\}.},
	urldate = {2023-11-13},
	publisher = {arXiv},
	author = {Cao, Jinkun and Pang, Jiangmiao and Weng, Xinshuo and Khirodkar, Rawal and Kitani, Kris},
	month = mar,
	year = {2023},
	note = {arXiv:2203.14360 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/nsirmpilatze/Zotero/storage/R5MJFCHZ/2203.html:text/html;Full Text PDF:/Users/nsirmpilatze/Zotero/storage/43VXCHCZ/Cao et al. - 2023 - Observation-Centric SORT Rethinking SORT for Robu.pdf:application/pdf},
}

@misc{ronchi_benchmarking_2017,
	title = {Benchmarking and {Error} {Diagnosis} in {Multi}-{Instance} {Pose} {Estimation}},
	url = {http://arxiv.org/abs/1707.05388},
	doi = {10.48550/arXiv.1707.05388},
	abstract = {We propose a new method to analyze the impact of errors in algorithms for multi-instance pose estimation and a principled benchmark that can be used to compare them. We define and characterize three classes of errors - localization, scoring, and background - study how they are influenced by instance attributes and their impact on an algorithm's performance. Our technique is applied to compare the two leading methods for human pose estimation on the COCO Dataset, measure the sensitivity of pose estimation with respect to instance size, type and number of visible keypoints, clutter due to multiple instances, and the relative score of instances. The performance of algorithms, and the types of error they make, are highly dependent on all these variables, but mostly on the number of keypoints and the clutter. The analysis and software tools we propose offer a novel and insightful approach for understanding the behavior of pose estimation algorithms and an effective method for measuring their strengths and weaknesses.},
	urldate = {2023-11-24},
	publisher = {arXiv},
	author = {Ronchi, Matteo Ruggero and Perona, Pietro},
	month = aug,
	year = {2017},
	note = {arXiv:1707.05388 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/nsirmpilatze/Zotero/storage/BGA8TI3P/Ronchi and Perona - 2017 - Benchmarking and Error Diagnosis in Multi-Instance.pdf:application/pdf;arXiv.org Snapshot:/Users/nsirmpilatze/Zotero/storage/XY3UYACJ/1707.html:text/html},
}

@article{graser_movingpandas_2019,
	title = {{MovingPandas}: {Efficient} {Structures} for {Movement} {Data} in {Python}},
	volume = {Volume 7,},
	copyright = {Österreichische Akademie der Wissenschaften},
	issn = {2308-1708},
	shorttitle = {{MovingPandas}},
	url = {https://www.austriaca.at?arp=0x003aba2b},
	doi = {10.1553/giscience2019_01_s54},
	abstract = {Movement data analysis is a high-interest topic in many scientific domains. Even though Python is the scripting language of choice in the GIS world, currently there is no Python library that would enable researchers and practitioners to interact with and analyse movement data efficiently. To close this gap, we present MovingPandas, a new Python library for dealing with movement data. Its development is based on an analysis of state-of-the-art conceptual frameworks and existing implementations (in PostGIS, Hermes, and the R package trajectories). We describe how MovingPandas avoids limitations of Simple Feature-based movement data models commonly used to handle trajectories in the GIS world. Finally, we present the current state of the MovingPandas implementation and demonstrate its use in stand-alone Python scripts, as well as within the context of the desktop GIS application QGIS. This work represents the first step towards a general-purpose Python library that enables researchers and practitioners in the GIS field and beyond to handle and analyse movement data more efficiently},
	language = {de},
	urldate = {2023-12-11},
	journal = {GI\_Forum 2019,},
	author = {Graser, Anita},
	month = jun,
	year = {2019},
	note = {Publisher: Verlag der Österreichischen Akademie der Wissenschaften},
	pages = {54--68},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/WRV9CS78/Graser - 2019 - MovingPandas Efficient Structures for Movement Da.pdf:application/pdf},
}

@misc{mathis_dlc2kinematics_2020,
	title = {{DLC2Kinematics}: a post-deeplabcut module for kinematic analysis},
	shorttitle = {{DLC2Kinematics}},
	url = {https://zenodo.org/records/6669074},
	abstract = {Kinematic analysis is crucial in biomedical, biomechanical, life sciences and medicine. Here, we present a python toolbox for analysis of markerless motion capture data collected with DeepLabCut. This toolbox represents the contributions of members of the Mathis Lab of Adaptive Motor Control from 2017-2022. Please see https://github.com/AdaptiveMotorControlLab/DLC2Kinematics for up to date versions. We kindly ask that if you use this code you cite the software.},
	urldate = {2023-12-15},
	publisher = {Zenodo},
	author = {Mathis, Mackenzie and Lauer, Jessy and Nath, Tanmay and Beauzile, Michael and Hausmann, Sébastien and Schneider, Steffen and Mathis, Alexander},
	month = feb,
	year = {2020},
	doi = {10.5281/zenodo.6669074},
	file = {Zenodo Snapshot:/Users/nsirmpilatze/Zotero/storage/X2KIRR6G/6669074.html:text/html},
}

@article{pappalardo_scikit-mobility_2022,
	title = {scikit-mobility: {A} {Python} {Library} for the {Analysis}, {Generation}, and {Risk} {Assessment} of {Mobility} {Data}},
	shorttitle = {scikit-mobility},
	url = {https://www.jstatsoft.org/article/view/v103i04},
	abstract = {The last decade has witnessed the emergence of massive mobility datasets, such as tracks generated by GPS devices, call detail records, and geo-tagged posts from social media platforms. These datasets have fostered a vast scientific production on various applications of mobility analysis, ranging from computational epidemiology to urban planning and transportation engineering. A strand of literature addresses data cleaning issues related to raw spatiotemporal trajectories, while the second line of research focuses on discovering the statistical "laws" that govern human movements. A significant effort has also been put on designing algorithms to generate synthetic trajectories able to reproduce, realistically, the laws of human mobility. Last but not least, a line of research addresses the crucial problem of privacy, proposing techniques to perform the re-identification of individuals in a database. A view on state-of-the-art cannot avoid noticing that there is no statistical software that can support scientists and practitioners with all the aspects mentioned above of mobility data analysis. In this paper, we propose scikit-mobility, a Python library that has the ambition of providing an environment to reproduce existing research, analyze mobility data, and simulate human mobility habits. scikit-mobility is efficient and easy to use as it extends pandas, a popular Python library for data analysis. Moreover, scikit-mobility provides the user with many functionalities, from visualizing trajectories to generating synthetic data, from analyzing statistical patterns to assessing the privacy risk related to the analysis of mobility datasets.},
	language = {en-US},
	urldate = {2023-12-15},
	author = {Pappalardo, Luca and Simini, Filippo and Barlacchi, Gianni and Pellungrini, Roberto},
	month = jul,
	year = {2022},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/II97TTPZ/2022 - scikit-mobility A Python Library for the Analysis.pdf:application/pdf},
}

@article{ho_fully_2023,
	title = {A fully automated home cage for long-term continuous phenotyping of mouse cognition and behavior},
	volume = {3},
	issn = {2667-2375},
	url = {https://www.sciencedirect.com/science/article/pii/S2667237523001686},
	doi = {10.1016/j.crmeth.2023.100532},
	abstract = {Automated home-cage monitoring systems present a valuable tool for comprehensive phenotyping of natural behaviors. However, current systems often involve complex training routines, water or food restriction, and probe a limited range of behaviors. Here, we present a fully automated home-cage monitoring system for cognitive and behavioral phenotyping in mice. The system incorporates T-maze alternation, novel object recognition, and object-in-place recognition tests combined with monitoring of locomotion, drinking, and quiescence patterns, all carried out over long periods. Mice learn the tasks rapidly without any need for water or food restrictions. Behavioral characterization employs a deep convolutional neural network image analysis. We show that combined statistical properties of multiple behaviors can be used to discriminate between mice with hippocampal, medial entorhinal, and sham lesions and predict the genotype of an Alzheimer’s disease mouse model with high accuracy. This technology may enable large-scale behavioral screening for genes and neural circuits underlying spatial memory and other cognitive processes.},
	number = {7},
	urldate = {2024-01-04},
	journal = {Cell Reports Methods},
	author = {Ho, Hinze and Kejzar, Nejc and Sasaguri, Hiroki and Saito, Takashi and Saido, Takaomi C. and De Strooper, Bart and Bauza, Marius and Krupic, Julija},
	month = jul,
	year = {2023},
	keywords = {Alzheimer's disease, machine learning, automated classification, home-cage system, longitudinal monitoring, mouse behavioural cognitive testing},
	pages = {100532},
	file = {Full Text:/Users/nsirmpilatze/Zotero/storage/SZAXMA7F/Ho et al. - 2023 - A fully automated home cage for long-term continuo.pdf:application/pdf},
}

@article{desai_openapepose_2023,
	title = {{OpenApePose}, a database of annotated ape photographs for pose estimation},
	volume = {12},
	issn = {2050-084X},
	url = {https://doi.org/10.7554/eLife.86873},
	doi = {10.7554/eLife.86873},
	abstract = {Because of their close relationship with humans, non-human apes (chimpanzees, bonobos, gorillas, orangutans, and gibbons, including siamangs) are of great scientific interest. The goal of understanding their complex behavior would be greatly advanced by the ability to perform video-based pose tracking. Tracking, however, requires high-quality annotated datasets of ape photographs. Here we present OpenApePose, a new public dataset of 71,868 photographs, annotated with 16 body landmarks of six ape species in naturalistic contexts. We show that a standard deep net (HRNet-W48) trained on ape photos can reliably track out-of-sample ape photos better than networks trained on monkeys (specifically, the OpenMonkeyPose dataset) and on humans (COCO) can. This trained network can track apes almost as well as the other networks can track their respective taxa, and models trained without one of the six ape species can track the held-out species better than the monkey and human models can. Ultimately, the results of our analyses highlight the importance of large, specialized databases for animal tracking systems and confirm the utility of our new ape database.},
	urldate = {2024-01-04},
	journal = {eLife},
	author = {Desai, Nisarg and Bala, Praneet and Richardson, Rebecca and Raper, Jessica and Zimmermann, Jan and Hayden, Benjamin},
	editor = {Kalan, Ammie K and Perry, George H},
	month = dec,
	year = {2023},
	note = {Publisher: eLife Sciences Publications, Ltd},
	keywords = {deep learning, pose estimation, apes, behavior tracking, dataset},
	pages = {RP86873},
	file = {Submitted Version:/Users/nsirmpilatze/Zotero/storage/YSXSBZA5/Desai et al. - 2023 - OpenApePose, a database of annotated ape photograp.pdf:application/pdf},
}

@misc{carandini_sensory_2024,
	title = {Sensory choices as logistic classification},
	copyright = {© 2024, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2024.01.17.576029v1},
	doi = {10.1101/2024.01.17.576029},
	abstract = {Logistic classification is a simple way to make choices based on a set of factors: give each factor a weight, sum them, and use the result to set the log odds of a random draw. This operation is known to describe human and animal choices based on value (economic decisions). There is increasing evidence that it also describes choices based on sensory inputs (perceptual decisions). Here I briefly review this evidence and I fit data from multiple studies in multiple species to show that logistic classification can describe a variety of choices. These include sensory choices influenced by stimuli of other modalities (multisensory integration) or by non-sensory factors such as value and recent history. Logistic classification is the optimal strategy if factors are independent of each other, and a useful heuristic in other conditions. Using it to describe sensory choices is useful to characterize brain function and the effects of brain inactivations.},
	language = {en},
	urldate = {2024-01-20},
	publisher = {bioRxiv},
	author = {Carandini, Matteo},
	month = jan,
	year = {2024},
	note = {Pages: 2024.01.17.576029
Section: New Results},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/27Y5VDH3/Carandini - 2024 - Sensory choices as logistic classification.pdf:application/pdf},
}

@article{li_improved_2023,
	title = {Improved {3D} {Markerless} {Mouse} {Pose} {Estimation} {Using} {Temporal} {Semi}-supervision},
	volume = {131},
	issn = {1573-1405},
	url = {https://doi.org/10.1007/s11263-023-01756-3},
	doi = {10.1007/s11263-023-01756-3},
	abstract = {Three-dimensional markerless pose estimation from multi-view video is emerging as an exciting method for quantifying the behavior of freely moving animals. Nevertheless, scientifically precise 3D animal pose estimation remains challenging, primarily due to a lack of large training and benchmark datasets and the immaturity of algorithms tailored to the demands of animal experiments and body plans. Existing techniques employ fully supervised convolutional neural networks (CNNs) trained to predict body keypoints in individual video frames, but this demands a large collection of labeled training samples to achieve desirable 3D tracking performance. Here, we introduce a semi-supervised learning strategy that incorporates unlabeled video frames via a simple temporal constraint applied during training. In freely moving mice, our new approach improves the current state-of-the-art performance of multi-view volumetric 3D pose estimation and further enhances the temporal stability and skeletal consistency of 3D tracking.},
	language = {en},
	number = {6},
	urldate = {2024-02-12},
	journal = {International Journal of Computer Vision},
	author = {Li, Tianqing and Severson, Kyle S. and Wang, Fan and Dunn, Timothy W.},
	month = jun,
	year = {2023},
	keywords = {3D pose estimation, Animal behavioral tracking, Markerless animal tracking, Semi-supervised learning},
	pages = {1389--1405},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/GWQJ2LCY/Li et al. - 2023 - Improved 3D Markerless Mouse Pose Estimation Using.pdf:application/pdf},
}

@article{dunn_geometric_2021,
	title = {Geometric deep learning enables {3D} kinematic profiling across species and environments},
	volume = {18},
	copyright = {2021 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/s41592-021-01106-6},
	doi = {10.1038/s41592-021-01106-6},
	abstract = {Comprehensive descriptions of animal behavior require precise three-dimensional (3D) measurements of whole-body movements. Although two-dimensional approaches can track visible landmarks in restrictive environments, performance drops in freely moving animals, due to occlusions and appearance changes. Therefore, we designed DANNCE to robustly track anatomical landmarks in 3D across species and behaviors. DANNCE uses projective geometry to construct inputs to a convolutional neural network that leverages learned 3D geometric reasoning. We trained and benchmarked DANNCE using a dataset of nearly seven million frames that relates color videos and rodent 3D poses. In rats and mice, DANNCE robustly tracked dozens of landmarks on the head, trunk, and limbs of freely moving animals in naturalistic settings. We extended DANNCE to datasets from rat pups, marmosets, and chickadees, and demonstrate quantitative profiling of behavioral lineage during development.},
	language = {en},
	number = {5},
	urldate = {2024-02-15},
	journal = {Nature Methods},
	author = {Dunn, Timothy W. and Marshall, Jesse D. and Severson, Kyle S. and Aldarondo, Diego E. and Hildebrand, David G. C. and Chettih, Selmaan N. and Wang, William L. and Gellis, Amanda J. and Carlson, David E. and Aronov, Dmitriy and Freiwald, Winrich A. and Wang, Fan and Ölveczky, Bence P.},
	month = may,
	year = {2021},
	note = {Number: 5
Publisher: Nature Publishing Group},
	keywords = {Neuroscience, Computational neuroscience, Behavioural methods, Machine learning, Model vertebrates},
	pages = {564--573},
	file = {Paper:/Users/nsirmpilatze/Zotero/storage/PDL49CDA/Dunn et al. - 2021 - Geometric deep learning enables 3D kinematic profi.pdf:application/pdf;Supplementary Info:/Users/nsirmpilatze/Zotero/storage/3IR83XJD/Dunn et al. - 2021 - Geometric deep learning enables 3D kinematic profi.pdf:application/pdf},
}

@article{marshall_leaving_2022,
	title = {Leaving flatland: {Advances} in {3D} behavioral measurement},
	volume = {73},
	issn = {0959-4388},
	shorttitle = {Leaving flatland},
	url = {https://www.sciencedirect.com/science/article/pii/S0959438822000071},
	doi = {10.1016/j.conb.2022.02.002},
	abstract = {Animals move in three dimensions (3D). Thus, 3D measurement is necessary to report the true kinematics of animal movement. Existing 3D measurement techniques draw on specialized hardware, such as motion capture or depth cameras, as well as deep multi-view and monocular computer vision. Continued advances at the intersection of deep learning and computer vision will facilitate 3D tracking across more anatomical features, with less training data, in additional species, and within more natural, occlusive environments. 3D behavioral measurement enables unique applications in phenotyping, investigating the neural basis of behavior, and designing artificial agents capable of imitating animal behavior.},
	urldate = {2024-02-15},
	journal = {Current Opinion in Neurobiology},
	author = {Marshall, Jesse D. and Li, Tianqing and Wu, Joshua H. and Dunn, Timothy W.},
	month = apr,
	year = {2022},
	pages = {102522},
}

@article{li_improved_2023-1,
	title = {Improved {3D} {Markerless} {Mouse} {Pose} {Estimation} {Using} {Temporal} {Semi}-supervision},
	volume = {131},
	issn = {1573-1405},
	url = {https://doi.org/10.1007/s11263-023-01756-3},
	doi = {10.1007/s11263-023-01756-3},
	abstract = {Three-dimensional markerless pose estimation from multi-view video is emerging as an exciting method for quantifying the behavior of freely moving animals. Nevertheless, scientifically precise 3D animal pose estimation remains challenging, primarily due to a lack of large training and benchmark datasets and the immaturity of algorithms tailored to the demands of animal experiments and body plans. Existing techniques employ fully supervised convolutional neural networks (CNNs) trained to predict body keypoints in individual video frames, but this demands a large collection of labeled training samples to achieve desirable 3D tracking performance. Here, we introduce a semi-supervised learning strategy that incorporates unlabeled video frames via a simple temporal constraint applied during training. In freely moving mice, our new approach improves the current state-of-the-art performance of multi-view volumetric 3D pose estimation and further enhances the temporal stability and skeletal consistency of 3D tracking.},
	language = {en},
	number = {6},
	urldate = {2024-02-15},
	journal = {International Journal of Computer Vision},
	author = {Li, Tianqing and Severson, Kyle S. and Wang, Fan and Dunn, Timothy W.},
	month = jun,
	year = {2023},
	keywords = {3D pose estimation, Animal behavioral tracking, Markerless animal tracking, Semi-supervised learning},
	pages = {1389--1405},
}

@misc{zimmermann_freipose_2020,
	title = {{FreiPose}: {A} {Deep} {Learning} {Framework} for {Precise} {Animal} {Motion} {Capture} in {3D} {Spaces}},
	copyright = {© 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	shorttitle = {{FreiPose}},
	url = {https://www.biorxiv.org/content/10.1101/2020.02.27.967620v1},
	doi = {10.1101/2020.02.27.967620},
	abstract = {The increasing awareness of the impact of spontaneous movements on neuronal activity has raised the need to track behavior. We present FreiPose, a versatile learning-based framework to directly capture 3D motion of freely definable points with high precision (median error {\textless} 3.5\% body length, 41.9\% improvement compared to state-of-the-art) and high reliability (82.8\% keypoints within {\textless} 5\% body length error boundary, 72.0\% improvement). The versatility of FreiPose is demonstrated in two experiments: (1) By tracking freely moving rats with simultaneous electrophysiological recordings in motor cortex, we identified neuronal tuning to behavioral states and individual paw trajectories. (2) We inferred time points of optogenetic stimulation in rat motor cortex from the measured pose across individuals and attributed the stimulation effect automatically to body parts. The versatility and accuracy of FreiPose open up new possibilities for quantifying behavior of freely moving animals and may lead to new ways of setting up experiments.},
	language = {en},
	urldate = {2024-02-15},
	publisher = {bioRxiv},
	author = {Zimmermann, Christian and Schneider, Artur and Alyahyay, Mansour and Brox, Thomas and Diester, Ilka},
	month = feb,
	year = {2020},
}

@misc{gosztolai_liftpose3d_2021,
	title = {{LiftPose3D}, a deep learning-based approach for transforming {2D} to {3D} pose in laboratory animals},
	copyright = {© 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2020.09.18.292680v2},
	doi = {10.1101/2020.09.18.292680},
	abstract = {Markerless 3D pose estimation has become an indispensable tool for kinematic studies of laboratory animals. Most current methods recover 3D pose by multi-view triangulation of deep network-based 2D pose estimates. However, triangulation requires multiple, synchronized cameras and elaborate calibration protocols that hinder its widespread adoption in laboratory studies. Here, we describe LiftPose3D, a deep network-based method that overcomes these barriers by reconstructing 3D poses from a single 2D camera view. We illustrate LiftPose3D’s versatility by applying it to multiple experimental systems using flies, mice, rats, and macaque monkeys and in circumstances where 3D triangulation is impractical or impossible. Our framework achieves accurate lifting for stereotyped and non-stereotyped behaviors from different camera angles. Thus, LiftPose3D permits high-quality 3D pose estimation in the absence of complex camera arrays, tedious calibration procedures, and despite occluded body parts in freely behaving animals.},
	language = {en},
	urldate = {2024-02-15},
	publisher = {bioRxiv},
	author = {Gosztolai, Adam and Günel, Semih and Ríos, Victor Lobato and Abrate, Marco Pietro and Morales, Daniel and Rhodin, Helge and Fua, Pascal and Ramdya, Pavan},
	month = apr,
	year = {2021},
}

@misc{karashchuk_anipose_2021-1,
	title = {Anipose: a toolkit for robust markerless {3D} pose estimation},
	copyright = {© 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	shorttitle = {Anipose},
	url = {https://www.biorxiv.org/content/10.1101/2020.05.26.117325v2},
	doi = {10.1101/2020.05.26.117325},
	abstract = {Quantifying movement is critical for understanding animal behavior. Advances in computer vision now enable markerless tracking from 2D video, but most animals live and move in 3D. Here, we introduce Anipose, a Python toolkit for robust markerless 3D pose estimation. Anipose is built on the popular 2D tracking method DeepLabCut, so users can easily expand their existing experimental setups to obtain accurate 3D tracking. It consists of four components: (1) a 3D calibration module, (2) filters to resolve 2D tracking errors, (3) a triangulation module that integrates temporal and spatial regularization, and (4) a pipeline to structure processing of large numbers of videos. We evaluate Anipose on four datasets: a moving calibration board, fruit flies walking on a treadmill, mice reaching for a pellet, and humans performing various actions. By analyzing 3D leg kinematics tracked with Anipose, we identify a key role for joint rotation in motor control of fly walking. We believe this open-source software and accompanying tutorials (anipose.org) will facilitate the analysis of 3D animal behavior and the biology that underlies it.},
	language = {en},
	urldate = {2024-02-15},
	publisher = {bioRxiv},
	author = {Karashchuk, Pierre and Rupp, Katie L. and Dickinson, Evyn S. and Walling-Bell, Sarah and Sanders, Elischa and Azim, Eiman and Brunton, Bingni W. and Tuthill, John C.},
	month = jul,
	year = {2021},
}

@article{karashchuk_dannce_2021,
	title = {The {DANNCE} of the rats: a new toolkit for {3D} tracking of animal behavior},
	volume = {18},
	copyright = {2021 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {1548-7105},
	shorttitle = {The {DANNCE} of the rats},
	url = {https://www.nature.com/articles/s41592-021-01110-w},
	doi = {10.1038/s41592-021-01110-w},
	abstract = {A new approach tracks animal movements in 3D from multiple camera views using volumetric triangulation, reconciling occlusions and ambiguities present in any one camera view.},
	language = {en},
	number = {5},
	urldate = {2024-02-15},
	journal = {Nature Methods},
	author = {Karashchuk, Pierre and Tuthill, John C. and Brunton, Bingni W.},
	month = may,
	year = {2021},
	keywords = {Neuroscience, Software},
	pages = {460--462},
}

@misc{tu_voxelpose_2020,
	title = {{VoxelPose}: {Towards} {Multi}-{Camera} {3D} {Human} {Pose} {Estimation} in {Wild} {Environment}},
	shorttitle = {{VoxelPose}},
	url = {http://arxiv.org/abs/2004.06239},
	doi = {10.48550/arXiv.2004.06239},
	abstract = {We present an approach to estimate 3D poses of multiple people from multiple camera views. In contrast to the previous efforts which require to establish cross-view correspondence based on noisy and incomplete 2D pose estimations, we present an end-to-end solution which directly operates in the \$3\$D space, therefore avoids making incorrect decisions in the 2D space. To achieve this goal, the features in all camera views are warped and aggregated in a common 3D space, and fed into Cuboid Proposal Network (CPN) to coarsely localize all people. Then we propose Pose Regression Network (PRN) to estimate a detailed 3D pose for each proposal. The approach is robust to occlusion which occurs frequently in practice. Without bells and whistles, it outperforms the state-of-the-arts on the public datasets. Code will be released at https://github.com/microsoft/multiperson-pose-estimation-pytorch.},
	urldate = {2024-02-15},
	publisher = {arXiv},
	author = {Tu, Hanyue and Wang, Chunyu and Zeng, Wenjun},
	month = aug,
	year = {2020},
	note = {arXiv:2004.06239 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/nsirmpilatze/Zotero/storage/C9J3R25A/Tu et al. - 2020 - VoxelPose Towards Multi-Camera 3D Human Pose Esti.pdf:application/pdf;arXiv.org Snapshot:/Users/nsirmpilatze/Zotero/storage/FG6YMGYJ/2004.html:text/html},
}

@article{tillmann_-soid_2024,
	title = {A-{SOiD}, an active-learning platform for expert-guided, data-efficient discovery of behavior},
	copyright = {2024 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/s41592-024-02200-1},
	doi = {10.1038/s41592-024-02200-1},
	abstract = {To identify and extract naturalistic behavior, two methods have become popular: supervised and unsupervised. Each approach carries its own strengths and weaknesses (for example, user bias, training cost, complexity and action discovery), which the user must consider in their decision. Here, an active-learning platform, A-SOiD, blends these strengths, and in doing so, overcomes several of their inherent drawbacks. A-SOiD iteratively learns user-defined groups with a fraction of the usual training data, while attaining expansive classification through directed unsupervised classification. In socially interacting mice, A-SOiD outperformed standard methods despite requiring 85\% less training data. Additionally, it isolated ethologically distinct mouse interactions via unsupervised classification. We observed similar performance and efficiency using nonhuman primate and human three-dimensional pose data. In both cases, the transparency in A-SOiD’s cluster definitions revealed the defining features of the supervised classification through a game-theoretic approach. To facilitate use, A-SOiD comes as an intuitive, open-source interface for efficient segmentation of user-defined behaviors and discovered sub-actions.},
	language = {en},
	urldate = {2024-02-22},
	journal = {Nature Methods},
	author = {Tillmann, Jens F. and Hsu, Alexander I. and Schwarz, Martin K. and Yttri, Eric A.},
	month = feb,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	keywords = {Software, Behavioural methods},
	pages = {1--9},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/QY8K4BAF/Tillmann et al. - 2024 - A-SOiD, an active-learning platform for expert-gui.pdf:application/pdf},
}

@article{storchi_high-dimensional_2020,
	title = {A {High}-{Dimensional} {Quantification} of {Mouse} {Defensive} {Behaviors} {Reveals} {Enhanced} {Diversity} and {Stimulus} {Specificity}},
	volume = {30},
	issn = {0960-9822},
	url = {https://www.sciencedirect.com/science/article/pii/S0960982220313385},
	doi = {10.1016/j.cub.2020.09.007},
	abstract = {Instinctive defensive behaviors, consisting of stereotyped sequences of movements and postures, are an essential component of the mouse behavioral repertoire. Since defensive behaviors can be reliably triggered by threatening sensory stimuli, the selection of the most appropriate action depends on the stimulus property. However, since the mouse has a wide repertoire of motor actions, it is not clear which set of movements and postures represent the relevant action. So far, this has been empirically identified as a change in locomotion state. However, the extent to which locomotion alone captures the diversity of defensive behaviors and their sensory specificity is unknown. To tackle this problem, we developed a method to obtain a faithful 3D reconstruction of the mouse body that enabled to quantify a wide variety of motor actions. This higher dimensional description revealed that defensive behaviors are more stimulus specific than indicated by locomotion data. Thus, responses to distinct stimuli that were equivalent in terms of locomotion (e.g., freezing induced by looming and sound) could be discriminated along other dimensions. The enhanced stimulus specificity was explained by a surprising diversity. A clustering analysis revealed that distinct combinations of movements and postures, giving rise to at least 7 different behaviors, were required to account for stimulus specificity. Moreover, each stimulus evoked more than one behavior, revealing a robust one-to-many mapping between sensations and behaviors that was not apparent from locomotion data. Our results indicate that diversity and sensory specificity of mouse defensive behaviors unfold in a higher dimensional space, spanning multiple motor actions.},
	number = {23},
	urldate = {2024-03-14},
	journal = {Current Biology},
	author = {Storchi, Riccardo and Milosavljevic, Nina and Allen, Annette E. and Zippo, Antonio G. and Agnihotri, Aayushi and Cootes, Timothy F. and Lucas, Robert J.},
	month = dec,
	year = {2020},
	keywords = {information theory, 3D reconstruction, behavioral clustering, computational ethology, defensive behaviors, freezing, looming, statistical shape models, stimulus decoding, variable-order Markov chains},
	pages = {4619--4630.e5},
	file = {Full Text:/Users/nsirmpilatze/Zotero/storage/3Z4CN9ZB/Storchi et al. - 2020 - A High-Dimensional Quantification of Mouse Defensi.pdf:application/pdf;ScienceDirect Snapshot:/Users/nsirmpilatze/Zotero/storage/FCV2FQNB/S0960982220313385.html:text/html},
}

@misc{weinreb_keypoint-moseq_2023,
	title = {Keypoint-{MoSeq}: parsing behavior by linking point tracking to pose dynamics},
	copyright = {© 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	shorttitle = {Keypoint-{MoSeq}},
	url = {https://www.biorxiv.org/content/10.1101/2023.03.16.532307v3},
	doi = {10.1101/2023.03.16.532307},
	abstract = {Keypoint tracking algorithms have revolutionized the analysis of animal behavior, enabling investigators to flexibly quantify behavioral dynamics from conventional video recordings obtained in a wide variety of settings. However, it remains unclear how to parse continuous keypoint data into the modules out of which behavior is organized. This challenge is particularly acute because keypoint data is susceptible to high frequency jitter that clustering algorithms can mistake for transitions between behavioral modules. Here we present keypoint-MoSeq, a machine learning-based platform for identifying behavioral modules (“syllables”) from keypoint data without human supervision. Keypoint-MoSeq uses a generative model to distinguish keypoint noise from behavior, enabling it to effectively identify syllables whose boundaries correspond to natural sub-second discontinuities inherent to mouse behavior. Keypoint-MoSeq outperforms commonly used alternative clustering methods at identifying these transitions, at capturing correlations between neural activity and behavior, and at classifying either solitary or social behaviors in accordance with human annotations. Keypoint-MoSeq therefore renders behavioral syllables and grammar accessible to the many researchers who use standard video to capture animal behavior.},
	language = {en},
	urldate = {2024-05-03},
	publisher = {bioRxiv},
	author = {Weinreb, Caleb and Pearl, Jonah and Lin, Sherry and Osman, Mohammed Abdal Monium and Zhang, Libby and Annapragada, Sidharth and Conlin, Eli and Hoffman, Red and Makowska, Sofia and Gillis, Winthrop F. and Jay, Maya and Ye, Shaokai and Mathis, Alexander and Mathis, Mackenzie Weygandt and Pereira, Talmo and Linderman, Scott W. and Datta, Sandeep Robert},
	month = dec,
	year = {2023},
	note = {Pages: 2023.03.16.532307
Section: New Results},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/XDJ3A97N/Weinreb et al. - 2023 - Keypoint-MoSeq parsing behavior by linking point .pdf:application/pdf},
}

@misc{rose_deep_2024,
	title = {Deep {Imputation} for {Skeleton} {Data} ({DISK}) for {Behavioral} {Science}},
	copyright = {© 2024, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial 4.0 International), CC BY-NC 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2024.05.03.592173v1},
	doi = {10.1101/2024.05.03.592173},
	abstract = {Pose estimation methods and motion capture systems have opened doors to quan- titative measurements of animal kinematics. However, these methods are not perfect and contain missing data. Our method, Deep Imputation for Skeleton data (DISK), leverages deep learning algorithms to learn dependencies between keypoints and their dynamics to impute missing tracking data. We developed an unsupervised training scheme, which does not rely on manual annotations, and tested several neural network architectures for the imputation task. We found that transformer outperforms other architectures including graph con- volutional networks that were developed specifically for skeleton-based action recognition. We demonstrate the usability and performance of our imputation method on seven different animal skeletons including two multi-animal set-ups. With an optional estimated imputation error, DISK enables behavior scien- tists to assess the reliability of the imputed data. The imputed recordings allow to detect more episodes of motion, such as steps, and to obtain more sta- tistically robust results when comparing these episodes between experimental conditions. While animal behavior experiments are expensive and complex, track- ing errors make sometimes large portions of the experimental data unusable. DISK allows for filling in the missing information and for taking full advantage of the rich behavioral data. This stand-alone imputation package, freely available at https://github.com/bozeklab/DISK.git, is applicable to results of any track- ing method (marker-based or markerless) and allows for any type of downstream analysis.},
	language = {en},
	urldate = {2024-05-16},
	publisher = {bioRxiv},
	author = {Rose, France and Michaluk, Monika and Blindauer, Timon and Ignatowska-Jankowska, Bogna M. and O’Shaughnessy, Liam and Stephens, Greg J. and Pereira, Talmo D. and Uusisaari, Marylka Y. and Bozek, Katarzyna},
	month = may,
	year = {2024},
	note = {Pages: 2024.05.03.592173
Section: New Results},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/I9PAEYXH/Rose et al. - 2024 - Deep Imputation for Skeleton Data (DISK) for Behav.pdf:application/pdf},
}

@article{goodwin_simple_2024,
	title = {Simple {Behavioral} {Analysis} ({SimBA}) as a platform for explainable machine learning in behavioral neuroscience},
	copyright = {2024 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {1546-1726},
	url = {https://www.nature.com/articles/s41593-024-01649-9},
	doi = {10.1038/s41593-024-01649-9},
	abstract = {The study of complex behaviors is often challenging when using manual annotation due to the absence of quantifiable behavioral definitions and the subjective nature of behavioral annotation. Integration of supervised machine learning approaches mitigates some of these issues through the inclusion of accessible and explainable model interpretation. To decrease barriers to access, and with an emphasis on accessible model explainability, we developed the open-source Simple Behavioral Analysis (SimBA) platform for behavioral neuroscientists. SimBA introduces several machine learning interpretability tools, including SHapley Additive exPlanation (SHAP) scores, that aid in creating explainable and transparent behavioral classifiers. Here we show how the addition of explainability metrics allows for quantifiable comparisons of aggressive social behavior across research groups and species, reconceptualizing behavior as a sharable reagent and providing an open-source framework. We provide an open-source, graphical user interface (GUI)-driven, well-documented package to facilitate the movement toward improved automation and sharing of behavioral classification tools across laboratories.},
	language = {en},
	urldate = {2024-05-22},
	journal = {Nature Neuroscience},
	author = {Goodwin, Nastacia L. and Choong, Jia J. and Hwang, Sophia and Pitts, Kayla and Bloom, Liana and Islam, Aasiya and Zhang, Yizhe Y. and Szelenyi, Eric R. and Tong, Xiaoyu and Newman, Emily L. and Miczek, Klaus and Wright, Hayden R. and McLaughlin, Ryan J. and Norville, Zane C. and Eshel, Neir and Heshmati, Mitra and Nilsson, Simon R. O. and Golden, Sam A.},
	month = may,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	keywords = {Social behaviour, Learning algorithms},
	pages = {1--14},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/Y3G3JP8L/Goodwin et al. - 2024 - Simple Behavioral Analysis (SimBA) as a platform f.pdf:application/pdf},
}

@misc{chindemi_lisbet_2023,
	title = {{LISBET}: a self-supervised {Transformer} model for the automatic segmentation of social behavior motifs},
	shorttitle = {{LISBET}},
	url = {http://arxiv.org/abs/2311.04069},
	doi = {10.48550/arXiv.2311.04069},
	abstract = {Social behavior, defined as the process by which individuals act and react in response to others, is crucial for the function of societies and holds profound implications for mental health. To fully grasp the intricacies of social behavior and identify potential therapeutic targets for addressing social deficits, it is essential to understand its core principles. Although machine learning algorithms have made it easier to study specific aspects of complex behavior, current methodologies tend to focus primarily on single-animal behavior. In this study, we introduce LISBET (seLf-supervIsed Social BEhavioral Transformer), a model designed to detect and segment social interactions. Our model eliminates the need for feature selection and extensive human annotation by using self-supervised learning to detect and quantify social behaviors from dynamic body parts tracking data. LISBET can be used in hypothesis-driven mode to automate behavior classification using supervised finetuning, and in discovery-driven mode to segment social behavior motifs using unsupervised learning. We found that motifs recognized using the discovery-driven approach not only closely match the human annotations but also correlate with the electrophysiological activity of dopaminergic neurons in the Ventral Tegmental Area (VTA). We hope LISBET will help the community improve our understanding of social behaviors and their neural underpinnings.},
	urldate = {2024-07-10},
	publisher = {arXiv},
	author = {Chindemi, Giuseppe and Girard, Benoit and Bellone, Camilla},
	month = nov,
	year = {2023},
	note = {arXiv:2311.04069 [cs, q-bio, stat]},
	keywords = {Computer Science - Machine Learning, Quantitative Biology - Quantitative Methods, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/nsirmpilatze/Zotero/storage/56D9AXYU/Chindemi et al. - 2023 - LISBET a self-supervised Transformer model for th.pdf:application/pdf;arXiv.org Snapshot:/Users/nsirmpilatze/Zotero/storage/ZSDEAMFN/2311.html:text/html},
}

@misc{phadke_reveals_2023,
	title = {{REVEALS}: {An} {Open} {Source} {Multi} {Camera} {GUI} {For} {Rodent} {Behavior} {Acquisition}},
	copyright = {© 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	shorttitle = {{REVEALS}},
	url = {https://www.biorxiv.org/content/10.1101/2023.08.22.554365v1},
	doi = {10.1101/2023.08.22.554365},
	abstract = {Understanding the rich behavioral data generated by mice is essential for deciphering the function of the healthy and diseased brain. However, the current landscape lacks effective, affordable, and accessible methods for acquiring such data, especially when employing multiple cameras simultaneously. We have developed REVEALS (Rodent BEhaVior Multi-camErA Laboratory AcquiSition), a graphical user interface (GUI) written in python for acquiring rodent behavioral data via commonly used USB3 cameras. REVEALS allows for user-friendly control of recording from one or multiple cameras simultaneously while streamlining the data acquisition process, enabling researchers to collect and analyze large datasets efficiently. We release this software package as a stand-alone, open-source framework for researchers to use and modify according to their needs. We describe the details of the GUI implementation, including the camera control software and the video recording functionality. We validate results demonstrating the GUI’s stability, reliability, and accuracy for capturing and analyzing rodent behavior using DeepLabCut pose estimation in both an object and social interaction assay. REVEALS can also be incorporated into other custom pipelines to analyze complex behavior, such as MoSeq. In summary, REVEALS provides an interface for collecting behavioral data from one or multiple perspectives that, combined with deep learning algorithms, will allow the scientific community to discover and characterize complex behavioral phenotypes to understand brain function better.},
	language = {en},
	urldate = {2024-07-20},
	publisher = {bioRxiv},
	author = {Phadke, Rhushikesh A. and Wetzel, Austin M. and Fournier, Luke A. and Sha, Mingqi and Padró-Luna, Nicole M. and Cruz-Martín, Alberto},
	month = aug,
	year = {2023},
	note = {Pages: 2023.08.22.554365
Section: New Results},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/2DJJMEWZ/Phadke et al. - 2023 - REVEALS An Open Source Multi Camera GUI For Roden.pdf:application/pdf},
}

@article{weinreb_keypoint-moseq_2024,
	title = {Keypoint-{MoSeq}: parsing behavior by linking point tracking to pose dynamics},
	volume = {21},
	language = {en},
	journal = {Nature Methods},
	author = {Weinreb, Caleb},
	year = {2024},
	file = {Weinreb - 2024 - Keypoint-MoSeq parsing behavior by linking point .pdf:/Users/nsirmpilatze/Zotero/storage/5QMZIQ5M/Weinreb - 2024 - Keypoint-MoSeq parsing behavior by linking point .pdf:application/pdf},
}

@inproceedings{zhang_animal_2021,
	title = {Animal pose estimation from video data with a hierarchical von {Mises}-{Fisher}-{Gaussian} model},
	url = {https://proceedings.mlr.press/v130/zhang21h.html},
	abstract = {Animal pose estimation from video data is an important step in many biological studies, but current methods struggle in complex environments where occlusions are common and training data is scarce. Recent work has demonstrated improved accuracy with deep neural networks, but these methods often do not incorporate prior distributions that could improve localization. Here we present GIMBAL: a hierarchical von Mises-Fisher-Gaussian model that improves upon deep networks’ estimates by leveraging spatiotemporal constraints. The spatial constraints come from the animal’s skeleton, which induces a curved manifold of keypoint configurations. The temporal constraints come from the postural dynamics, which govern how angles between keypoints change over time. Importantly, the conditional conjugacy of the model permits simple and efficient Bayesian inference algorithms. We assess the model on a unique experimental dataset with video of a freely-behaving rodent from multiple viewpoints and ground-truth motion capture data for 20 keypoints. GIMBAL extends existing techniques, and in doing so offers more accurate estimates of keypoint positions, especially in challenging contexts.},
	language = {en},
	urldate = {2024-08-30},
	booktitle = {Proceedings of {The} 24th {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Zhang, Libby and Dunn, Tim and Marshall, Jesse and Olveczky, Bence and Linderman, Scott},
	month = mar,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {2800--2808},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/JHANK4II/Zhang et al. - 2021 - Animal pose estimation from video data with a hier.pdf:application/pdf;Supplementary PDF:/Users/nsirmpilatze/Zotero/storage/CS3E6HAD/Zhang et al. - 2021 - Animal pose estimation from video data with a hier.pdf:application/pdf},
}

@misc{wu_deep_2020,
	title = {Deep {Graph} {Pose}: a semi-supervised deep graphical model for improved animal pose tracking},
	copyright = {© 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	shorttitle = {Deep {Graph} {Pose}},
	url = {https://www.biorxiv.org/content/10.1101/2020.08.20.259705v2},
	doi = {10.1101/2020.08.20.259705},
	abstract = {Noninvasive behavioral tracking of animals is crucial for many scientific investigations. Recent transfer learning approaches for behavioral tracking have considerably advanced the state of the art. Typically these methods treat each video frame and each object to be tracked independently. In this work, we improve on these methods (particularly in the regime of few training labels) by leveraging the rich spatiotemporal structures pervasive in behavioral video — specifically, the spatial statistics imposed by physical constraints (e.g., paw to elbow distance), and the temporal statistics imposed by smoothness from frame to frame. We propose a probabilistic graphical model built on top of deep neural networks, Deep Graph Pose (DGP), to leverage these useful spatial and temporal constraints, and develop an efficient structured variational approach to perform inference in this model. The resulting semi-supervised model exploits both labeled and unlabeled frames to achieve significantly more accurate and robust tracking while requiring users to label fewer training frames. In turn, these tracking improvements enhance performance on downstream applications, including robust unsupervised segmentation of behavioral “syllables,” and estimation of interpretable “disentangled” low-dimensional representations of the full behavioral video. Open source code is available at https://github.com/paninski-lab/deepgraphpose.},
	language = {en},
	urldate = {2024-08-30},
	publisher = {bioRxiv},
	author = {Wu, Anqi and Buchanan, E. Kelly and Whiteway, Matthew R. and Schartner, Michael and Meijer, Guido and Noel, Jean-Paul and Rodriguez, Erica and Everett, Claire and Norovich, Amy and Schaffer, Evan and Mishra, Neeli and Salzman, C. Daniel and Angelaki, Dora and Bendesky, Andrés and Laboratory, The International Brain and Cunningham, John and Paninski, Liam},
	month = oct,
	year = {2020},
	note = {Pages: 2020.08.20.259705
Section: New Results},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/KXN7QV62/Wu et al. - 2020 - Deep Graph Pose a semi-supervised deep graphical .pdf:application/pdf},
}

@article{mclean_trajr_2018,
	title = {trajr: {An} {R} package for characterisation of animal trajectories},
	volume = {124},
	copyright = {© 2018 The Authors. Ethology Published by Blackwell Verlag GmbH},
	issn = {1439-0310},
	shorttitle = {trajr},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/eth.12739},
	doi = {10.1111/eth.12739},
	abstract = {Quantitative characterisation of the trajectories of moving animals is an important component of many behavioural and ecological studies, however methods are complicated and varied, and sometimes require well-developed programming skills to implement. Here, we introduce trajr, an R package that serves to analyse animal paths, from unicellular organisms, through insects to whales. It makes a variety of statistical characterisations of trajectories, such as tortuosity, speed and changes in direction, available to biologists who may not have a background in programming. We discuss a range of indices that have been used by researchers, describe the package in detail, then use movement observations of whales and clearwing moths to demonstrate some of the capabilities of trajr. As an open-source R package, trajr encourages open and reproducible research. It supports the implementation of additional methods by providing access to trajectory analysis “building blocks,” allows the full suite of R statistical analysis tools to be applied to trajectory analysis, and the source code can be independently validated.},
	language = {en},
	number = {6},
	urldate = {2024-09-17},
	journal = {Ethology},
	author = {McLean, Donald James and Skowron Volponi, Marta A.},
	year = {2018},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/eth.12739},
	keywords = {behaviour, locomotor mimicry, navigation, speed, tortuosity, whales},
	pages = {440--448},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/7MZXC3E2/McLean and Skowron Volponi - 2018 - trajr An R package for characterisation of animal.pdf:application/pdf;Snapshot:/Users/nsirmpilatze/Zotero/storage/MIV2AXQK/eth.html:text/html},
}

@article{wilson_ten_2019,
	title = {Ten simple rules for the computational modeling of behavioral data},
	volume = {8},
	issn = {2050-084X},
	url = {https://doi.org/10.7554/eLife.49547},
	doi = {10.7554/eLife.49547},
	abstract = {Computational modeling of behavior has revolutionized psychology and neuroscience. By fitting models to experimental data we can probe the algorithms underlying behavior, find neural correlates of computational variables and better understand the effects of drugs, illness and interventions. But with great power comes great responsibility. Here, we offer ten simple rules to ensure that computational modeling is used with care and yields meaningful insights. In particular, we present a beginner-friendly, pragmatic and details-oriented introduction on how to relate models to data. What, exactly, can a model tell us about the mind? To answer this, we apply our rules to the simplest modeling techniques most accessible to beginning modelers and illustrate them with examples and code available online. However, most rules apply to more advanced techniques. Our hope is that by following our guidelines, researchers will avoid many pitfalls and unleash the power of computational modeling on their own data.},
	urldate = {2024-10-04},
	journal = {eLife},
	author = {Wilson, Robert C and Collins, Anne GE},
	editor = {Behrens, Timothy E},
	month = nov,
	year = {2019},
	note = {Publisher: eLife Sciences Publications, Ltd},
	keywords = {reproducibility, computational modeling, model fitting, validation},
	pages = {e49547},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/SF8I4HTD/Wilson and Collins - 2019 - Ten simple rules for the computational modeling of.pdf:application/pdf},
}

@article{von_ziegler_analysis_2024,
	title = {Analysis of behavioral flow resolves latent phenotypes},
	copyright = {2024 The Author(s)},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/s41592-024-02500-6},
	doi = {10.1038/s41592-024-02500-6},
	abstract = {The accurate detection and quantification of rodent behavior forms a cornerstone of basic biomedical research. Current data-driven approaches, which segment free exploratory behavior into clusters, suffer from low statistical power due to multiple testing, exhibit poor transferability across experiments and fail to exploit the rich behavioral profiles of individual animals. Here we introduce a pipeline to capture each animal’s behavioral flow, yielding a single metric based on all observed transitions between clusters. By stabilizing these clusters through machine learning, we ensure data transferability, while dimensionality reduction techniques facilitate detailed analysis of individual animals. We provide a large dataset of 771 behavior recordings of freely moving mice—including stress exposures, pharmacological and brain circuit interventions—to identify hidden treatment effects, reveal subtle variations on the level of individual animals and detect brain processes underlying specific interventions. Our pipeline, compatible with popular clustering methods, substantially enhances statistical power and enables predictions of an animal’s future behavior.},
	language = {en},
	urldate = {2024-11-13},
	journal = {Nature Methods},
	author = {von Ziegler, Lukas M. and Roessler, Fabienne K. and Sturman, Oliver and Waag, Rebecca and Privitera, Mattia and Duss, Sian N. and O’Connor, Eoin C. and Bohacek, Johannes},
	month = nov,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	keywords = {Mouse, Software, Behavioural methods, Diseases of the nervous system, Emotion},
	pages = {1--12},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/769W6WQG/von Ziegler et al. - 2024 - Analysis of behavioral flow resolves latent phenot.pdf:application/pdf},
}

@article{von_ziegler_big_2021,
	title = {Big behavior: challenges and opportunities in a new era of deep behavior profiling},
	volume = {46},
	copyright = {2020 The Author(s)},
	issn = {1740-634X},
	shorttitle = {Big behavior},
	url = {https://www.nature.com/articles/s41386-020-0751-7},
	doi = {10.1038/s41386-020-0751-7},
	abstract = {The assessment of rodent behavior forms a cornerstone of preclinical assessment in neuroscience research. Nonetheless, the true and almost limitless potential of behavioral analysis has been inaccessible to scientists until very recently. Now, in the age of machine vision and deep learning, it is possible to extract and quantify almost infinite numbers of behavioral variables, to break behaviors down into subcategories and even into small behavioral units, syllables or motifs. However, the rapidly growing field of behavioral neuroethology is experiencing birthing pains. The community has not yet consolidated its methods, and new algorithms transfer poorly between labs. Benchmarking experiments as well as the large, well-annotated behavior datasets required are missing. Meanwhile, big data problems have started arising and we currently lack platforms for sharing large datasets—akin to sequencing repositories in genomics. Additionally, the average behavioral research lab does not have access to the latest tools to extract and analyze behavior, as their implementation requires advanced computational skills. Even so, the field is brimming with excitement and boundless opportunity. This review aims to highlight the potential of recent developments in the field of behavioral analysis, whilst trying to guide a consensus on practical issues concerning data collection and data sharing.},
	language = {en},
	number = {1},
	urldate = {2024-11-13},
	journal = {Neuropsychopharmacology},
	author = {von Ziegler, Lukas and Sturman, Oliver and Bohacek, Johannes},
	month = jan,
	year = {2021},
	note = {Publisher: Nature Publishing Group},
	keywords = {Behavioural methods, Stress and resilience},
	pages = {33--44},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/KM8CAP7K/von Ziegler et al. - 2021 - Big behavior challenges and opportunities in a ne.pdf:application/pdf},
}

@article{gupte_guide_2022,
	title = {A guide to pre-processing high-throughput animal tracking data},
	volume = {91},
	copyright = {© 2021 The Authors. Journal of Animal Ecology published by John Wiley \& Sons Ltd on behalf of British Ecological Society.},
	issn = {1365-2656},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/1365-2656.13610},
	doi = {10.1111/1365-2656.13610},
	abstract = {Modern, high-throughput animal tracking increasingly yields ‘big data’ at very fine temporal scales. At these scales, location error can exceed the animal's step size, leading to mis-estimation of behaviours inferred from movement. ‘Cleaning’ the data to reduce location errors is one of the main ways to deal with position uncertainty. Although data cleaning is widely recommended, inclusive, uniform guidance on this crucial step, and on how to organise the cleaning of massive datasets, is relatively scarce. A pipeline for cleaning massive high-throughput datasets must balance ease of use and computationally efficiency, in which location errors are rejected while preserving valid animal movements. Another useful feature of a pre-processing pipeline is efficiently segmenting and clustering location data for statistical methods while also being scalable to large datasets and robust to imperfect sampling. Manual methods being prohibitively time-consuming, and to boost reproducibility, pre-processing pipelines must be automated. We provide guidance on building pipelines for pre-processing high-throughput animal tracking data to prepare it for subsequent analyses. We apply our proposed pipeline to simulated movement data with location errors, and also show how large volumes of cleaned data can be transformed into biologically meaningful ‘residence patches’, for exploratory inference on animal space use. We use tracking data from the Wadden Sea ATLAS system (WATLAS) to show how pre-processing improves its quality, and to verify the usefulness of the residence patch method. Finally, with tracks from Egyptian fruit bats Rousettus aegyptiacus, we demonstrate the pre-processing pipeline and residence patch method in a fully worked out example. To help with fast implementation of standardised methods, we developed the R package atlastools, which we also introduce here. Our pre-processing pipeline and atlastools can be used with any high-throughput animal movement data in which the high data-volume combined with knowledge of the tracked individuals' movement capacity can be used to reduce location errors. atlastools is easy to use for beginners while providing a template for further development. The common use of simple yet robust pre-processing steps promotes standardised methods in the field of movement ecology and leads to better inferences from data.},
	language = {en},
	number = {2},
	urldate = {2024-11-28},
	journal = {Journal of Animal Ecology},
	author = {Gupte, Pratik Rajan and Beardsworth, Christine E. and Spiegel, Orr and Lourie, Emmanuel and Toledo, Sivan and Nathan, Ran and Bijleveld, Allert I.},
	year = {2022},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/1365-2656.13610},
	keywords = {ATLAS tracking, atlastools, big data, biotelemetry, data cleaning, high-throughput movement ecology, residence patch, reverse GPS},
	pages = {287--307},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/EPFRIHWU/Gupte et al. - 2022 - A guide to pre-processing high-throughput animal t.pdf:application/pdf;Snapshot:/Users/nsirmpilatze/Zotero/storage/897N4LPQ/1365-2656.html:text/html},
}

@article{von_ziegler_analysis_2024-1,
	title = {Analysis of behavioral flow resolves latent phenotypes},
	volume = {21},
	copyright = {2024 The Author(s)},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/s41592-024-02500-6},
	doi = {10.1038/s41592-024-02500-6},
	abstract = {The accurate detection and quantification of rodent behavior forms a cornerstone of basic biomedical research. Current data-driven approaches, which segment free exploratory behavior into clusters, suffer from low statistical power due to multiple testing, exhibit poor transferability across experiments and fail to exploit the rich behavioral profiles of individual animals. Here we introduce a pipeline to capture each animal’s behavioral flow, yielding a single metric based on all observed transitions between clusters. By stabilizing these clusters through machine learning, we ensure data transferability, while dimensionality reduction techniques facilitate detailed analysis of individual animals. We provide a large dataset of 771 behavior recordings of freely moving mice—including stress exposures, pharmacological and brain circuit interventions—to identify hidden treatment effects, reveal subtle variations on the level of individual animals and detect brain processes underlying specific interventions. Our pipeline, compatible with popular clustering methods, substantially enhances statistical power and enables predictions of an animal’s future behavior.},
	language = {en},
	number = {12},
	urldate = {2025-01-14},
	journal = {Nature Methods},
	author = {von Ziegler, Lukas M. and Roessler, Fabienne K. and Sturman, Oliver and Waag, Rebecca and Privitera, Mattia and Duss, Sian N. and O’Connor, Eoin C. and Bohacek, Johannes},
	month = dec,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	keywords = {Mouse, Software, Behavioural methods, Diseases of the nervous system, Emotion},
	pages = {2376--2387},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/FFBD7YC9/von Ziegler et al. - 2024 - Analysis of behavioral flow resolves latent phenot.pdf:application/pdf},
}

@article{blau_study_2024,
	title = {A study of animal action segmentation algorithms across supervised, unsupervised, and semi-supervised learning paradigms},
	url = {https://nbdt.scholasticahq.com/article/127770-a-study-of-animal-action-segmentation-algorithms-across-supervised-unsupervised-and-semi-supervised-learning-paradigms},
	doi = {10.51628/001c.127770},
	abstract = {Action segmentation of behavioral videos is the process of labeling each frame as belonging to one or more discrete classes, and is a crucial component of many studies that investigate animal behavior. A wide range of algorithms exist to automatically parse discrete animal behavior, encompassing supervised, unsupervised, and semi-supervised learning paradigms. These algorithms -- which include tree-based models, deep neural networks, and graphical models -- differ widely in their structure and assumptions on the data. Using four datasets spanning multiple species -- fly, mouse, and human -- we systematically study how the outputs of these various algorithms align with manually annotated behaviors of interest. Along the way, we introduce a semi-supervised action segmentation model that bridges the gap between supervised deep neural networks and unsupervised graphical models. We find that fully supervised temporal convolutional networks with the addition of temporal information in the observations perform the best on our supervised metrics across all datasets.},
	language = {en},
	urldate = {2025-01-21},
	journal = {Neurons, Behavior, Data analysis, and Theory},
	author = {Blau, Ari and Schaffer, Evan S. and Mishra, Neeli and Miska, Nathaniel J. and Laboratory, International Brain and Paninski, Liam and Whiteway, Matthew R.},
	month = dec,
	year = {2024},
	note = {Publisher: The neurons, behavior, data analysis and theory collective},
	pages = {1--46},
}

@misc{blau_study_2024-1,
	title = {A study of animal action segmentation algorithms across supervised, unsupervised, and semi-supervised learning paradigms},
	url = {http://arxiv.org/abs/2407.16727},
	doi = {10.48550/arXiv.2407.16727},
	abstract = {Action segmentation of behavioral videos is the process of labeling each frame as belonging to one or more discrete classes, and is a crucial component of many studies that investigate animal behavior. A wide range of algorithms exist to automatically parse discrete animal behavior, encompassing supervised, unsupervised, and semi-supervised learning paradigms. These algorithms -- which include tree-based models, deep neural networks, and graphical models -- differ widely in their structure and assumptions on the data. Using four datasets spanning multiple species -- fly, mouse, and human -- we systematically study how the outputs of these various algorithms align with manually annotated behaviors of interest. Along the way, we introduce a semi-supervised action segmentation model that bridges the gap between supervised deep neural networks and unsupervised graphical models. We find that fully supervised temporal convolutional networks with the addition of temporal information in the observations perform the best on our supervised metrics across all datasets.},
	urldate = {2025-01-21},
	publisher = {arXiv},
	author = {Blau, Ari and Schaffer, Evan S. and Mishra, Neeli and Miska, Nathaniel J. and Laboratory, The International Brain and Paninski, Liam and Whiteway, Matthew R.},
	month = dec,
	year = {2024},
	note = {arXiv:2407.16727 [cs]},
	keywords = {Quantitative Biology - Quantitative Methods, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/Users/nsirmpilatze/Zotero/storage/YDMK6NMP/Blau et al. - 2024 - A study of animal action segmentation algorithms a.pdf:application/pdf;Snapshot:/Users/nsirmpilatze/Zotero/storage/KZ8YSIFQ/2407.html:text/html},
}

@article{gu_emergence_2025,
	title = {Emergence of collective oscillations in massive human crowds},
	volume = {638},
	copyright = {2025 The Author(s)},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-024-08514-6},
	doi = {10.1038/s41586-024-08514-6},
	abstract = {Dense crowds form some of the most dangerous environments in modern society1. Dangers arise from uncontrolled collective motions, leading to compression against walls, suffocation and fatalities2–4. Our current understanding of crowd dynamics primarily relies on heuristic collision models, which effectively capture the behaviour observed in small groups of people5,6. However, the emergent dynamics of dense crowds, composed of thousands of individuals, remains a formidable many-body problem lacking quantitative experimental characterization and explanations rooted in first principles. Here we analyse the dynamics of thousands of densely packed individuals at the San Fermín festival (Spain) and infer a physical theory of dense crowds in confinement. Our measurements reveal that dense crowds can self-organize into macroscopic chiral oscillators, coordinating the orbital motion of hundreds of individuals without external guidance. Guided by these measurements and symmetry principles, we construct a mechanical model of dense-crowd motion. Our model demonstrates that emergent odd frictional forces drive a non-reciprocal phase transition7 towards collective chiral oscillations, capturing all our experimental observations. To test the robustness of our findings, we show that similar chiral dynamics emerged at the onset of the 2010 Love Parade disaster and propose a protocol that could help anticipate these previously unpredictable dynamics.},
	language = {en},
	number = {8049},
	urldate = {2025-02-07},
	journal = {Nature},
	author = {Gu, François and Guiselin, Benjamin and Bain, Nicolas and Zuriguel, Iker and Bartolo, Denis},
	month = feb,
	year = {2025},
	note = {Publisher: Nature Publishing Group},
	keywords = {Biological physics, Fluid dynamics, Statistical physics, thermodynamics and nonlinear dynamics},
	pages = {112--119},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/56KZNV5E/Gu et al. - 2025 - Emergence of collective oscillations in massive hu.pdf:application/pdf},
}

@article{cavagna_physics_2018,
	series = {The physics of flocking: {Correlation} as a compass from experiments to theory},
	title = {The physics of flocking: {Correlation} as a compass from experiments to theory},
	volume = {728},
	issn = {0370-1573},
	shorttitle = {The physics of flocking},
	url = {https://www.sciencedirect.com/science/article/pii/S0370157317303575},
	doi = {10.1016/j.physrep.2017.11.003},
	abstract = {Collective behavior in biological systems is a complex topic, to say the least. It runs wildly across scales in both space and time, involving taxonomically vastly different organisms, from bacteria and cell clusters, to insect swarms and up to vertebrate groups. It entails concepts as diverse as coordination, emergence, interaction, information, cooperation, decision-making, and synchronization. Amid this jumble, however, we cannot help noting many similarities between collective behavior in biological systems and collective behavior in statistical physics, even though none of these organisms remotely looks like an Ising spin. Such similarities, though somewhat qualitative, are startling, and regard mostly the emergence of global dynamical patterns qualitatively different from individual behavior, and the development of system-level order from local interactions. It is therefore tempting to describe collective behavior in biology within the conceptual framework of statistical physics, in the hope to extend to this new fascinating field at least part of the great predictive power of theoretical physics. In this review we propose that the conceptual cornerstone of this ambitious program be that of correlation. To illustrate this idea we address the case of collective behavior in bird flocks. Two key threads emerge, as two sides of one single story: the presence of scale-free correlations and the dynamical mechanism of information transfer. We discuss first static correlations in starling flocks, in particular the experimental finding of their scale-free nature, the formulation of models that account for this fact using maximum entropy, and the relation of scale-free correlations to information transfer. This is followed by a dynamic treatment of information propagation (propagation of turns across a flock), starting with a discussion of experimental results and following with possible theoretical explanations of those, which require the addition of behavioral inertia to existing theories of flocking. We finish with the definition and analysis of space–time correlations and their relevance to the detection of inertial behavior in the absence of external perturbations.},
	urldate = {2025-02-07},
	journal = {Physics Reports},
	author = {Cavagna, Andrea and Giardina, Irene and Grigera, Tomás S.},
	month = jan,
	year = {2018},
	pages = {1--62},
	file = {ScienceDirect Snapshot:/Users/nsirmpilatze/Zotero/storage/9SCGA3JZ/S0370157317303575.html:text/html},
}

@article{cavagna_scale-free_2010,
	title = {Scale-free correlations in starling flocks},
	volume = {107},
	url = {https://www.pnas.org/doi/10.1073/pnas.1005766107},
	doi = {10.1073/pnas.1005766107},
	abstract = {From bird flocks to fish schools, animal groups often seem to react to environmental perturbations as if of one mind. Most studies in collective animal behavior have aimed to understand how a globally ordered state may emerge from simple behavioral rules. Less effort has been devoted to understanding the origin of collective response, namely the way the group as a whole reacts to its environment. Yet, in the presence of strong predatory pressure on the group, collective response may yield a significant adaptive advantage. Here we suggest that collective response in animal groups may be achieved through scale-free behavioral correlations. By reconstructing the 3D position and velocity of individual birds in large flocks of starlings, we measured to what extent the velocity fluctuations of different birds are correlated to each other. We found that the range of such spatial correlation does not have a constant value, but it scales with the linear size of the flock. This result indicates that behavioral correlations are scale free: The change in the behavioral state of one animal affects and is affected by that of all other animals in the group, no matter how large the group is. Scale-free correlations provide each animal with an effective perception range much larger than the direct interindividual interaction range, thus enhancing global response to perturbations. Our results suggest that flocks behave as critical systems, poised to respond maximally to environmental perturbations.},
	number = {26},
	urldate = {2025-02-07},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Cavagna, Andrea and Cimarelli, Alessio and Giardina, Irene and Parisi, Giorgio and Santagati, Raffaele and Stefanini, Fabio and Viale, Massimiliano},
	month = jun,
	year = {2010},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	pages = {11865--11870},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/8CEJF6BV/Cavagna et al. - 2010 - Scale-free correlations in starling flocks.pdf:application/pdf},
}

@misc{noauthor_my_nodate,
	title = {My {Drive}},
	url = {https://drive.google.com/drive/my-drive},
	language = {en},
	urldate = {2025-02-24},
	journal = {Google Drive},
	file = {Snapshot:/Users/nsirmpilatze/Zotero/storage/BWVFC3QF/my-drive.html:text/html},
}

@article{klibaite_mapping_2025,
	title = {Mapping the landscape of social behavior},
	volume = {0},
	issn = {0092-8674, 1097-4172},
	url = {https://www.cell.com/cell/abstract/S0092-8674(25)00154-0},
	doi = {10.1016/j.cell.2025.01.044},
	language = {English},
	number = {0},
	urldate = {2025-03-05},
	journal = {Cell},
	author = {Klibaite, Ugne and Li, Tianqing and Aldarondo, Diego and Akoad, Jumana F. and Ölveczky, Bence P. and Dunn, Timothy W.},
	month = mar,
	year = {2025},
	note = {Publisher: Elsevier},
	keywords = {neuroscience, machine learning, 3D animal pose, animal behavior, ASD, autism, computer vision, high-resolution phenotyping, social behavior},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/UYGZSHKP/Klibaite et al. - 2025 - Mapping the landscape of social behavior.pdf:application/pdf},
}

@article{koger_quantifying_2023,
	title = {Quantifying the movement, behaviour and environmental context of group-living animals using drones and computer vision},
	volume = {92},
	issn = {1365-2656},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/1365-2656.13904},
	doi = {10.1111/1365-2656.13904},
	abstract = {Methods for collecting animal behaviour data in natural environments, such as direct observation and biologging, are typically limited in spatiotemporal resolution, the number of animals that can be observed and information about animals' social and physical environments. Video imagery can capture rich information about animals and their environments, but image-based approaches are often impractical due to the challenges of processing large and complex multi-image datasets and transforming resulting data, such as animals' locations, into geographical coordinates. We demonstrate a new system for studying behaviour in the wild that uses drone-recorded videos and computer vision approaches to automatically track the location and body posture of free-roaming animals in georeferenced coordinates with high spatiotemporal resolution embedded in contemporaneous 3D landscape models of the surrounding area. We provide two worked examples in which we apply this approach to videos of gelada monkeys and multiple species of group-living African ungulates. We demonstrate how to track multiple animals simultaneously, classify individuals by species and age–sex class, estimate individuals' body postures (poses) and extract environmental features, including topography of the landscape and animal trails. By quantifying animal movement and posture while reconstructing a detailed 3D model of the landscape, our approach opens the door to studying the sensory ecology and decision-making of animals within their natural physical and social environments.},
	language = {en},
	number = {7},
	urldate = {2025-03-06},
	journal = {Journal of Animal Ecology},
	author = {Koger, Benjamin and Deshpande, Adwait and Kerby, Jeffrey T. and Graving, Jacob M. and Costelloe, Blair R. and Couzin, Iain D.},
	year = {2023},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/1365-2656.13904},
	keywords = {computer vision, behavioural tracking, drones, environmental reconstruction, gelada monkey, pose, posture, video analysis, wildlife, zebra},
	pages = {1357--1371},
	file = {Snapshot:/Users/nsirmpilatze/Zotero/storage/8NRM73D4/1365-2656.html:text/html},
}

@article{benhamou_how_2004,
	title = {How to reliably estimate the tortuosity of an animal's path:: straightness, sinuosity, or fractal dimension?},
	volume = {229},
	issn = {0022-5193},
	shorttitle = {How to reliably estimate the tortuosity of an animal's path},
	url = {https://www.sciencedirect.com/science/article/pii/S0022519304001353},
	doi = {10.1016/j.jtbi.2004.03.016},
	abstract = {The tortuosity of an animal's path is a key parameter in orientation and searching behaviours. The tortuosity of an oriented path is inversely related to the efficiency of the orientation mechanism involved, the best mechanism being assumed to allow the animal to reach its goal along a straight line movement. The tortuosity of a random search path controls the local searching intensity, allowing the animal to adjust its search effort to the local profitability of the environment. This paper shows that (1) the efficiency of an oriented path can be reliably estimated by a straightness index computed as the ratio between the distance from the starting point to the goal and the path length travelled to reach the goal, but such a simple index, ranging between 0 and 1, cannot be applied to random search paths; (2) the tortuosity of a random search path, ranging between straight line movement and Brownian motion, can be reliably estimated by a sinuosity index which combines the mean cosine of changes of direction with the mean step length; and (3) in the current state of the art, the fractal analysis of animals’ paths, which may appear as an alternative and promising way to measure the tortuosity of a random search path as a fractal dimension ranging between 1 (straight line movement) and 2 (Brownian motion), is only liable to generate artifactual results. This paper also provides some help for distinguishing between oriented and random search paths, and depicts a general, comprehensive framework for analysing individual animals’ paths in a two-dimensional space.},
	number = {2},
	urldate = {2025-03-27},
	journal = {Journal of Theoretical Biology},
	author = {Benhamou, Simon},
	month = jul,
	year = {2004},
	keywords = {Diffusion, Fractal, Orientation, Random Search},
	pages = {209--220},
	file = {ScienceDirect Snapshot:/Users/nsirmpilatze/Zotero/storage/ZX47HPZ7/S0022519304001353.html:text/html},
}

@article{almeida_indices_2010,
	title = {Indices of movement behaviour: conceptual background, effects of scale and location errors},
	volume = {27},
	issn = {1984-4670, 1984-4689},
	shorttitle = {Indices of movement behaviour},
	url = {https://www.scielo.br/j/zool/a/8F9QpD7mRFttmkY9QdxZTmm/?lang=en},
	doi = {https://doi.org/10.1590/S1984-46702010000500002},
	abstract = {A fundamental step in the emerging Movement Theory is the description of movement paths, and the identification of its proximate and ultimate drivers. The most common characteristic used to describe and analyze movement paths is its tortuosity, and a variety of tortuosity indices have been proposed in different theoretical or empirical contexts. Here we review conceptual differences between five movement indices and their bias due to locations errors, sample sizes and scale-dependency: Intensity of Habitat use (IU), Fractal D, MSD (Mean Squared Distance), Straightness (ST), and Sinuosity (SI). Intensity of Habitat use and ST are straightforward to compute, but ST is actually an unbiased estimator of oriented search and ballistic movements. Fractal D is less straightforward to compute and represents an index of propensity to cover the plane, whereas IU is the only completely empirical of the three. These three indices could be used to identify different phases of path, and their path tortuosity is a dimensionless feature of the path, depending mostly on path shape, not on the unit of measurement. This concept of tortuosity differs from a concept implied in the sinuosity of BENHAMOU (2004), where a specific random walk movement model is assumed, and diffusion distance is a function of path length and turning angles, requiring their inclusion in a measure of sinuosity. MSD should be used as a diagnostic tool of random walk paths rather than an index of tortuosity. Bias due to location errors, sample size and scale, differs between the indices, as well as the concept of tortuosity implied. These differences must be considered when choosing the most appropriate index.},
	language = {en},
	urldate = {2025-03-27},
	journal = {Zoologia (Curitiba)},
	author = {Almeida, Paulo J. A. L. and Vieira, Marcus V. and Kajin, Maja and Forero-Medina, German and Cerqueira, Rui},
	month = oct,
	year = {2010},
	note = {Publisher: Sociedade Brasileira de Zoologia},
	keywords = {Diffusion, fractal dimension, oriented search, random walk, search behaviour},
	pages = {674--680},
}

@article{wario_automatic_2017,
	title = {Automatic detection and decoding of honey bee waggle dances},
	volume = {12},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0188626},
	doi = {10.1371/journal.pone.0188626},
	abstract = {The waggle dance is one of the most popular examples of animal communication. Forager bees direct their nestmates to profitable resources via a complex motor display. Essentially, the dance encodes the polar coordinates to the resource in the field. Unemployed foragers follow the dancer’s movements and then search for the advertised spots in the field. Throughout the last decades, biologists have employed different techniques to measure key characteristics of the waggle dance and decode the information it conveys. Early techniques involved the use of protractors and stopwatches to measure the dance orientation and duration directly from the observation hive. Recent approaches employ digital video recordings and manual measurements on screen. However, manual approaches are very time-consuming. Most studies, therefore, regard only small numbers of animals in short periods of time. We have developed a system capable of automatically detecting, decoding and mapping communication dances in real-time. In this paper, we describe our recording setup, the image processing steps performed for dance detection and decoding and an algorithm to map dances to the field. The proposed system performs with a detection accuracy of 90.07\%. The decoded waggle orientation has an average error of -2.92° (± 7.37°), well within the range of human error. To evaluate and exemplify the system’s performance, a group of bees was trained to an artificial feeder, and all dances in the colony were automatically detected, decoded and mapped. The system presented here is the first of this kind made publicly available, including source code and hardware specifications. We hope this will foster quantitative analyses of the honey bee waggle dance.},
	language = {en},
	number = {12},
	urldate = {2025-03-27},
	journal = {PLOS ONE},
	author = {Wario, Fernando and Wild, Benjamin and Rojas, Raúl and Landgraf, Tim},
	month = dec,
	year = {2017},
	note = {Publisher: Public Library of Science},
	keywords = {Computer software, Advertising, Bees, Foraging, Fourier analysis, Honey bees, Video recording, Waggle dancing},
	pages = {e0188626},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/RZHVVZ5M/Wario et al. - 2017 - Automatic detection and decoding of honey bee wagg.pdf:application/pdf},
}

@article{bialek_long_2024,
	title = {Long {Timescales}, {Individual} {Differences}, and {Scale} {Invariance} in {Animal} {Behavior}},
	volume = {132},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.132.048401},
	doi = {10.1103/PhysRevLett.132.048401},
	abstract = {The explosion of data on animal behavior in more natural contexts highlights the fact that these behaviors exhibit correlations across many timescales. However, there are major challenges in analyzing these data: records of behavior in single animals have fewer independent samples than one might expect. In pooling data from multiple animals, individual differences can mimic long-ranged temporal correlations; conversely, long-ranged correlations can lead to an overestimate of individual differences. We suggest an analysis scheme that addresses these problems directly, apply this approach to data on the spontaneous behavior of walking flies, and find evidence for scale-invariant correlations over nearly three decades in time, from seconds to one hour. Three different measures of correlation are consistent with a single underlying scaling field of dimension Δ=0.180±0.005.},
	number = {4},
	urldate = {2025-03-27},
	journal = {Physical Review Letters},
	author = {Bialek, William and Shaevitz, Joshua W.},
	month = jan,
	year = {2024},
	note = {Publisher: American Physical Society},
	pages = {048401},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/Q6QN4FLE/Bialek and Shaevitz - 2024 - Long Timescales, Individual Differences, and Scale.pdf:application/pdf},
}

@article{sasaki_groups_2012,
	title = {Groups have a larger cognitive capacity than individuals},
	volume = {22},
	issn = {0960-9822},
	url = {https://www.sciencedirect.com/science/article/pii/S0960982212008834},
	doi = {10.1016/j.cub.2012.07.058},
	abstract = {Increasing the number of options can paradoxically lead to worse decisions, a phenomenon known as cognitive overload [1]. This happens when an individual decision-maker attempts to digest information exceeding its processing capacity. Highly integrated groups, such as social insect colonies, make consensus decisions that combine the efforts of many members, suggesting that these groups can overcome individual limitations 2, 3, 4. Here we report that an ant colony choosing a new nest site is less vulnerable to cognitive overload than an isolated ant making this decision on her own. We traced this improvement to differences in individual behavior. In whole colonies, each ant assesses only a small subset of available sites, and the colony combines their efforts to thoroughly explore all options. An isolated ant, on the other hand, must personally assess a larger number of sites to approach the same level of option coverage. By sharing the burden of assessment, the colony avoids overtaxing the abilities of its members.},
	number = {19},
	urldate = {2025-03-27},
	journal = {Current Biology},
	author = {Sasaki, Takao and Pratt, Stephen C.},
	month = oct,
	year = {2012},
	pages = {R827--R829},
}

@article{sankey_absence_2021,
	title = {Absence of “selfish herd” dynamics in bird flocks under threat},
	volume = {31},
	issn = {0960-9822},
	url = {https://www.sciencedirect.com/science/article/pii/S0960982221006680},
	doi = {10.1016/j.cub.2021.05.009},
	abstract = {The “selfish herd” hypothesis1 provides a potential mechanism to explain a ubiquitous phenomenon in nature: that of non-kin aggregations. Individuals in selfish herds are thought to benefit by reducing their own risk at the expense of conspecifics by attracting toward their neighbors’ positions1,2 or central locations in the aggregation.3, 4, 5 Alternatively, increased alignment with their neighbors’ orientation could reduce the chance of predation through information sharing6, 7, 8 or collective escape.6 Using both small and large flocks of homing pigeons (Columba livia; n = 8–10 or n = 27–34 individuals) tagged with 5-Hz GPS loggers and a GPS-tagged, remote-controlled model peregrine falcon (Falco peregrinus), we tested whether individuals increase their use of attraction over alignment when under perceived threat. We conducted n = 27 flights in treatment conditions, chased by the robotic “predator,” and n = 16 flights in control conditions (not chased). Despite responding strongly to the RobotFalcon—by turning away from its flight direction—individuals in treatment flocks demonstrated no increased attraction compared with control flocks, and this result held across both flock sizes. We suggest that mutualistic alignment is more advantageous than selfish attraction in groups with a high coincidence of individual and collective interests (adaptive hypothesis). However, we also explore alternative explanations, such as high cognitive demand under threat and collision avoidance (mechanistic hypotheses). We conclude that selfish herd may not be an appropriate paradigm for understanding the function of highly synchronous collective motion, as observed in bird flocks and perhaps also fish shoals and highly aligned mammal aggregations, such as moving herds.},
	number = {14},
	urldate = {2025-03-27},
	journal = {Current Biology},
	author = {Sankey, Daniel W. E. and Storms, Rolf F. and Musters, Robert J. and Russell, Timothy W. and Hemelrijk, Charlotte K. and Portugal, Steven J.},
	month = jul,
	year = {2021},
	keywords = {social behavior, animal-robot interactions, collective decision making, collective motion, flight dynamics, movement ecology, mutualism},
	pages = {3192--3198.e7},
	file = {Full Text:/Users/nsirmpilatze/Zotero/storage/96CY4ZID/Sankey et al. - 2021 - Absence of “selfish herd” dynamics in bird flocks .pdf:application/pdf},
}

@article{sasaki_ant_2013,
	title = {Ant colonies outperform individuals when a sensory discrimination task is difficult but not when it is easy},
	volume = {110},
	url = {https://www.pnas.org/doi/full/10.1073/pnas.1304917110},
	doi = {10.1073/pnas.1304917110},
	abstract = {“Collective intelligence” and “wisdom of crowds” refer to situations in which groups achieve more accurate perception and better decisions than solitary agents. Whether groups outperform individuals should depend on the kind of task and its difficulty, but the nature of this relationship remains unknown. Here we show that colonies of Temnothorax ants outperform individuals for a difficult perception task but that individuals do better than groups when the task is easy. Subjects were required to choose the better of two nest sites as the quality difference was varied. For small differences, colonies were more likely than isolated ants to choose the better site, but this relationship was reversed for large differences. We explain these results using a mathematical model, which shows that positive feedback between group members effectively integrates information and sharpens the discrimination of fine differences. When the task is easier the same positive feedback can lock the colony into a suboptimal choice. These results suggest the conditions under which crowds do or do not become wise.},
	number = {34},
	urldate = {2025-03-27},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Sasaki, Takao and Granovskiy, Boris and Mann, Richard P. and Sumpter, David J. T. and Pratt, Stephen C.},
	month = aug,
	year = {2013},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	pages = {13769--13773},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/PJX2IMVR/Sasaki et al. - 2013 - Ant colonies outperform individuals when a sensory.pdf:application/pdf},
}

@article{morrell_mechanisms_2008,
	title = {Mechanisms for aggregation in animals: rule success depends on ecological variables},
	volume = {19},
	issn = {1045-2249},
	shorttitle = {Mechanisms for aggregation in animals},
	url = {https://doi.org/10.1093/beheco/arm122},
	doi = {10.1093/beheco/arm122},
	abstract = {Under the threat of predation, animals often group tightly together, with all group members benefiting from a reduction in predation risk through various mechanisms, including the dilution, encounter-dilution, and predator confusion effects. Additionally, the selfish herd hypothesis was first put forward by Hamilton (1971). He proposed that in order to reduce its risk of predation, an individual should approach its nearest neighbor, reducing its risk at the expense of those around it. Despite extensive empirical support, the selfish herd hypothesis has been criticized on theoretical grounds: approaching the nearest neighbor does not result in the observed dense aggregations, and the nearest neighbor in space is not necessarily the one that can be reached fastest. Increasingly complex movement rules have been proposed, successfully producing dense aggregations of individuals. However, no study to date has made a full comparison of the different proposed movement rules within the same modeling environment. Further, ecologically relevant parameters, such as the size and density of a population or group and the time it takes a predator to attack, have thus far been ignored. Here, we investigate the reduction in risk for animals aggregating using different strategies and demonstrate the importance of ecological parameters on risk reduction in group-living animals. We find that complex rules are most successful at reducing risk in small, compact populations, whereas simpler rules are most successful in larger, low-density populations, and when predators attack quickly after being detected by their prey.},
	number = {1},
	urldate = {2025-03-27},
	journal = {Behavioral Ecology},
	author = {Morrell, Lesley J. and James, Richard},
	month = jan,
	year = {2008},
	pages = {193--201},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/P544K9L6/Morrell and James - 2008 - Mechanisms for aggregation in animals rule succes.pdf:application/pdf},
}

@article{biro_bringing_2016,
	title = {Bringing a {Time}–{Depth} {Perspective} to {Collective} {Animal} {Behaviour}},
	volume = {31},
	issn = {0169-5347},
	url = {https://www.sciencedirect.com/science/article/pii/S0169534716300027},
	doi = {10.1016/j.tree.2016.03.018},
	abstract = {The field of collective animal behaviour examines how relatively simple, local interactions between individuals in groups combine to produce global-level outcomes. Existing mathematical models and empirical work have identified candidate mechanisms for numerous collective phenomena but have typically focused on one-off or short-term performance. We argue that feedback between collective performance and learning – giving the former the capacity to become an adaptive, and potentially cumulative, process – is a currently poorly explored but crucial mechanism in understanding collective systems. We synthesise material ranging from swarm intelligence in social insects through collective movements in vertebrates to collective decision making in animal and human groups, to propose avenues for future research to identify the potential for changes in these systems to accumulate over time.},
	number = {7},
	urldate = {2025-03-27},
	journal = {Trends in Ecology \& Evolution},
	author = {Biro, Dora and Sasaki, Takao and Portugal, Steven J.},
	month = jul,
	year = {2016},
	keywords = {decision making, collective behaviour, collective intelligence, collective learning, energetics, time–depth},
	pages = {550--562},
	file = {Submitted Version:/Users/nsirmpilatze/Zotero/storage/HAWDUEXZ/Biro et al. - 2016 - Bringing a Time–Depth Perspective to Collective An.pdf:application/pdf},
}

@book{sumpter_collective_2010,
	title = {Collective {Animal} {Behavior}},
	copyright = {De Gruyter expressly reserves the right to use all content for commercial text and data mining within the meaning of Section 44b of the German Copyright Act.},
	isbn = {978-1-4008-3710-6},
	url = {https://www.degruyter.com/document/doi/10.1515/9781400837106/html},
	abstract = {How and why animals produce group behaviors Fish travel in schools, birds migrate in flocks, honeybees swarm, and ants build trails. How and why do these collective behaviors occur? Exploring how coordinated group patterns emerge from individual interactions, Collective Animal Behavior reveals why animals produce group behaviors and examines their evolution across a range of species. Providing a synthesis of mathematical modeling, theoretical biology, and experimental work, David Sumpter investigates how animals move and arrive together, how they transfer information, how they make decisions and synchronize their activities, and how they build collective structures. Sumpter constructs a unified appreciation of how different group-living species coordinate their behaviors and why natural selection has produced these groups. For the first time, the book combines traditional approaches to behavioral ecology with ideas about self-organization and complex systems from physics and mathematics. Sumpter offers a guide for working with key models in this area along with case studies of their application, and he shows how ideas about animal behavior can be applied to understanding human social behavior. Containing a wealth of accessible examples as well as qualitative and quantitative features, Collective Animal Behavior will interest behavioral ecologists and all scientists studying complex systems.},
	language = {en},
	urldate = {2025-03-27},
	publisher = {Princeton University Press},
	author = {Sumpter, David J. T.},
	month = sep,
	year = {2010},
	doi = {10.1515/9781400837106},
	keywords = {Foraging, Accuracy and precision, Addition, Ant, Approximation, Army ant, Attendance, Bark beetle, Bifurcation theory, Co-operation (evolution), Collective animal behavior, Collective behavior, Collective motion, Commodity, Competition, Cooperation, Cost–benefit analysis, Counterintuitive, Customer, Decision-making, Differential equation, Dimension, Emergence, Estimation, Eusociality, Evolution, Evolutionarily stable strategy, Experimental data, Explanation, Fitness function, Honey bee, Inclusive fitness, Initial condition, Insect, Mathematics, Nonlinear system, Normal distribution, Odor, Parameter, Parasitism, Phenomenon, Pheromone, Poisson distribution, Population model, Positive feedback, Power law, Predation, Prediction, Preferential attachment, Probability, Proportionality (mathematics), Quantity, Rational agent, Reproductive success, Requirement, Self-organization, Shoaling and schooling, Small number, Social behavior, Stable distribution, Standard deviation, Supply (economics), Supply and demand, Termite, Time evolution, Time series, Toy model, Trade-off, Waggle dance, Worker bee},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/YIVDL73A/Sumpter - 2010 - Collective Animal Behavior.pdf:application/pdf},
}

@article{pruitt_social_2018,
	title = {Social tipping points in animal societies},
	volume = {285},
	url = {https://royalsocietypublishing.org/doi/10.1098/rspb.2018.1282},
	doi = {10.1098/rspb.2018.1282},
	abstract = {Animal social groups are complex systems that are likely to exhibit tipping points—which are defined as drastic shifts in the dynamics of systems that arise from small changes in environmental conditions—yet this concept has not been carefully applied to these systems. Here, we summarize the concepts behind tipping points and describe instances in which they are likely to occur in animal societies. We also offer ways in which the study of social tipping points can open up new lines of inquiry in behavioural ecology and generate novel questions, methods, and approaches in animal behaviour and other fields, including community and ecosystem ecology. While some behaviours of living systems are hard to predict, we argue that probing tipping points across animal societies and across tiers of biological organization—populations, communities, ecosystems—may help to reveal principles that transcend traditional disciplinary boundaries.},
	number = {1887},
	urldate = {2025-03-27},
	journal = {Proceedings of the Royal Society B: Biological Sciences},
	author = {Pruitt, Jonathan N. and Berdahl, Andrew and Riehl, Christina and Pinter-Wollman, Noa and Moeller, Holly V. and Pringle, Elizabeth G. and Aplin, Lucy M. and Robinson, Elva J. H. and Grilli, Jacopo and Yeh, Pamela and Savage, Van M. and Price, Michael H. and Garland, Joshua and Gilby, Ian C. and Crofoot, Margaret C. and Doering, Grant N. and Hobson, Elizabeth A.},
	month = sep,
	year = {2018},
	note = {Publisher: Royal Society},
	keywords = {collapse, complex system, cooperation, critical point, hysteresis, social network},
	pages = {20181282},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/PSXU5GUZ/Pruitt et al. - 2018 - Social tipping points in animal societies.pdf:application/pdf},
}

@article{torney_single_2018,
	title = {From single steps to mass migration: the problem of scale in the movement ecology of the {Serengeti} wildebeest},
	volume = {373},
	shorttitle = {From single steps to mass migration},
	url = {https://royalsocietypublishing.org/doi/10.1098/rstb.2017.0012},
	doi = {10.1098/rstb.2017.0012},
	abstract = {A central question in ecology is how to link processes that occur over different scales. The daily interactions of individual organisms ultimately determine community dynamics, population fluctuations and the functioning of entire ecosystems. Observations of these multiscale ecological processes are constrained by various technological, biological or logistical issues, and there are often vast discrepancies between the scale at which observation is possible and the scale of the question of interest. Animal movement is characterized by processes that act over multiple spatial and temporal scales. Second-by-second decisions accumulate to produce annual movement patterns. Individuals influence, and are influenced by, collective movement decisions, which then govern the spatial distribution of populations and the connectivity of meta-populations. While the field of movement ecology is experiencing unprecedented growth in the availability of movement data, there remain challenges in integrating observations with questions of ecological interest. In this article, we present the major challenges of addressing these issues within the context of the Serengeti wildebeest migration, a keystone ecological phenomena that crosses multiple scales of space, time and biological complexity.
This article is part of the theme issue 'Collective movement ecology'.},
	number = {1746},
	urldate = {2025-03-27},
	journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
	author = {Torney, Colin J. and Hopcraft, J. Grant C. and Morrison, Thomas A. and Couzin, Iain D. and Levin, Simon A.},
	month = mar,
	year = {2018},
	note = {Publisher: Royal Society},
	keywords = {migration, scale, wildebeest},
	pages = {20170012},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/7HYXJJ38/Torney et al. - 2018 - From single steps to mass migration the problem o.pdf:application/pdf},
}

@article{portugal_bird_2020,
	title = {Bird flocks},
	volume = {30},
	issn = {0960-9822},
	url = {https://www.sciencedirect.com/science/article/pii/S0960982220300130},
	doi = {10.1016/j.cub.2020.01.013},
	number = {5},
	urldate = {2025-03-27},
	journal = {Current Biology},
	author = {Portugal, Steven J.},
	month = mar,
	year = {2020},
	pages = {R206--R210},
}

@article{bejan_unifying_2006,
	title = {Unifying constructal theory for scale effects in running, swimming and flying},
	volume = {209},
	issn = {0022-0949},
	url = {https://doi.org/10.1242/jeb.01974},
	doi = {10.1242/jeb.01974},
	abstract = {Biologists have treated the view that fundamental differences exist between running, flying and swimming as evident, because the forms of locomotion and the animals are so different: limbs and wings vs body undulations,neutrally buoyant vs weighted bodies, etc. Here we show that all forms of locomotion can be described by a single physics theory. The theory is an invocation of the principle that flow systems evolve in such a way that they destroy minimum useful energy (exergy, food). This optimization approach delivers in surprisingly direct fashion the observed relations between speed and body mass (Mb) raised to 1/6, and between frequency(stride, flapping) and {\textbackslash}batchmode {\textbackslash}documentclass[fleqn,10pt,legalpaper]\{article\} {\textbackslash}usepackage\{amssymb\} {\textbackslash}usepackage\{amsfonts\} {\textbackslash}usepackage\{amsmath\} {\textbackslash}pagestyle\{empty\} {\textbackslash}begin\{document\} {\textbackslash}(M\_\{{\textbackslash}mathrm\{b\}\}{\textasciicircum}\{-1\{/\}6\}{\textbackslash}) {\textbackslash}end\{document\}, and shows why these relations hold for running, flying and swimming. Animal locomotion is an optimized two-step intermittency: an optimal balance is achieved between the vertical loss of useful energy (lifting the body weight,which later drops), and the horizontal loss caused by friction against the surrounding medium. The theory predicts additional features of animal design:the Strouhal number constant, which holds for running as well as flying and swimming, the proportionality between force output and mass in animal motors,and the fact that undulating swimming and flapping flight occur only if the body Reynolds number exceeds approximately 30. This theory, and the general body of work known as constructal theory, together now show that animal movement (running, flying, swimming) and fluid eddy movement (turbulent structure) are both forms of optimized intermittent movement.},
	number = {2},
	urldate = {2025-03-27},
	journal = {Journal of Experimental Biology},
	author = {Bejan, Adrian and Marden, James H.},
	month = jan,
	year = {2006},
	pages = {238--248},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/QESJE9RV/Bejan and Marden - 2006 - Unifying constructal theory for scale effects in r.pdf:application/pdf},
}

@article{farine_collective_2021,
	title = {Collective behaviour: {Movement} rules under imminent threat},
	volume = {31},
	issn = {0960-9822},
	shorttitle = {Collective behaviour},
	url = {https://www.sciencedirect.com/science/article/pii/S0960982221007417},
	doi = {10.1016/j.cub.2021.05.043},
	abstract = {Evading predator attacks requires making rapid decisions. A new study has found that instead of moving towards others, as predicted by classical models of anti-predator behaviour, homing pigeons move away from their flock when faced with an imminent attack.},
	number = {14},
	urldate = {2025-03-27},
	journal = {Current Biology},
	author = {Farine, Damien R.},
	month = jul,
	year = {2021},
	pages = {R902--R904},
	file = {Full Text:/Users/nsirmpilatze/Zotero/storage/4E57IX3S/Farine - 2021 - Collective behaviour Movement rules under imminen.pdf:application/pdf},
}

@article{delcourt_collective_2016,
	title = {Collective {Vortex} {Behaviors}: {Diversity}, {Proximate}, and {Ultimate} {Causes} of {Circular} {Animal} {Group} {Movements}},
	volume = {91},
	issn = {0033-5770},
	shorttitle = {Collective {Vortex} {Behaviors}},
	url = {https://www.journals.uchicago.edu/doi/10.1086/685301},
	doi = {10.1086/685301},
	abstract = {Ant mill, caterpillar circle, bat doughnut, amphibian vortex, duck swirl, and fish torus are different names for rotating circular animal formations, where individuals turn around a common center. These “collective vortex behaviors” occur at different group sizes from pairs to several million individuals and have been reported in a large number of organisms, from bacteria to vertebrates, including humans. However, to date, no comprehensive review and synthesis of the literature on vortex behaviors has been conducted. Here, we review the state of the art of the proximate and ultimate causes of vortex behaviors. The ubiquity of this behavioral phenomenon could suggest common causes or fundamental underlying principles across contexts. However, we find that a variety of proximate mechanisms give rise to vortex behaviors. We highlight the potential benefits of collective vortex behaviors to individuals involved in them. For example, in some species, vortices increase feeding efficiency and could give protection against predators. It has also been argued that vortices could improve collective decision-making and information transfer. We highlight gaps in our understanding of these ubiquitous behavioral phenomena and discuss future directions for research in vortex studies.},
	number = {1},
	urldate = {2025-03-27},
	journal = {The Quarterly Review of Biology},
	author = {Delcourt, Johann and Bode, Nikolai W. F. and Denoël, Mathieu},
	month = mar,
	year = {2016},
	note = {Publisher: The University of Chicago Press},
	keywords = {collective motion, collective behavior, group behavior, milling behavior, self-organization, torus},
	pages = {1--24},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/MZD9JYY2/Delcourt et al. - 2016 - Collective Vortex Behaviors Diversity, Proximate,.pdf:application/pdf},
}

@article{papadopoulou_self-organization_2022,
	title = {Self-organization of collective escape in pigeon flocks},
	volume = {18},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1009772},
	doi = {10.1371/journal.pcbi.1009772},
	abstract = {Bird flocks under predation demonstrate complex patterns of collective escape. These patterns may emerge by self-organization from local interactions among group-members. Computational models have been shown to be valuable for identifying what behavioral rules may govern such interactions among individuals during collective motion. However, our knowledge of such rules for collective escape is limited by the lack of quantitative data on bird flocks under predation in the field. In the present study, we analyze the first GPS trajectories of pigeons in airborne flocks attacked by a robotic falcon in order to build a species-specific model of collective escape. We use our model to examine a recently identified distance-dependent pattern of collective behavior: the closer the prey is to the predator, the higher the frequency with which flock members turn away from it. We first extract from the empirical data of pigeon flocks the characteristics of their shape and internal structure (bearing angle and distance to nearest neighbors). Combining these with information on their coordination from the literature, we build an agent-based model adjusted to pigeons’ collective escape. We show that the pattern of turning away from the predator with increased frequency when the predator is closer arises without prey prioritizing escape when the predator is near. Instead, it emerges through self-organization from a behavioral rule to avoid the predator independently of their distance to it. During this self-organization process, we show how flock members increase their consensus over which direction to escape and turn collectively as the predator gets closer. Our results suggest that coordination among flock members, combined with simple escape rules, reduces the cognitive costs of tracking the predator while flocking. Such escape rules that are independent of the distance to the predator can now be investigated in other species. Our study showcases the important role of computational models in the interpretation of empirical findings of collective behavior.},
	language = {en},
	number = {1},
	urldate = {2025-03-27},
	journal = {PLOS Computational Biology},
	author = {Papadopoulou, Marina and Hildenbrandt, Hanno and Sankey, Daniel W. E. and Portugal, Steven J. and Hemelrijk, Charlotte K.},
	month = jan,
	year = {2022},
	note = {Publisher: Public Library of Science},
	keywords = {Collective animal behavior, Predation, Animal behavior, Bird flight, Birds, Homing behavior, Pigeons, Simulation and modeling},
	pages = {e1009772},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/3267YD2S/Papadopoulou et al. - 2022 - Self-organization of collective escape in pigeon f.pdf:application/pdf},
}

@article{papadopoulou_emergence_2022,
	title = {Emergence of splits and collective turns in pigeon flocks under predation},
	volume = {9},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsos.211898},
	doi = {10.1098/rsos.211898},
	abstract = {Complex patterns of collective behaviour may emerge through self-organization, from local interactions among individuals in a group. To understand what behavioural rules underlie these patterns, computational models are often necessary. These rules have not yet been systematically studied for bird flocks under predation. Here, we study airborne flocks of homing pigeons attacked by a robotic falcon, combining empirical data with a species-specific computational model of collective escape. By analysing GPS trajectories of flocking individuals, we identify two new patterns of collective escape: early splits and collective turns, occurring even at large distances from the predator. To examine their formation, we extend an agent-based model of pigeons with a ‘discrete’ escape manoeuvre by a single initiator, namely a sudden turn interrupting the continuous coordinated motion of the group. Both splits and collective turns emerge from this rule. Their relative frequency depends on the angular velocity and position of the initiator in the flock: sharp turns by individuals at the periphery lead to more splits than collective turns. We confirm this association in the empirical data. Our study highlights the importance of discrete and uncoordinated manoeuvres in the collective escape of bird flocks and advocates the systematic study of their patterns across species.},
	number = {2},
	urldate = {2025-03-27},
	journal = {Royal Society Open Science},
	author = {Papadopoulou, Marina and Hildenbrandt, Hanno and Sankey, Daniel W. E. and Portugal, Steven J. and Hemelrijk, Charlotte K.},
	month = feb,
	year = {2022},
	note = {Publisher: Royal Society},
	keywords = {collective behaviour, self-organization, escape patterns, flocking, pigeon},
	pages = {211898},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/RAA3NLNS/Papadopoulou et al. - 2022 - Emergence of splits and collective turns in pigeon.pdf:application/pdf},
}

@article{shellard_rules_2020,
	title = {Rules of collective migration: from the wildebeest to the neural crest},
	volume = {375},
	shorttitle = {Rules of collective migration},
	url = {https://royalsocietypublishing.org/doi/10.1098/rstb.2019.0387},
	doi = {10.1098/rstb.2019.0387},
	abstract = {Collective migration, the movement of groups in which individuals affect the behaviour of one another, occurs at practically every scale, from bacteria up to whole species' populations. Universal principles of collective movement can be applied at all levels. In this review, we will describe the rules governing collective motility, with a specific focus on the neural crest, an embryonic stem cell population that undergoes extensive collective migration during development. We will discuss how the underlying principles of individual cell behaviour, and those that emerge from a supracellular scale, can explain collective migration.
This article is part of the theme issue ‘Multi-scale analysis and modelling of collective migration in biological systems’.},
	number = {1807},
	urldate = {2025-03-27},
	journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
	author = {Shellard, Adam and Mayor, Roberto},
	month = jul,
	year = {2020},
	note = {Publisher: Royal Society},
	keywords = {alignment, co-attraction, collective migration, contact inhibition of locomotion, neural crest, supracellular},
	pages = {20190387},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/KNULQDHP/Shellard and Mayor - 2020 - Rules of collective migration from the wildebeest.pdf:application/pdf},
}

@article{landeau_oddity_1986,
	title = {Oddity and the ‘confusion effect’ in predation},
	volume = {34},
	issn = {0003-3472},
	url = {https://www.sciencedirect.com/science/article/pii/S0003347286802081},
	doi = {10.1016/S0003-3472(86)80208-1},
	abstract = {We report on two sets of experiments designed to clarify the roles of sensory ‘confusion’ and prey ‘oddity’ as they interact to influence the hunting success of a pursuit predator, the largemouth bass (Micropterus salmoides), on silvery minnows (Hybognathus nuchalis). Bass quickly captured solitary minnows, but performed many unsuccessful attacks and took much longer to make a capture as prey school size was increased. At school sizes of eight and above, bass were effectively stymied, demonstrating the ‘confusion effect’. The inclusion of one or two ‘odd’ (blue-dyed) minnows in a school of eight greatly increased the ability of bass to capture both normal and odd prey, but this effect of oddity disappeared at a school size of 15. The implications of these results for understanding the adaptive basis of mixed species flocks, herds and schools is discussed.},
	number = {5},
	urldate = {2025-03-27},
	journal = {Animal Behaviour},
	author = {Landeau, Laurie and Terborgh, John},
	month = oct,
	year = {1986},
	pages = {1372--1380},
}

@article{lukeman_inferring_2010,
	title = {Inferring individual rules from collective behavior},
	volume = {107},
	url = {https://www.pnas.org/doi/full/10.1073/pnas.1001763107},
	doi = {10.1073/pnas.1001763107},
	abstract = {Social organisms form striking aggregation patterns, displaying cohesion, polarization, and collective intelligence. Determining how they do so in nature is challenging; a plethora of simulation studies displaying life-like swarm behavior lack rigorous comparison with actual data because collecting field data of sufficient quality has been a bottleneck. Here, we bridge this gap by gathering and analyzing a high-quality dataset of flocking surf scoters, forming well-spaced groups of hundreds of individuals on the water surface. By reconstructing each individual's position, velocity, and trajectory, we generate spatial and angular neighbor-distribution plots, revealing distinct concentric structure in positioning, a preference for neighbors directly in front, and strong alignment with neighbors on each side. We fit data to zonal interaction models and characterize which individual interaction forces suffice to explain observed spatial patterns. Results point to strong short-range repulsion, intermediate-range alignment, and longer-range attraction (with circular zones), as well as a weak but significant frontal-sector interaction with one neighbor. A best-fit model with such interactions accounts well for observed group structure, whereas absence or alteration in any one of these rules fails to do so. We find that important features of observed flocking surf scoters can be accounted for by zonal models with specific, well-defined rules of interaction.},
	number = {28},
	urldate = {2025-03-27},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Lukeman, Ryan and Li, Yue-Xian and Edelstein-Keshet, Leah},
	month = jul,
	year = {2010},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	pages = {12576--12580},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/9AJMV2CA/Lukeman et al. - 2010 - Inferring individual rules from collective behavio.pdf:application/pdf},
}

@article{ballerini_interaction_2008,
	title = {Interaction ruling animal collective behavior depends on topological rather than metric distance: {Evidence} from a field study},
	volume = {105},
	shorttitle = {Interaction ruling animal collective behavior depends on topological rather than metric distance},
	url = {https://www.pnas.org/doi/full/10.1073/pnas.0711437105},
	doi = {10.1073/pnas.0711437105},
	abstract = {Numerical models indicate that collective animal behavior may emerge from simple local rules of interaction among the individuals. However, very little is known about the nature of such interaction, so that models and theories mostly rely on aprioristic assumptions. By reconstructing the three-dimensional positions of individual birds in airborne flocks of a few thousand members, we show that the interaction does not depend on the metric distance, as most current models and theories assume, but rather on the topological distance. In fact, we discovered that each bird interacts on average with a fixed number of neighbors (six to seven), rather than with all neighbors within a fixed metric distance. We argue that a topological interaction is indispensable to maintain a flock's cohesion against the large density changes caused by external perturbations, typically predation. We support this hypothesis by numerical simulations, showing that a topological interaction grants significantly higher cohesion of the aggregation compared with a standard metric one.},
	number = {4},
	urldate = {2025-03-27},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Ballerini, M. and Cabibbo, N. and Candelier, R. and Cavagna, A. and Cisbani, E. and Giardina, I. and Lecomte, V. and Orlandi, A. and Parisi, G. and Procaccini, A. and Viale, M. and Zdravkovic, V.},
	month = jan,
	year = {2008},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	pages = {1232--1237},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/IRPJKZFI/Ballerini et al. - 2008 - Interaction ruling animal collective behavior depe.pdf:application/pdf},
}

@article{strandburg-peshkin_visual_2013,
	title = {Visual sensory networks and effective information transfer in animal groups},
	volume = {23},
	issn = {0960-9822},
	url = {https://www.sciencedirect.com/science/article/pii/S0960982213009202},
	doi = {10.1016/j.cub.2013.07.059},
	abstract = {Social transmission of information is vital for many group-living animals, allowing coordination of motion and effective response to complex environments. Revealing the interaction networks underlying information flow within these groups is a central challenge [1]. Previous work has modeled interactions between individuals based directly on their relative spatial positions: each individual is considered to interact with all neighbors within a fixed distance (metric range [2]), a fixed number of nearest neighbors (topological range [3]), a ‘shell’ of near neighbors (Voronoi range [4]), or some combination (Figure 1A). However, conclusive evidence to support these assumptions is lacking. Here, we employ a novel approach that considers individual movement decisions to be based explicitly on the sensory information available to the organism. In other words, we consider that while spatial relations do inform interactions between individuals, they do so indirectly, through individuals’ detection of sensory cues. We reconstruct computationally the visual field of each individual throughout experiments designed to investigate information propagation within fish schools (golden shiners, Notemigonus crysoleucas). Explicitly considering visual sensing allows us to more accurately predict the propagation of behavioral change in these groups during leadership events. Furthermore, we find that structural properties of visual interaction networks differ markedly from those of metric and topological counterparts, suggesting that previous assumptions may not appropriately reflect information flow in animal groups.},
	number = {17},
	urldate = {2025-03-27},
	journal = {Current Biology},
	author = {Strandburg-Peshkin, Ariana and Twomey, Colin R. and Bode, Nikolai W. F. and Kao, Albert B. and Katz, Yael and Ioannou, Christos C. and Rosenthal, Sara B. and Torney, Colin J. and Wu, Hai Shan and Levin, Simon A. and Couzin, Iain D.},
	month = sep,
	year = {2013},
	pages = {R709--R711},
	file = {Accepted Version:/Users/nsirmpilatze/Zotero/storage/IAYANG5G/Strandburg-Peshkin et al. - 2013 - Visual sensory networks and effective information .pdf:application/pdf;ScienceDirect Snapshot:/Users/nsirmpilatze/Zotero/storage/HARIKNDA/S0960982213009202.html:text/html},
}

@article{hamilton_geometry_1971,
	title = {Geometry for the selfish herd},
	volume = {31},
	issn = {0022-5193},
	url = {https://www.sciencedirect.com/science/article/pii/0022519371901895},
	doi = {10.1016/0022-5193(71)90189-5},
	abstract = {This paper presents an antithesis to the view that gregarious behaviour is evolved through benefits to the population or species. Following Galton (1871) and Williams (1964) gregarious behaviour is considered as a form of cover-seeking in which each animal tries to reduce its chance of being caught by a predator. It is easy to see how pruning of marginal individuals can maintain centripetal instincts in already gregarious species; some evidence that marginal pruning actually occurs is summarized. Besides this, simply defined models are used to show that even in non-gregarious species selection is likely to favour individuals who stay close to others. Although not universal or unipotent, cover-seeking is a widespread and important element in animal aggregation, as the literature shows. Neglect of the idea has probably followed from a general disbelief that evolution can be dysgenic for a species. Nevertheless, selection theory provides no support for such disbelief in the case of species with outbreeding or unsubdivided populations. The model for two dimensions involves a complex problem in geometrical probability which has relevance also in metallurgy and communication science. Some empirical data on this, gathered from random number plots, is presented as of possible heuristic value.},
	number = {2},
	urldate = {2025-03-27},
	journal = {Journal of Theoretical Biology},
	author = {Hamilton, W. D.},
	month = may,
	year = {1971},
	pages = {295--311},
	file = {ScienceDirect Snapshot:/Users/nsirmpilatze/Zotero/storage/K9WIAF2U/0022519371901895.html:text/html},
}

@article{sumpter_information_2008,
	title = {Information transfer in moving animal groups},
	volume = {127},
	issn = {1611-7530},
	url = {https://doi.org/10.1007/s12064-008-0040-1},
	doi = {10.1007/s12064-008-0040-1},
	abstract = {Moving animal groups provide some of the most intriguing and difficult to characterise examples of collective behaviour. We review some recent (and not so recent) empirical research on the motion of animal groups, including fish, locusts and homing pigeons. An important concept which unifies our understanding of these groups is that of transfer of directional information. Individuals which change their direction of travel in response to the direction taken by their near neighbours can quickly transfer information about the presence of a predatory threat or food source. We show that such information transfer is optimised when the density of individuals in a group is close to that at which a phase transition occurs between random and ordered motion. Similarly, we show that even relatively small differences in information possessed by group members can lead to strong collective-level decisions for one of two options. By combining the use of self-propelled particle and social force models of collective motion with thinking about the evolution of flocking we aim to better understand how complexity arises within these groups.},
	language = {en},
	number = {2},
	urldate = {2025-03-27},
	journal = {Theory in Biosciences},
	author = {Sumpter, David and Buhl, Camille and Biro, Dora and Couzin, Iain},
	month = may,
	year = {2008},
	keywords = {Collective Motion, Information Transfer, Intermediate Density, Maximum Attraction, Phase Transition},
	pages = {177--186},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/DWXQK8ZA/Sumpter et al. - 2008 - Information transfer in moving animal groups.pdf:application/pdf},
}

@article{lemasson_collective_2009,
	title = {Collective motion in animal groups from a neurobiological perspective: {The} adaptive benefits of dynamic sensory loads and selective attention},
	volume = {261},
	issn = {0022-5193},
	shorttitle = {Collective motion in animal groups from a neurobiological perspective},
	url = {https://www.sciencedirect.com/science/article/pii/S0022519309003658},
	doi = {10.1016/j.jtbi.2009.08.013},
	abstract = {We explore mechanisms associated with collective animal motion by drawing on the neurobiological bases of sensory information processing and decision-making. The model uses simplified retinal processes to translate neighbor movement patterns into information through spatial signal integration and threshold responses. The structure provides a mechanism by which individuals can vary their sets of influential neighbors, a measure of an individual's sensory load. Sensory loads are correlated with group order and density, and we discuss their adaptive values in an ecological context. The model also provides a mechanism by which group members can identify, and rapidly respond to, novel visual stimuli.},
	number = {4},
	urldate = {2025-03-27},
	journal = {Journal of Theoretical Biology},
	author = {Lemasson, B. H. and Anderson, J. J. and Goodwin, R. A.},
	month = dec,
	year = {2009},
	keywords = {Vision, Collective behavior, Decision-making, Information, Retina, Trafalgar effect},
	pages = {501--510},
	file = {ScienceDirect Snapshot:/Users/nsirmpilatze/Zotero/storage/TB3L6T88/S0022519309003658.html:text/html},
}

@article{king_selfish-herd_2012,
	title = {Selfish-herd behaviour of sheep under threat},
	volume = {22},
	issn = {0960-9822},
	url = {https://www.sciencedirect.com/science/article/pii/S0960982212005295},
	doi = {10.1016/j.cub.2012.05.008},
	abstract = {Flocking is a striking example of collective behaviour that is found in insect swarms, fish schools and mammal herds [1]. A major factor in the evolution of flocking behaviour is thought to be predation, whereby larger and/or more cohesive groups are better at detecting predators (as, for example, in the ‘many eyes theory’), and diluting the effects of predators (as in the ‘selfish-herd theory’) than are individuals in smaller and/or dispersed groups [2]. The former theory assumes that information (passively or actively transferred) can be disseminated more effectively in larger/cohesive groups, while the latter assumes that there are spatial benefits to individuals in a large group, since individuals can alter their spatial position relative to their group-mates and any potential predator, thus reducing their predation risk [3]. We used global positioning system (GPS) data to characterise the response of a group of ‘prey’ animals (a flock of sheep) to an approaching ‘predator’ (a herding dog). Analyses of relative sheep movement trajectories showed that sheep exhibit a strong attraction towards the centre of the flock under threat, a pattern that we could re-create using a simple model. These results support the long-standing assertion that individuals can respond to potential danger by moving towards the centre of a fleeing group [2].},
	number = {14},
	urldate = {2025-03-27},
	journal = {Current Biology},
	author = {King, Andrew J. and Wilson, Alan M. and Wilshin, Simon D. and Lowe, John and Haddadi, Hamed and Hailes, Stephen and Morton, A. Jennifer},
	month = jul,
	year = {2012},
	pages = {R561--R562},
	file = {ScienceDirect Snapshot:/Users/nsirmpilatze/Zotero/storage/NG8XCVAV/S0960982212005295.html:text/html;Submitted Version:/Users/nsirmpilatze/Zotero/storage/9KIQJ8GW/King et al. - 2012 - Selfish-herd behaviour of sheep under threat.pdf:application/pdf},
}

@article{weimerskirch_energy_2001,
	title = {Energy saving in flight formation},
	volume = {413},
	copyright = {2001 Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/35099670},
	doi = {10.1038/35099670},
	abstract = {Pelicans flying in a 'V' can glide for extended periods using the other birds' air streams.},
	language = {en},
	number = {6857},
	urldate = {2025-03-27},
	journal = {Nature},
	author = {Weimerskirch, Henri and Martin, Julien and Clerquin, Yannick and Alexandre, Peggy and Jiraskova, Sarka},
	month = oct,
	year = {2001},
	note = {Publisher: Nature Publishing Group},
	keywords = {Humanities and Social Sciences, multidisciplinary, Science},
	pages = {697--698},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/7LMHXZER/Weimerskirch et al. - 2001 - Energy saving in flight formation.pdf:application/pdf},
}

@article{herbert-read_initiation_2015,
	title = {Initiation and spread of escape waves within animal groups},
	volume = {2},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsos.140355},
	doi = {10.1098/rsos.140355},
	abstract = {The exceptional reactivity of animal collectives to predatory attacks is thought to be owing to rapid, but local, transfer of information between group members. These groups turn together in unison and produce escape waves. However, it is not clear how escape waves are created from local interactions, nor is it understood how these patterns are shaped by natural selection. By startling schools of fish with a simulated attack in an experimental arena, we demonstrate that changes in the direction and speed by a small percentage of individuals that detect the danger initiate an escape wave. This escape wave consists of a densely packed band of individuals that causes other school members to change direction. In the majority of cases, this wave passes through the entire group. We use a simulation model to demonstrate that this mechanism can, through local interactions alone, produce arbitrarily large escape waves. In the model, when we set the group density to that seen in real fish schools, we find that the risk to the members at the edge of the group is roughly equal to the risk of those within the group. Our experiments and modelling results provide a plausible explanation for how escape waves propagate in nature without centralized control.},
	number = {4},
	urldate = {2025-03-27},
	journal = {Royal Society Open Science},
	author = {Herbert-Read, James E. and Buhl, Camille and Hu, Feng and Ward, Ashley J. W. and Sumpter, David J. T.},
	month = apr,
	year = {2015},
	note = {Publisher: Royal Society},
	keywords = {self-organization, collective animal behaviour, escape waves, fish schools},
	pages = {140355},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/5ATZW2PQ/Herbert-Read et al. - 2015 - Initiation and spread of escape waves within anima.pdf:application/pdf},
}

@article{sasaki_emergence_2011,
	title = {Emergence of group rationality from irrational individuals},
	volume = {22},
	issn = {1045-2249},
	url = {https://doi.org/10.1093/beheco/arq198},
	doi = {10.1093/beheco/arq198},
	abstract = {Evolutionary theory predicts that animal decision makers should be rational, meaning that they consistently choose fitness-maximizing options. Despite this, violations of rationality have been found repeatedly in humans and other animals. The significance of these violations remains controversial, but many explanations point to cognitive limitations that prevent animals from adequately processing the information needed for fully rational choice. Instead, they rely on heuristics that usually work well but yield systematic errors in specific contexts. Although past research on rationality has focused on individuals, many highly integrated groups, such as ant colonies, regularly make consensus choices among food sources, nest sites, or other options. These collective choices emerge from local interactions among many group members, none of whom take on the whole burden of decision making. We hypothesized that groups may evade the irrational consequences of individual limitations by distributing their decision making across many group members. We tested this in the well-studied case of collective nest-site selection by Temnothorax ants. We found that individual ants, but not colonies, strongly violated rationality when presented with a challenging nest-site choice. Specifically, isolated individuals irrationally switched their preference between 2 alternative nest sites based on their experience of an unattractive decoy. Given the same choice, intact colonies maintained consistent preferences regardless of the decoy's presence. Previous studies have stressed how distributed decision making can filter out random errors made by group members. Our results show that collectives can also suppress systematic errors that emerge from the decision heuristics of cognitively limited individuals.},
	number = {2},
	urldate = {2025-03-27},
	journal = {Behavioral Ecology},
	author = {Sasaki, Takao and Pratt, Stephen C.},
	month = mar,
	year = {2011},
	pages = {276--281},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/YVP6G5NF/Sasaki and Pratt - 2011 - Emergence of group rationality from irrational ind.pdf:application/pdf;Snapshot:/Users/nsirmpilatze/Zotero/storage/6UM8M2C6/207435.html:text/html},
}

@article{couzin_effective_2005,
	title = {Effective leadership and decision-making in animal groups on the move},
	volume = {433},
	copyright = {2004 Macmillan Magazines Ltd.},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature03236},
	doi = {10.1038/nature03236},
	abstract = {Moving groups of animals, including fish, ungulates, birds and honeybee swarms seem able to take complex decisions in the absence of signalling mechanisms, and when group members cannot establish who has or has not got information. A numerical simulation shows how such groups can make accurate consensus decisions, and that the larger the group, the smaller the proportion of informed individuals needed to guide the group. A very small proportion of informed individuals is sufficient for near maximal accuracy. This has implications for our understanding of the evolution of information transfer in groups, and also suggests a new design protocol for the guidance of grouping robots. Cover photo, by Phillip Colla Natural History Photography (http://www.OceanLight.com), shows schooling jack mackerel.},
	language = {en},
	number = {7025},
	urldate = {2025-03-27},
	journal = {Nature},
	author = {Couzin, Iain D. and Krause, Jens and Franks, Nigel R. and Levin, Simon A.},
	month = feb,
	year = {2005},
	note = {Publisher: Nature Publishing Group},
	keywords = {Humanities and Social Sciences, multidisciplinary, Science},
	pages = {513--516},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/2LM2XLL4/Couzin et al. - 2005 - Effective leadership and decision-making in animal.pdf:application/pdf},
}

@article{couzin_uninformed_2011,
	title = {Uninformed {Individuals} {Promote} {Democratic} {Consensus} in {Animal} {Groups}},
	volume = {334},
	url = {https://www.science.org/doi/10.1126/science.1210280},
	doi = {10.1126/science.1210280},
	abstract = {Conflicting interests among group members are common when making collective decisions, yet failure to achieve consensus can be costly. Under these circumstances individuals may be susceptible to manipulation by a strongly opinionated, or extremist, minority. It has previously been argued, for humans and animals, that social groups containing individuals who are uninformed, or exhibit weak preferences, are particularly vulnerable to such manipulative agents. Here, we use theory and experiment to demonstrate that, for a wide range of conditions, a strongly opinionated minority can dictate group choice, but the presence of uninformed individuals spontaneously inhibits this process, returning control to the numerical majority. Our results emphasize the role of uninformed individuals in achieving democratic consensus amid internal group conflict and informational constraints.},
	number = {6062},
	urldate = {2025-03-27},
	journal = {Science},
	author = {Couzin, Iain D. and Ioannou, Christos C. and Demirel, Güven and Gross, Thilo and Torney, Colin J. and Hartnett, Andrew and Conradt, Larissa and Levin, Simon A. and Leonard, Naomi E.},
	month = dec,
	year = {2011},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {1578--1580},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/4MJFRZKJ/Couzin et al. - 2011 - Uninformed Individuals Promote Democratic Consensu.pdf:application/pdf},
}

@article{gueron_self-organization_1993,
	title = {Self-organization of {Front} {Patterns} in {Large} {Wildebeest} {Herds}},
	volume = {165},
	issn = {0022-5193},
	url = {https://www.sciencedirect.com/science/article/pii/S0022519383712063},
	doi = {10.1006/jtbi.1993.1206},
	abstract = {Aerial photographs of migrating wildebeest herds reveal striking distributional patterns, including wavy fronts. These patterns vary over scales that are much larger than the individual's perceptual range, and thus cannot be explained simply as random fluctuations on uniformity. Furthermore, since the individual is only aware of its immediate surroundings, broad-range patterns must be explained in terms of local decisions. This paper suggests a model for the dynamics of large herds and a mechanism for their self-organizing pattern formation. We overcome the problem of modeling a two-dimensional distribution of a large population by considering only the leading layer. The conditions under which uniform fronts are stable (or unstable) are analyzed. In the latter case, small perturbations on uniformity evolve to large-scale patterns, as we demonstrate by computer simulations. We suggest this as a possible mechanism for spontaneous generation of long-range patterns.},
	number = {4},
	urldate = {2025-03-27},
	journal = {Journal of Theoretical Biology},
	author = {Gueron, Shay and Levin, Simon A.},
	month = dec,
	year = {1993},
	pages = {541--552},
	file = {ScienceDirect Snapshot:/Users/nsirmpilatze/Zotero/storage/IUYGQ39K/S0022519383712063.html:text/html},
}

@article{viscido_dilemma_2002,
	title = {The {Dilemma} of the {Selfish} {Herd}: {The} {Search} for a {Realistic} {Movement} {Rule}},
	volume = {217},
	issn = {0022-5193},
	shorttitle = {The {Dilemma} of the {Selfish} {Herd}},
	url = {https://www.sciencedirect.com/science/article/pii/S0022519302930250},
	doi = {10.1006/jtbi.2002.3025},
	abstract = {The selfish herd hypothesis predicts that aggregations form because individuals move toward one another to minimize their own predation risk. The “dilemma of the selfish herd” is that movement rules that are easy for individuals to follow, fail to produce true aggregations, while rules that produce aggregations require individual behavior so complex that one may doubt most animals can follow them. If natural selection at the individual level is responsible for herding behavior, a solution to the dilemma must exist. Using computer simulations, we examined four different movement rules. Relative predation risk was different for all four movement rules (p{\textless}0.05). We defined three criteria for measuring the quality of a movement rule. A good movement rule should (a) be statistically likely to benefit an individual that follows it, (b) be something we can imagine most animals are capable of following, and (c) result in a centrally compact flock. The local crowded horizon rule, which allowed individuals to take the positions of many flock-mates into account, but decreased the influence of flock-mates with distance, best satisfied these criteria. The local crowded horizon rule was very sensitive to the animal's perceptive ability. Therefore, the animal's ability to detect its neighbors is an important factor in the dynamics of group formation.},
	number = {2},
	urldate = {2025-03-27},
	journal = {Journal of Theoretical Biology},
	author = {Viscido, STEVEN V. and Miller, MATTHEW and Wethey, DAVID S.},
	month = jul,
	year = {2002},
	pages = {183--194},
}

@misc{kamm_central_2025,
	title = {Central infusion of prostaglandin {E2} reveals a unified representation of sickness in the mouse insular cortex},
	copyright = {© 2025, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2025.04.28.651028v1},
	doi = {10.1101/2025.04.28.651028},
	abstract = {During infections, vertebrates develop stereotypic symptoms such as elevated body temperature, reduced appetite, and lethargy. These changes, collectively known as sickness syndrome, are orchestrated by the brain in response to immune mediators released during systemic inflammation. While the roles of subcortical regions, including the hypothalamus and brainstem nuclei, in regulating sickness symptoms are well established, the contribution of the neocortex to the encoding and modulation of the sick state remains less well understood. We examined the neuronal correlates of sickness in the neocortex of awake mice following a single intracerebroventricular (i.c.v.) injection of prostaglandin E2 (PGE2), a well-characterized mediator of sickness. Behavioral analysis revealed that PGE2 elicited a rapid and robust sickness response, characterized by fever, slower locomotion, quiescence, anorexia, and eye squinting. Whole-brain Fos mapping showed that PGE2 generates a distinct neural activation pattern encompassing much of the interoceptive network. Electrophysiological recordings using Neuropixel probes in awake mice revealed that neuronal population dynamics in the insular cortex (IC) and the primary somatosensory cortex (SSp), two regions involved in body state representation, encode sickness-related information, such as body temperature, walking velocity, grooming, and eye squinting. However, unlike SSp, ongoing neuronal activity in IC exhibited a better decoding performance for an integrated measure of sickness rather than individual symptoms. Together, these results suggest that PGE2 induces a coordinated physiological and behavioral response akin to a sick state, which is preferentially encoded in the IC.},
	language = {en},
	urldate = {2025-05-06},
	publisher = {bioRxiv},
	author = {Kamm, Gretel B. and Boffi, Juan C. and Hay, Muad Y. Abd El and Rajot, Domitille and Cukić, Ana and Havenith, Martha N. and Scholvinck, Marieke and Renier, Nicolas and Asari, Hiroki and Prevedel, Robert},
	month = apr,
	year = {2025},
	note = {Pages: 2025.04.28.651028
Section: New Results},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/8RUKJTFB/Kamm et al. - 2025 - Central infusion of prostaglandin E2 reveals a uni.pdf:application/pdf},
}

@article{jeung_motion-bids_2024,
	title = {Motion-{BIDS}: an extension to the brain imaging data structure to organize motion data for reproducible research},
	volume = {11},
	copyright = {2024 The Author(s)},
	issn = {2052-4463},
	shorttitle = {Motion-{BIDS}},
	url = {https://www.nature.com/articles/s41597-024-03559-8},
	doi = {10.1038/s41597-024-03559-8},
	abstract = {We present an extension to the Brain Imaging Data Structure (BIDS) for motion data. Motion data is frequently recorded alongside human brain imaging and electrophysiological data. The goal of Motion-BIDS is to make motion data interoperable across different laboratories and with other data modalities in human brain and behavioral research. To this end, Motion-BIDS standardizes the data format and metadata structure. It describes how to document experimental details, considering the diversity of hardware and software systems for motion data. This promotes findable, accessible, interoperable, and reusable data sharing and Open Science in human motion research.},
	language = {en},
	number = {1},
	urldate = {2025-05-08},
	journal = {Scientific Data},
	author = {Jeung, Sein and Cockx, Helena and Appelhoff, Stefan and Berg, Timotheus and Gramann, Klaus and Grothkopp, Sören and Warmerdam, Elke and Hansen, Clint and Oostenveld, Robert and Welzel, Julius},
	month = jul,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	keywords = {Data publication and archiving, Research management},
	pages = {716},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/VDZCYEK8/Jeung et al. - 2024 - Motion-BIDS an extension to the brain imaging dat.pdf:application/pdf},
}

@article{matthis_freemocap_2022,
	title = {The {FreeMoCap} {Project} - and - {Gaze}/{Hand} coupling during a combined  three-ball juggling and balance task},
	volume = {22},
	issn = {1534-7362},
	url = {https://doi.org/10.1167/jov.22.14.4195},
	doi = {10.1167/jov.22.14.4195},
	abstract = {We present a broad scale, long term, open science endeavor that aims to create a free, open source, low cost, research-grade full-body motion capture system alongside a research project that utilizes this tool to investigate visual motor coupling during a combined three-ball juggling and balance task. The FreeMoCap Project - The computer vision community is making tremendous advances in the field of markerless motion capture software. However, these advances often require a high floor of technical knowledge to be used effectively. This limits their utility to the scientific community and creates a near insurmountable barrier for the general population. The FreeMoCap system leverages emerging markerless motion capture software (e.g. OpenPose, MediaPipe, DeepLabCut, etc) to create a streamlined ‘one-click’ pipeline for 3D kinematic reconstruction of full-body human, animal, and robotic movement. This system works with arbitrary camera hardware and provides methods for synchronous recording of wired cameras (e.g. USB webcams) as well as the post-hoc synchronization of independent cameras (e.g. GoPros). The FreeMoCap project emphasizes ease-of-use, with the eventual goal of developing a system that will allow a 14-year-old with no technical training and no outside assistance to recreate a research-grade motion capture system for less than 100 US Dollars. Juggling/Balance Task - The FreeMoCap system was used to record a subject performing a three-ball juggling task while balancing on a ‘wobble-board’ platform. The full body (and hand) kinematic data produced by FreeMoCap was spatiotemporally calibrated with binocular gaze data recorded by a Pupil Labs mobile eye tracker in a manner analogous to methods used in Matthis, Yates, and Hayhoe (2018), Matthis, et al (PLoS Comp Bio, In Press) and Wirth and Matthis (VSS 2022). The resulting data reveals a tight coupling between gaze and the hand/juggling ball system as well as a complex relationship between the juggling and balance task.},
	number = {14},
	urldate = {2025-05-08},
	journal = {Journal of Vision},
	author = {Matthis, Jonathan and Cherian, Aaron and Wirth, Trent},
	month = dec,
	year = {2022},
	pages = {4195},
	file = {Snapshot:/Users/nsirmpilatze/Zotero/storage/FSAI8KCH/article.html:text/html},
}

@article{di_biase_tremor_2017,
	title = {Tremor stability index: a new tool for differential diagnosis in tremor syndromes},
	volume = {140},
	issn = {0006-8950},
	shorttitle = {Tremor stability index},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5493195/},
	doi = {10.1093/brain/awx104},
	abstract = {Misdiagnosis among tremor syndromes is common, and can impact on both clinical care and research. To date no validated neurophysiological technique is available that has proven to have good classification performance, and the diagnostic gold standard is the clinical evaluation made by a movement disorders expert. We present a robust new neurophysiological measure, the tremor stability index, which can discriminate Parkinson’s disease tremor and essential tremor with high diagnostic accuracy. The tremor stability index is derived from kinematic measurements of tremulous activity. It was assessed in a test cohort comprising 16 rest tremor recordings in tremor-dominant Parkinson’s disease and 20 postural tremor recordings in essential tremor, and validated on a second, independent cohort comprising a further 50 tremulous Parkinson’s disease and essential tremor recordings. Clinical diagnosis was used as gold standard. One hundred seconds of tremor recording were selected for analysis in each patient. The classification accuracy of the new index was assessed by binary logistic regression and by receiver operating characteristic analysis. The diagnostic performance was examined by calculating the sensitivity, specificity, accuracy, likelihood ratio positive, likelihood ratio negative, area under the receiver operating characteristic curve, and by cross-validation. Tremor stability index with a cut-off of 1.05 gave good classification performance for Parkinson’s disease tremor and essential tremor, in both test and validation datasets. Tremor stability index maximum sensitivity, specificity and accuracy were 95\%, 95\% and 92\%, respectively. Receiver operating characteristic analysis showed an area under the curve of 0.916 (95\% confidence interval 0.797–1.000) for the test dataset and a value of 0.855 (95\% confidence interval 0.754–0.957) for the validation dataset. Classification accuracy proved independent of recording device and posture. The tremor stability index can aid in the differential diagnosis of the two most common tremor types. It has a high diagnostic accuracy, can be derived from short, cheap, widely available and non-invasive tremor recordings, and is independent of operator or postural context in its interpretation.},
	number = {7},
	urldate = {2025-06-06},
	journal = {Brain : a journal of neurology},
	author = {di Biase, Lazzaro and Brittain, John-Stuart and Shah, Syed Ahmar and Pedrosa, David J. and Cagnan, Hayriye and Mathy, Alexandre and Chen, Chiung Chu and Martín-Rodríguez, Juan Francisco and Mir, Pablo and Timmerman, Lars and Schwingenschuh, Petra and Bhatia, Kailash and Di Lazzaro, Vincenzo and Brown, Peter},
	month = jul,
	year = {2017},
	pmid = {28459950},
	pmcid = {PMC5493195},
	pages = {1977--1986},
	file = {Full Text:/Users/nsirmpilatze/Zotero/storage/E7YDS2GX/di Biase et al. - 2017 - Tremor stability index a new tool for differentia.pdf:application/pdf},
}

@article{ho_fully_2023,
	title = {A fully automated home cage for long-term continuous phenotyping of mouse cognition and behavior},
	volume = {3},
	issn = {2667-2375},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10391580/},
	doi = {10.1016/j.crmeth.2023.100532},
	abstract = {Automated home-cage monitoring systems present a valuable tool for comprehensive phenotyping of natural behaviors. However, current systems often involve complex training routines, water or food restriction, and probe a limited range of behaviors. Here, we present a fully automated home-cage monitoring system for cognitive and behavioral phenotyping in mice. The system incorporates T-maze alternation, novel object recognition, and object-in-place recognition tests combined with monitoring of locomotion, drinking, and quiescence patterns, all carried out over long periods. Mice learn the tasks rapidly without any need for water or food restrictions. Behavioral characterization employs a deep convolutional neural network image analysis. We show that combined statistical properties of multiple behaviors can be used to discriminate between mice with hippocampal, medial entorhinal, and sham lesions and predict the genotype of an Alzheimer’s disease mouse model with high accuracy. This technology may enable large-scale behavioral screening for genes and neural circuits underlying spatial memory and other cognitive processes., •Fully automated execution of T-maze and novel object recognition test in home cages•Rapid task learning with no food or water restrictions•Testing has been done over long periods•Performance accuracy comparable to analogous standard tests, Automated phenotyping of mouse behavior is essential for improving standardization and increasing throughput, especially in the context of translational research, which may involve large numbers of different mouse groups. Unsupervised automated phenotyping of mouse cognitive functions, such as learning and memory, has been particularly challenging. Moreover, ideally the testing should be animal-friendly and ethologically relevant. Here, we present the home monitoring system, which includes fully automated implementation of T-maze, novel object recognition, and object-in-place tests, as well as monitoring of locomotion activities. All testing is carried out with no food or water restrictions., Ho et al. develop a home-cage system for mice with a fully automated T-maze, novel object recognition, and object-in-place tasks, as well as monitoring of locomotion. The system shows accuracy comparable to analogous standard tests and can be used for large-scale behavioral screening for genes and neural circuits underlying learning and memory.},
	number = {7},
	urldate = {2025-07-31},
	journal = {Cell Reports Methods},
	author = {Ho, Hinze and Kejzar, Nejc and Sasaguri, Hiroki and Saito, Takashi and Saido, Takaomi C. and De Strooper, Bart and Bauza, Marius and Krupic, Julija},
	month = jul,
	year = {2023},
	pmid = {37533650},
	pmcid = {PMC10391580},
	pages = {100532},
	file = {Full Text:/Users/nsirmpilatze/Zotero/storage/HBSR6ZTP/Ho et al. - 2023 - A fully automated home cage for long-term continuo.pdf:application/pdf},
}

@article{romero-ferrero_idtrackerai_2019,
	title = {idtracker.ai: tracking all individuals in small or large collectives of unmarked animals},
	volume = {16},
	copyright = {2019 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {1548-7105},
	shorttitle = {idtracker.ai},
	url = {https://www.nature.com/articles/s41592-018-0295-5},
	doi = {10.1038/s41592-018-0295-5},
	abstract = {Understanding of animal collectives is limited by the ability to track each individual. We describe an algorithm and software that extract all trajectories from video, with high identification accuracy for collectives of up to 100 individuals. idtracker.ai uses two convolutional networks: one that detects when animals touch or cross and another for animal identification. The tool is trained with a protocol that adapts to video conditions and tracking difficulty.},
	language = {en},
	number = {2},
	urldate = {2025-08-10},
	journal = {Nature Methods},
	author = {Romero-Ferrero, Francisco and Bergomi, Mattia G. and Hinz, Robert C. and Heras, Francisco J. H. and de Polavieja, Gonzalo G.},
	month = feb,
	year = {2019},
	note = {Publisher: Nature Publishing Group},
	keywords = {Machine learning, Software},
	pages = {179--182},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/HR4XQ9JT/Romero-Ferrero et al. - 2019 - idtracker.ai tracking all individuals in small or.pdf:application/pdf},
}

@article{gallois_fasttrack_2021,
	title = {{FastTrack}: {An} open-source software for tracking varying numbers of deformable objects},
	volume = {17},
	issn = {1553-7358},
	shorttitle = {{FastTrack}},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008697},
	doi = {10.1371/journal.pcbi.1008697},
	abstract = {Analyzing the dynamical properties of mobile objects requires to extract trajectories from recordings, which is often done by tracking movies. We compiled a database of two-dimensional movies for very different biological and physical systems spanning a wide range of length scales and developed a general-purpose, optimized, open-source, cross-platform, easy to install and use, self-updating software called FastTrack. It can handle a changing number of deformable objects in a region of interest, and is particularly suitable for animal and cell tracking in two-dimensions. Furthermore, we introduce the probability of incursions as a new measure of a movie’s trackability that doesn’t require the knowledge of ground truth trajectories, since it is resilient to small amounts of errors and can be computed on the basis of an ad hoc tracking. We also leveraged the versatility and speed of FastTrack to implement an iterative algorithm determining a set of nearly-optimized tracking parameters—yet further reducing the amount of human intervention—and demonstrate that FastTrack can be used to explore the space of tracking parameters to optimize the number of swaps for a batch of similar movies. A benchmark shows that FastTrack is orders of magnitude faster than state-of-the-art tracking algorithms, with a comparable tracking accuracy. The source code is available under the GNU GPLv3 at https://github.com/FastTrackOrg/FastTrack and pre-compiled binaries for Windows, Mac and Linux are available at http://www.fasttrack.sh.},
	language = {en},
	number = {2},
	urldate = {2025-08-10},
	journal = {PLOS Computational Biology},
	author = {Gallois, Benjamin and Candelier, Raphaël},
	month = feb,
	year = {2021},
	note = {Publisher: Public Library of Science},
	keywords = {Algorithms, Computer software, Ellipses, Image processing, Imaging techniques, Kinematics, Open source software, Probability distribution},
	pages = {e1008697},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/MME27TLV/Gallois and Candelier - 2021 - FastTrack An open-source software for tracking va.pdf:application/pdf},
}

@article{biderman_lightning_2024,
	title = {Lightning {Pose}: improved animal pose estimation via semi-supervised learning, {Bayesian} ensembling and cloud-native open-source tools},
	volume = {21},
	copyright = {2024 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {1548-7105},
	shorttitle = {Lightning {Pose}},
	url = {https://www.nature.com/articles/s41592-024-02319-1},
	doi = {10.1038/s41592-024-02319-1},
	abstract = {Contemporary pose estimation methods enable precise measurements of behavior via supervised deep learning with hand-labeled video frames. Although effective in many cases, the supervised approach requires extensive labeling and often produces outputs that are unreliable for downstream analyses. Here, we introduce ‘Lightning Pose’, an efficient pose estimation package with three algorithmic contributions. First, in addition to training on a few labeled video frames, we use many unlabeled videos and penalize the network whenever its predictions violate motion continuity, multiple-view geometry and posture plausibility (semi-supervised learning). Second, we introduce a network architecture that resolves occlusions by predicting pose on any given frame using surrounding unlabeled frames. Third, we refine the pose predictions post hoc by combining ensembling and Kalman smoothing. Together, these components render pose trajectories more accurate and scientifically usable. We released a cloud application that allows users to label data, train networks and process new videos directly from the browser.},
	language = {en},
	number = {7},
	urldate = {2025-08-10},
	journal = {Nature Methods},
	author = {Biderman, Dan and Whiteway, Matthew R. and Hurwitz, Cole and Greenspan, Nicholas and Lee, Robert S. and Vishnubhotla, Ankit and Warren, Richard and Pedraja, Federico and Noone, Dillon and Schartner, Michael M. and Huntenburg, Julia M. and Khanal, Anup and Meijer, Guido T. and Noel, Jean-Paul and Pan-Vazquez, Alejandro and Socha, Karolina Z. and Urai, Anne E. and Cunningham, John P. and Sawtell, Nathaniel B. and Paninski, Liam},
	month = jul,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computational neuroscience, Motor control},
	pages = {1316--1328},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/VXZ85Q2H/Biderman et al. - 2024 - Lightning Pose improved animal pose estimation vi.pdf:application/pdf},
}


@article{gunel_deepfly3d_2019,
	title = {{DeepFly3D}, a deep learning-based approach for {3D} limb and appendage tracking in tethered, adult {Drosophila}},
	volume = {8},
	issn = {2050-084X},
	url = {https://doi.org/10.7554/eLife.48571},
	doi = {10.7554/eLife.48571},
	abstract = {Studying how neural circuits orchestrate limbed behaviors requires the precise measurement of the positions of each appendage in three-dimensional (3D) space. Deep neural networks can estimate two-dimensional (2D) pose in freely behaving and tethered animals. However, the unique challenges associated with transforming these 2D measurements into reliable and precise 3D poses have not been addressed for small animals including the fly, Drosophila melanogaster. Here, we present DeepFly3D, a software that infers the 3D pose of tethered, adult Drosophila using multiple camera images. DeepFly3D does not require manual calibration, uses pictorial structures to automatically detect and correct pose estimation errors, and uses active learning to iteratively improve performance. We demonstrate more accurate unsupervised behavioral embedding using 3D joint angles rather than commonly used 2D pose data. Thus, DeepFly3D enables the automated acquisition of Drosophila behavioral measurements at an unprecedented level of detail for a variety of biological applications.},
	urldate = {2025-08-10},
	journal = {eLife},
	author = {Günel, Semih and Rhodin, Helge and Morales, Daniel and Campagnolo, João and Ramdya, Pavan and Fua, Pascal},
	editor = {O'Leary, Timothy and Calabrese, Ronald L and Shaevitz, Josh W},
	month = oct,
	year = {2019},
	note = {Publisher: eLife Sciences Publications, Ltd},
	keywords = {3D pose estimation, animal behavior, computer vision, deep learning, unsupervised classification},
	pages = {e48571},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/2LPN4S58/Günel et al. - 2019 - DeepFly3D, a deep learning-based approach for 3D l.pdf:application/pdf},
}

@article{opencv_library,
    author = {Bradski, G.},
    citeulike-article-id = {2236121},
    journal = {Dr. Dobb's Journal of Software Tools},
    keywords = {bibtex-import},
    posted-at = {2008-01-15 19:21:54},
    priority = {4},
    title = {{The OpenCV Library}},
    year = {2000}
}

@article{tomar2006converting,
  title={Converting video formats with FFmpeg},
  author={Tomar, Suramya},
  journal={Linux Journal},
  volume={2006},
  number={146},
  pages={10},
  year={2006},
  publisher={Belltown Media}
}


@misc{sirmpilatze_movement_2025,
	title = {neuroinformatics-unit/movement: v0.9.0},
	url = {https://doi.org/10.5281/zenodo.16754905},
	publisher = {Zenodo},
	author = {Sirmpilatze, Niko and Miñano, Sofía and Lo, Chang Huan and Tyson, Adam and Graham, Will and Prins, Stella and Peri, Brandon and {Dhruv} and Shaikh, Ishaan and Petrucco, Luigi and {Tushar-Verma} and {DPWebster} and Bhanushali, Harsh and Tatarnikov, Igor and {Kasra-Shirvanian} and Porta, Laura and {Lauraschwarz} and Halchenko, Yaroslav and {angkul} and V, Iván},
	month = aug,
	year = {2025},
	doi = {10.5281/zenodo.16754905},
}

@Misc{roaldarbol_animovement_2024,
	title = {animovement: An R toolbox for analysing animal movement across space and time.},
	author = {Mikkel Roald-Arbøl},
	year = {2024},
	url = {http://www.roald-arboel.com/animovement/},
	abstract = {An R toolbox for analysing animal movement across space and time.},
	version = {0.2.0},
}

@article{Martinez2020,
  doi = {10.21105/joss.02431},
  url = {https://doi.org/10.21105/joss.02431},
  year = {2020},
  publisher = {The Open Journal},
  volume = {5},
  number = {53},
  pages = {2431},
  author = {Romain Martinez and Benjamin Michaud and Mickael Begon},
  title = {`pyomeca`: An Open-Source Framework for Biomechanical Analysis},
  journal = {Journal of Open Source Software}
}

@article{scikit-mobility,
 title={scikit-mobility: A Python Library for the Analysis, Generation, and Risk Assessment of Mobility Data},
 volume={103},
 url={https://www.jstatsoft.org/index.php/jss/article/view/v103i04},
 doi={10.18637/jss.v103.i04},
 number={1},
 journal={Journal of Statistical Software},
 author={Pappalardo, Luca and Simini, Filippo and Barlacchi, Gianni and Pellungrini, Roberto},
 year={2022},
 pages={1–38}
}


@article{bohnslav_deepethogram_2021,
	title = {{DeepEthogram}, a machine learning pipeline for supervised behavior classification from raw pixels},
	volume = {10},
	issn = {2050-084X},
	url = {https://doi.org/10.7554/eLife.63377},
	doi = {10.7554/eLife.63377},
	abstract = {Videos of animal behavior are used to quantify researcher-defined behaviors of interest to study neural function, gene mutations, and pharmacological therapies. Behaviors of interest are often scored manually, which is time-consuming, limited to few behaviors, and variable across researchers. We created DeepEthogram: software that uses supervised machine learning to convert raw video pixels into an ethogram, the behaviors of interest present in each video frame. DeepEthogram is designed to be general-purpose and applicable across species, behaviors, and video-recording hardware. It uses convolutional neural networks to compute motion, extract features from motion and images, and classify features into behaviors. Behaviors are classified with above 90\% accuracy on single frames in videos of mice and flies, matching expert-level human performance. DeepEthogram accurately predicts rare behaviors, requires little training data, and generalizes across subjects. A graphical interface allows beginning-to-end analysis without end-user programming. DeepEthogram’s rapid, automatic, and reproducible labeling of researcher-defined behaviors of interest may accelerate and enhance supervised behavior analysis. Code is available at: https://github.com/jbohnslav/deepethogram.},
	urldate = {2025-08-10},
	journal = {eLife},
	author = {Bohnslav, James P and Wimalasena, Nivanthika K and Clausing, Kelsey J and Dai, Yu Y and Yarmolinsky, David A and Cruz, Tomás and Kashlan, Adam D and Chiappe, M Eugenia and Orefice, Lauren L and Woolf, Clifford J and Harvey, Christopher D},
	editor = {Mathis, Mackenzie W and Behrens, Timothy E and Mathis, Mackenzie W and Bohacek, Johannes},
	month = sep,
	year = {2021},
	note = {Publisher: eLife Sciences Publications, Ltd},
	keywords = {behavior analysis, computer vision, deep learning},
	pages = {e63377},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/E23LLR8X/Bohnslav et al. - 2021 - DeepEthogram, a machine learning pipeline for supe.pdf:application/pdf},
}

@article{Miranda2023,
doi = {10.21105/joss.05394},
url = {https://doi.org/10.21105/joss.05394},
year = {2023},
publisher = {The Open Journal},
volume = {8},
number = {86},
pages = {5394},
author = {Miranda, Lucas and Bordes, Joeri and Pütz, Benno and Schmidt, Mathias V. and Müller-Myhsok, Bertram},
title = {DeepOF: a Python package for supervised and unsupervised pattern recognition in mice motion tracking data},
journal = {Journal of Open Source Software}
}

@misc{chindemi2023lisbet,
  title={LISBET: a machine learning model for the automatic segmentation of social behavior motifs},
  author={Giuseppe Chindemi and Benoit Girard and Camilla Bellone},
  year={2023},
  eprint={2311.04069},
  archivePrefix={arXiv},
  primaryClass={cs.CV}
}

@article{berman_mapping_2014,
	title = {Mapping the stereotyped behaviour of freely moving fruit flies},
	volume = {11},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsif.2014.0672},
	doi = {10.1098/rsif.2014.0672},
	abstract = {A frequent assumption in behavioural science is that most of an animal's activities can be described in terms of a small set of stereotyped motifs. Here, we introduce a method for mapping an animal's actions, relying only upon the underlying structure of postural movement data to organize and classify behaviours. Applying this method to the ground-based behaviour of the fruit fly, Drosophila melanogaster, we find that flies perform stereotyped actions roughly 50\% of the time, discovering over 100 distinguishable, stereotyped behavioural states. These include multiple modes of locomotion and grooming. We use the resulting measurements as the basis for identifying subtle sex-specific behavioural differences and revealing the low-dimensional nature of animal motions.},
	number = {99},
	urldate = {2025-08-10},
	journal = {Journal of The Royal Society Interface},
	author = {Berman, Gordon J. and Choi, Daniel M. and Bialek, William and Shaevitz, Joshua W.},
	month = oct,
	year = {2014},
	note = {Publisher: Royal Society},
	keywords = {behaviour, Drosophila, phase reconstruction, stereotypy, unsupervised learning},
	pages = {20140672},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/H9GQPG4S/Berman et al. - 2014 - Mapping the stereotyped behaviour of freely moving.pdf:application/pdf},
}

@article{friard_boris_2016,
	title = {{BORIS}: a free, versatile open-source event-logging software for video/audio coding and live observations},
	volume = {7},
	copyright = {© 2016 The Authors. Methods in Ecology and Evolution © 2016 British Ecological Society},
	issn = {2041-210X},
	shorttitle = {{BORIS}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.12584},
	doi = {10.1111/2041-210X.12584},
	abstract = {Quantitative aspects of the study of animal and human behaviour are increasingly relevant to test hypotheses and find empirical support for them. At the same time, photo and video cameras can store a large number of video recordings and are often used to monitor the subjects remotely. Researchers frequently face the need to code considerable quantities of video recordings with relatively flexible software, often constrained by species-specific options or exact settings. BORIS is a free, open-source and multiplatform standalone program that allows a user-specific coding environment to be set for a computer-based review of previously recorded videos or live observations. Being open to user-specific settings, the program allows a project-based ethogram to be defined that can then be shared with collaborators, or can be imported or modified. Projects created in BORIS can include a list of observations, and each observation may include one or two videos (e.g. simultaneous screening of visual stimuli and the subject being tested; recordings from different sides of an aquarium). Once the user has set an ethogram, including state or point events or both, coding can be performed using previously assigned keys on the computer keyboard. BORIS allows definition of an unlimited number of events (states/point events) and subjects. Once the coding process is completed, the program can extract a time-budget or single or grouped observations automatically and present an at-a-glance summary of the main behavioural features. The observation data and time-budget analysis can be exported in many common formats (TSV, CSV, ODF, XLS, SQL and JSON). The observed events can be plotted and exported in various graphic formats (SVG, PNG, JPG, TIFF, EPS and PDF).},
	language = {en},
	number = {11},
	urldate = {2025-08-10},
	journal = {Methods in Ecology and Evolution},
	author = {Friard, Olivier and Gamba, Marco},
	year = {2016},
	note = {\_eprint: https://besjournals.onlinelibrary.wiley.com/doi/pdf/10.1111/2041-210X.12584},
	keywords = {behaviour coding, behavioural analysis, coding scheme, ethology, observational data, ttime-budget},
	pages = {1325--1330},
	file = {Full Text PDF:/Users/nsirmpilatze/Zotero/storage/IH26IC5B/Friard and Gamba - 2016 - BORIS a free, versatile open-source event-logging.pdf:application/pdf;Snapshot:/Users/nsirmpilatze/Zotero/storage/FSJPAYHT/2041-210X.html:text/html},
}

@article{Tinbergen1954-TINTSO-2,
	author = {N. Tinbergen},
	journal = {British Journal for the Philosophy of Science},
	number = {17},
	pages = {72--76},
	publisher = {Taylor \& Francis},
	title = {The Study of Instinct},
	volume = {5},
	year = {1954}
}


@book{tinbergen_study_1951,
	title = {The {Study} of {Instinct}},
	isbn = {978-0-19-857343-2},
	language = {en},
	publisher = {Clarendon Press},
	author = {Tinbergen, Niko},
	year = {1951},
	note = {Google-Books-ID: ZtUlAAAAMAAJ},
}


@article{ants_crispr_2017,
	title = {{CRISPR} ants lose ability to smell},
	volume = {548},
	copyright = {2017 Springer Nature Limited},
	url = {https://www.nature.com/articles/d41586-017-02337-4},
	doi = {10.1038/d41586-017-02337-4},
	abstract = {First gene-edited mutant-ant lines show defects in social behaviour.},
	language = {en},
	number = {7667},
	urldate = {2025-08-10},
	journal = {Nature},
	month = aug,
	year = {2017},
	note = {Bandiera\_abtest: a
Cg\_type: Research Highlight
Publisher: Nature Publishing Group
Subject\_term: Genetics},
	keywords = {Genetics},
	pages = {263--263},
	file = {Snapshot:/Users/nsirmpilatze/Zotero/storage/PTXHAIUY/d41586-017-02337-4.html:text/html},
}

@article{anderson_toward_2014,
	title = {Toward a {Science} of {Computational} {Ethology}},
	volume = {84},
	issn = {0896-6273},
	url = {https://www.sciencedirect.com/science/article/pii/S0896627314007934},
	doi = {10.1016/j.neuron.2014.09.005},
	abstract = {The new field of “Computational Ethology” is made possible by advances in technology, mathematics, and engineering that allow scientists to automate the measurement and the analysis of animal behavior. We explore the opportunities and long-term directions of research in this area.},
	number = {1},
	urldate = {2025-08-10},
	journal = {Neuron},
	author = {Anderson, David J. and Perona, Pietro},
	month = oct,
	year = {2014},
	pages = {18--31},
	file = {ScienceDirect Snapshot:/Users/nsirmpilatze/Zotero/storage/56KI8FS2/S0896627314007934.html:text/html},
}