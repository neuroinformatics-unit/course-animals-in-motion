# Introduction {#sec-intro}

## Summary

Measuring animal behaviour often comes down to measuring movement, whether of the whole body or specific parts, using tools such as video cameras and GPS loggers.

Advances in computer vision, particularly deep learning-based markerless motion tracking, have transformed how we study animal behaviour, bridging traditions from multiple scientific disciplines.
These methods now underpin the emerging field of computational (neuro)ethology, enabling researchers to extract precise motion data from videos, quantify behavioural features, and (in neuroscience) relate them directly to neural activity.

In this handbook, we focus on two key steps in this workflow: pose estimation and motion quantification, while recognising the broader ecosystem of open-source tools available.

## Measuring behaviour as movement

Defining behaviour is tricky, and behavioural biologists cannot agree on a single definition [@levitis_behavioural_2009].
The following one by Tinbergen has some historical sway:

> The total movements made by the intact animal [@tinbergen_study_1951].

Framing behaviour as "movement" is useful.
In most studies, what we actually measure is the motion of one or more animalsâ€”and/or their body partsâ€”over time.
We can see that reflected in the devices we most often use to record animal behaviour:

- ðŸŽ¥ **Video cameras**
- ðŸ“± Inertial measurement units (IMUs)
- ðŸ›°ï¸ GPS-based biologgers
- ðŸŽ¤ Microphones

With the exception of microphones, these devices measure movement, at different spatial and temporal scales.

::: {.callout-note}
You could say that microphones capture body movements indirectly: sound waves are generated via the motion of specialised organs such as vocal cords.
:::

In this handbook, **we focus exclusively on video recordings**.
There are two main ways to extract motion from videos:

- **Marker-based** methods: physical markers are placed on individuals or body parts.
- **Markerless** methods: computer vision is used to extract user-defined features directly from videos.

::: {#fig-motion-capture layout="[[40,60], [100]]" layout-valign="bottom"}
![By danceinthesky, openverse.org](img/motion_capture.jpg)

![Source: @ants_crispr_2017](img/ants_colored.jpeg)

![Source: @mathis_primer_2020](img/from_pixels_to_keypoints.png)

Examples of marker-based (top row) and markerless (bottom row) methods.
:::

::: {.callout-tip title="Discuss"}

- What are some advantages and disadvantages of marker-based vs. markerless methods?
- In which cases would you use one over the other?
:::

## The rise of markerless methods

There have broadly been two traditions in the study of animal behaviour [@datta_computational_2019]:

- **Neuroscientists** have long focused on how animals generate behaviours in response to rewards and punishments, often training them to perform simple, easily measured actions.
- **Ethologists**, in contrast, tend to study naturalistic behaviours expressed freely in ecologically relevant contexts, aiming to understand how behaviour is structured and organised over timeâ€”for example through the use of ethograms.

::: {#fig-two-traditions layout="[[25,25,50]]"}
![By Ethan Tyler & Lex Kravitz, scidraw.io](img/mouse_lever.svg)

![By Andrea Colins Rodriguez, scidraw.io](img/Monkey_schematic.svg)

![Excerpt from a fiddler crab ethogram by Sanna Titus](img/crab_ethogram.png)

Schematic representation of the neuroscientific tradition, juxtaposed with an ethogram.
:::

In the last 10 years, computer vision methods for motion tracking have had an utterly transformative impact.
Tools such as [DeepLabCut](https://www.mackenziemathislab.org/deeplabcut/) [@mathis_deeplabcut_2018] and [SLEAP](https://sleap.ai/) [@pereira_sleap_2022] enable researchers to track animal movements in video recordings with impressive accuracy, without the need for physical markers.

::: {#fig-tracking-gifs layout="[[60,40], [100]]" layout-valign="bottom"}
![Source: mackenziemathislab.org/deeplabcut](img/MousereachGIF.gif)

![Source: idtracker.ai](img/idtracker_zebrafish.gif)

![Source: sleap.ai](img/sleap_movie.gif)

Examples of markerless motion tracking in action.
:::

This rise is mostly driven by advances in deep learning and has dramatically accelerated the scale at which naturalistic behaviour can be measured and analysed. The old distinctions between research traditions are being erased and a new field is emerging:

> **In the past decade, a field we now call "computational ethology" has begun to take shape**. It involves the use of machine vision and machine learning to measure and analyze the patterns of action generated by animals in contexts designed to evoke ecologically relevant behaviors [@anderson_toward_2014]. Technical progress in statistical inference and deep learning, the democratization of high-performance computing (due to falling hardware costs and the ability to rent GPUs and CPUs in the cloud), and new and creative ideas about how to apply technology to measuring naturalistic behavior have dramatically accelerated progress in this research area. [@datta_computational_2019]

![Scientific disciplines converging on markerless motion tracking methods.](img/fields_converge_on_tracking.png){#fig-fields-converge}

::: {.callout-tip title="Discuss"}

- Which scientific disciplines do you represent?
- What species are you working with?
- Have you witnessed an increasing use of markerless tracking methods in your field?
:::

## The computational (neuro)ethology workflow

The rise of markerless methods has reconfigured the data acquisition and analysis workflows for scientists interested in measuring animal behaviour:

- Video recordings are the primary data source.
- Computer vision tools (most often based on deep learning) are used to extract user-defined features from video frames and track them over time.
- The resulting tracks can then be used to quantify various aspects of motion, such as speed, orientation, distance travelled, etc.
- The motion tracks, video features and derived kinematic features may serve as input for behaviour segmentation algorithms.

![An overview of modern computational ethology workflows](img/ethology_workflow.png){#fig-ethology-workflow}

An ecosystem of open-source tools has emerged to support this workflow. See @luxem_open-source_2023, @pereira_quantifying_2020, @blau_study_2024 for comprehensive reviews. We also provide a non-exhaustive list of open-source tools in [@sec-useful-tools].

::: {.callout-note title="Where does the 'neuro' come in?" collapse=true}

Neuroscientists are increasingly interested in precisely quantifying naturalistic behaviour, for several reasons:

> if we are to understand how the brain works, we need to think about the actual problems it evolved to solve. Addressing this challenge means studying natural behavior â€” the kinds of behaviors generated by animals when they are free to act on their own internally-generated goals without physical or psychological restraint ([source: Datta Lab website](http://datta.hms.harvard.edu/research/overview/)).
>
> â€¦detailed examination of brain parts or their selective perturbation is not sufficient to understand how the brain generates behavior [@krakauer_neuroscience_2017].
>
> The behavioral work needs to be as fine-grained as work at the neural level. Otherwise one is imperiled by a granularity mismatch between levelsâ€¦ [@krakauer_neuroscience_2017].

This shift in focus within neuroscience has been a major driver for the rapid development of the computational approaches described above.
Some refer to this field as **computational neuroethology**â€”the science of quantifying naturalistic behaviours to understand the brain [@datta_computational_2019].

The data acquisition and analysis workflows used in computational neuroethology are similar to those shown in @fig-ethology-workflow.
The key difference is that derived measures of behaviourâ€”whether continuous variables like speed or discrete actions such as "grooming"â€”are ultimately analysed alongside neural data, such as spike trains or calcium imaging traces (@fig-neuroethology-workflow).

![An overview of modern computational neuroethology workflows.](img/neuroethology_workflow.png){#fig-neuroethology-workflow}

Highly **recommended readings**:

- @krakauer_neuroscience_2017
- @datta_computational_2019
- @pereira_quantifying_2020

:::

## The scope of this handbook

- @sec-dl-cv: A primer on deep learning for computer vision, going over the key concepts and technologies that underpin most markerless tracking approaches.
- @sec-sleap: Pose estimation and tracking with [SLEAP](https://sleap.ai/) [@pereira_sleap_2022]. We chose SLEAP because we know it best, but the knowledge gained should be applicable to most other pose estimation tools.
- @sec-movement-intro: An introduction into analysing motion tracks with [movement](https://movement.neuroinformatics.dev/)â€”a Python package we develop [@sirmpilatze_movement_2025]â€”followed by two case studies applying `movement` to real-world datasets:
  - @sec-movement-mouse: Continuous home cage monitoring of mouse activity levels for weeks.
  - @sec-movement-zebras: Collective escape behaviour in a herd of zebras.

This leaves out some key stages of the computational ethology workflow [@fig-ethology-workflow], most notably video acquisition and behaviour segmentation, which may be covered in future versions of this handbook.

![What is covered in this handbook](img/SLEAP_and_movement.png){#fig-sleap-movement}

::: {.callout-tip}
We want to emphasise that there are many excellent open-source tools beyond those we focus on here.
The next section provides a non-exhaustive list of tools that may be useful for your own projects.
:::

## Useful open-source tools {#sec-useful-tools}

::: {.callout-note}
We've mainly highlighted tools commonly applied to animal behaviour data.
This is not a comprehensive list, and the tools appear in no particular order.
Some don't fit neatly into the categories below, and their classification is somewhat subjective.
If you'd like to add a tool you've built or enjoy using, please [open an issue](https://github.com/neuroinformatics-unit/course-animals-in-motion/issues) or submit a pull request.
:::

### Data acquisition

- [Bonsai](https://bonsai-rx.org/) [@lopes_bonsai_2015]

### Video processing

- [OpenCV](https://opencv.org/) @opencv_library
- [ffmpeg](https://ffmpeg.org/) @tomar2006converting

### Motion tracking

- [DeepLabCut](http://www.mackenziemathislab.org/deeplabcut) @mathis_deeplabcut_2018; @lauer_multi-animal_2022
- [SLEAP](https://sleap.ai/) @pereira_sleap_2022
- [LightningPose](https://lightning-pose.readthedocs.io/en/latest/) @biderman_lightning_2024
- [TRex](https://trex.run/) @walter_trex_2021
- [idtracker.ai](https://idtrackerai.readthedocs.io/en/latest/) @romero-ferrero_idtrackerai_2019
- [Anipose](https://anipose.readthedocs.io/en/latest/) @karashchuk_anipose_2021
- [DANNCE](https://github.com/spoonsso/dannce/) @dunn_geometric_2021
- [DeepPoseKit](https://github.com/jgraving/deepposekit) @graving_deepposekit_2019
- [FastTrack](https://www.fasttrack.sh/) @gallois_fasttrack_2021
- [DeepFly3D](https://github.com/NeLy-EPFL/DeepFly3D) @gunel_deepfly3d_2019

### Motion quantification

- [movement](https://github.com/neuroinformatics-unit/movement) @sirmpilatze_movement_2025
- [animovement](https://roald-arboel.com/animovement/) @roaldarbol_animovement_2024
- [PyRat](https://github.com/pyratlib/pyrat) @de_almeida_pyrat_2022
- [DLC2Kinematics](https://github.com/AdaptiveMotorControlLab/DLC2Kinematics)
- [pyomeca](https://pyomeca.github.io/pyomeca/) @Martinez2020
- [movingpandas](https://github.com/movingpandas/movingpandas) @graser_movingpandas_2019
- [scikit-mobility](https://github.com/scikit-mobility/scikit-mobility) @scikit-mobility
- [Rtrack](https://rupertoverall.net/Rtrack/index.html) @Rtrack_2020
- [ColonyTrack](https://rupertoverall.net/ColonyTrack) @colony_track_2024
- [swaRm](https://swarm-lab.github.io/swaRm)

### Behaviour segmentation

- [(Keypoint) MoSeq](https://dattalab.github.io/moseq2-website/index.html) @wiltschko_mapping_2015; @weinreb_keypoint-moseq_2024
- [VAME](https://ethoml.github.io/VAME/) @luxem_identifying_2022
- [B-SOiD](https://github.com/YttriLab/B-SOID) @hsu_b-soid_2021
- [A-SOiD](https://github.com/YttriLab/A-SOID) @schweihoff_-soid_2022
- [DeepEthogram](https://github.com/jbohnslav/deepethogram) @bohnslav_deepethogram_2021
- [SimBA](https://simba-uw-tf-dev.readthedocs.io/en/latest/) @goodwin_simple_2024
- [DeepOF](https://deepof.readthedocs.io/en/latest/) @Miranda2023
- [LISBET](https://github.com/BelloneLab/lisbet) @chindemi2023lisbet
- [DLC2action](https://github.com/amathislab/DLC2action)
- [LabGym](https://github.com/umyelab/LabGym) @hu_labgym_2023
- [JABS](https://github.com/KumarLabJax/JABS-behavior-classifier) @beane_jax_2023
- [JAABA](https://jaaba.sourceforge.net/) @kabra_jaaba_2013
- [MotionMapper](https://github.com/gordonberman/MotionMapper) @berman_mapping_2014
- [BORIS](https://www.boris.unito.it/) @friard_boris_2016
- [MARS](https://neuroethology.github.io/MARS/) @mars2021
