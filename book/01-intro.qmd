# Introduction {#sec-intro}

Machine learning methods for motion tracking have transformed a wide range of scientific disciplines—from neuroscience and biomechanics to conservation and ethology.
Tools such as [DeepLabCut](https://www.mackenziemathislab.org/deeplabcut/) [@mathis_deeplabcut_2018] and [SLEAP](https://sleap.ai/) [@pereira_sleap_2022] enable researchers to track animal movements in video recordings with impressive accuracy, without the need for physical markers.

However, the variety of available tools can be overwhelming [@luxem_open-source_2023].
It's often unclear which tool is best suited to a given application, or how to get started.
We'll provide an overview of the approaches used for quantifying animal behaviour,
and we'll narrow down into Computer Vision (CV) methods for detecting and tracking animals in videos.

## Quantifing behaviour

How do we define behaviour?

> The total movements made by the intact animal [@tinbergen_study_1951]

> Behavior is the **internally coordinated responses** (actions or inactions) of whole living organisms (individuals or groups) **to internal and/or external stimuli**, excluding responses more easily understood as developmental changes [@levitis_behavioural_2009]

![Source: @pereira_quantifying_2020](img/pose_estimation_2D.png)

![Source: @mathis_primer_2020](img/from_pixels_to_keypoints.png)

> **In the past decade, a field we now call "computational ethology" has begun to take shape**. It involves the use of machine vision and machine learning to measure and analyze the patterns of action generated by animals in contexts designed to evoke ecologically relevant behaviors (Anderson and Perona, 2014). Technical progress in statistical inference and deep learning, the democratization of high-performance computing (due to falling hardware costs and the ability to rent GPUs and CPUs in the cloud), and new and creative ideas about how to apply technology to measuring naturalistic behavior have dramatically accelerated progress in this research area. [@datta_computational_2019]

![An overview of modern computational ethology workflows](img/ethology_workflow.png){#fig-ethology-workflow}

::: {.callout-note title="Where does neuroscience come in?" collapse=true}

Neuroscientists are increasingly interested in precisely quantifying naturalistic behaviour, for several reasons:

> if we are to understand how the brain works, we need to think about the actual problems it evolved to solve. Addressing this challenge means studying natural behavior — the kinds of behaviors generated by animals when they are free to act on their own internally-generated goals without physical or psychological restraint ([source: Datta Lab website](http://datta.hms.harvard.edu/research/overview/)).

> …detailed examination of brain parts or their selective perturbation is not sufficient to understand how the brain generates behavior [@krakauer_neuroscience_2017].

> The behavioral work needs to be as fine-grained as work at the neural level. Otherwise one is imperiled by a granularity mismatch between levels… [@krakauer_neuroscience_2017].

This shift in focus within neuroscience has been a major driver of the rapid development of the computational methods described above.
Some refer to this field as **computational neuroethology**—the science of quantifying naturalistic behaviours to understand the brain [@datta_computational_2019].

The data acquisition and analysis workflows used in computational neuroethology are similar to those shown in @fig-ethology-workflow.
The key difference is that derived measures of behaviour—whether continuous variables like speed or discrete actions such as "grooming"—are ultimately analysed alongside neural data, such as spike trains or calcium imaging traces (@fig-neuroethology-workflow).

![An overview of modern computational neuroethology workflows](img/neuroethology_workflow.png){#fig-neuroethology-workflow}

Highly **recommended readings**:

- @krakauer_neuroscience_2017
- @datta_computational_2019
- @pereira_quantifying_2020

:::

## The scope of this course

As we saw, modern computational ethology workflows involve many steps, and the variety of tools available for some of them can be overwhelming [@luxem_open-source_2023; @blau_study_2024].

We could take a whirlwind tour through all the stages of a typical workflow and the tools available for each. However, since we want this to work as a two-day hands-on workshop with plenty of time for exercises and active learning, we have chosen to focus on the two steps we know best:

- [@sec-sleap]: Pose estimation and tracking with [SLEAP](https://sleap.ai/) [@pereira_sleap_2022]
- [@sec-movement-intro; @sec-movement-mouse; @sec-movement-zebras]: Motion quantification with [movement](https://movement.neuroinformatics.dev/) [@sirmpilatze_movement_2025]

![What we'll cover in this course](img/SLEAP_and_movement.png){#fig-sleap-movement}

Pose estimation is a key computer vision method due to its popularity and versatility, made possible by excellent, easy-to-use open-source tools such as [DeepLabCut](https://www.mackenziemathislab.org/deeplabcut/) [@mathis_deeplabcut_2018] and [SLEAP](https://sleap.ai/) [@pereira_sleap_2022].

In @sec-sleap, we will use SLEAP to detect animals and their body parts (keypoints) in videos and track them over time. We chose SLEAP because we know it best, but the lessons learned apply equally to other pose estimation tools, most of which share the same underlying principles.

In @sec-movement-intro, we will introduce motion quantification with `movement`, a Python package we develop, which addresses the question: what can I do now with these tracks?

The last two chapters — [@sec-movement-mouse] and [@sec-movement-zebras] — are case studies applying `movement` to real-world datasets.

::: {.callout-tip}
That said, we want to emphasise that there are many excellent open-source tools beyond those we focus on here.
The next section provides a non-exhaustive list of tools that may be useful for your own projects.
:::

## Useful open-source software tools {#sec-useful-tools}

::: {.callout-note}
We've mainly highlighted tools commonly applied to animal behaviour data.
This is not a comprehensive list, and the tools appear in no particular order.
Some don't fit neatly into the categories below, and their classification is somewhat subjective.
If you'd like to add a tool you've built or enjoy using, please [open an issue](https://github.com/neuroinformatics-unit/course-animals-in-motion/issues) or submit a pull request.
:::

### Data acquisition

- [Bonsai](https://bonsai-rx.org/) [@lopes_bonsai_2015]

### Video processing

- [OpenCV](https://opencv.org/) @opencv_library
- [ffmpeg](https://ffmpeg.org/) @tomar2006converting

### Motion tracking

- [DeepLabCut](http://www.mackenziemathislab.org/deeplabcut) @mathis_deeplabcut_2018; @lauer_multi-animal_2022
- [SLEAP](https://sleap.ai/) @pereira_sleap_2022
- [LightningPose](https://lightning-pose.readthedocs.io/en/latest/) @biderman_lightning_2024
- [TRex](https://trex.run/) @walter_trex_2021
- [idtracker.ai](https://idtrackerai.readthedocs.io/en/latest/) @romero-ferrero_idtrackerai_2019
- [Anipose](https://anipose.readthedocs.io/en/latest/) @karashchuk_anipose_2021
- [DANNCE](https://github.com/spoonsso/dannce/) @dunn_geometric_2021
- [DeepPoseKit](https://github.com/jgraving/deepposekit) @graving_deepposekit_2019
- [FastTrack](https://www.fasttrack.sh/) @gallois_fasttrack_2021
- [DeepFly3D](https://github.com/NeLy-EPFL/DeepFly3D) @gunel_deepfly3d_2019

### Motion quantification

- [movement](https://github.com/neuroinformatics-unit/movement) @sirmpilatze_movement_2025
- [animovement](https://roald-arboel.com/animovement/) @roaldarbol_animovement_2024
- [PyRat](https://github.com/pyratlib/pyrat) @de_almeida_pyrat_2022
- [DLC2Kinematics](https://github.com/AdaptiveMotorControlLab/DLC2Kinematics)
- [pyomeca](https://pyomeca.github.io/pyomeca/) @Martinez2020
- [movingpandas](https://github.com/movingpandas/movingpandas) @graser_movingpandas_2019
- [scikit-mobility](https://github.com/scikit-mobility/scikit-mobility) @scikit-mobility

### Behaviour segmentation

- [(Keypoint) MoSeq](https://dattalab.github.io/moseq2-website/index.html) @wiltschko_mapping_2015; @weinreb_keypoint-moseq_2024
- [VAME](https://ethoml.github.io/VAME/) @luxem_identifying_2022
- [B-SOiD](https://github.com/YttriLab/B-SOID) @hsu_b-soid_2021
- [A-SOiD](https://github.com/YttriLab/A-SOID) @schweihoff_-soid_2022
- [DeepEthogram](https://github.com/jbohnslav/deepethogram) @bohnslav_deepethogram_2021
- [SimBA](https://simba-uw-tf-dev.readthedocs.io/en/latest/) @goodwin_simple_2024
- [DeepOF](https://deepof.readthedocs.io/en/latest/) @Miranda2023
- [LISBET](https://github.com/BelloneLab/lisbet) @chindemi2023lisbet
- [DLC2action](https://github.com/amathislab/DLC2action)
- [LabGym](https://github.com/umyelab/LabGym) @hu_labgym_2023
- [JABS](https://github.com/KumarLabJax/JABS-behavior-classifier) @beane_jax_2023
- [JAABA](https://jaaba.sourceforge.net/) @kabra_jaaba_2013
- [MotionMapper](https://github.com/gordonberman/MotionMapper) @berman_mapping_2014
- [BORIS](https://www.boris.unito.it/) @friard_boris_2016
